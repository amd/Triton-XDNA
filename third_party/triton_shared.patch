diff --git a/backend/compiler.py b/backend/compiler.py
index d534835..ee941bb 100644
--- a/backend/compiler.py
+++ b/backend/compiler.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 from triton.backends.compiler import BaseBackend, GPUTarget
 from triton._C.libtriton import ir, passes
 from dataclasses import dataclass
@@ -67,6 +64,7 @@ def _ttir_to_ttsharedir(mod):
             subprocess_args.insert(2, "--add-llvm-debug-info")
 
         subprocess.check_call(subprocess_args)
+        _dump_ir_if_needed([dst_path])
         return Path(dst_path).read_text()
 
 
@@ -116,6 +114,7 @@ def _ttsharedir_to_llir(ttsharedir: str):
             "--mlir-print-debuginfo",
             "-o",
             llmlir_path])
+        _dump_ir_if_needed([llmlir_path])
 
         # LLVM-MLIR to LLVM-IR
         mlir_translate_path = _get_llvm_bin_path("mlir-translate")
@@ -123,7 +122,7 @@ def _ttsharedir_to_llir(ttsharedir: str):
             "--mlir-to-llvmir",
             "-o",
             llir_path])
-        _dump_ir_if_needed([ttshared_path, llmlir_path, llir_path])
+        _dump_ir_if_needed([llir_path])
         return Path(llir_path).read_text()
 
 
@@ -154,7 +153,7 @@ def _llir_to_bin(llir: str, metadata):
             sanitizer_attributes_pass_path = str(next(Path(top_level_triton_path).rglob("libSanitizerAttributes.so"), None))
 
             if not sanitizer_attributes_pass_path:
-                raise Exception(f"libSanitizerAttributes.so does not exist.")
+                raise Exception("libSanitizerAttributes.so does not exist.")
 
             subprocess.check_call([opt_path, "-load-pass-plugin", sanitizer_attributes_pass_path, 
                 "-passes=sanitizer-attributes", f"-sanitizer-type={sanitizer_type}", "-S", src_path, 
@@ -197,6 +196,7 @@ class CPUOptions:
     allow_fp8e4nv: bool = False
     allowed_dot_input_precisions: Tuple[str] = ("ieee", )
     sanitize_overflow: bool = True
+    instrumentation_mode: str = ""
 
     def __post_init__(self):
         pass
@@ -259,7 +259,7 @@ class CPUBackend(BaseBackend):
         passes.common.add_symbol_dce(pm)
         passes.ttir.add_loop_unroll(pm)
         passes.common.add_cse(pm)
-        pm.run(mod)
+        pm.run(mod, 'make_ttir')
         return mod
 
     def add_stages(self, stages, options, language):
diff --git a/backend/driver.py b/backend/driver.py
index f528179..de45f70 100644
--- a/backend/driver.py
+++ b/backend/driver.py
@@ -1,13 +1,12 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import hashlib
 import tempfile
-import sysconfig
 
-import os, subprocess, tempfile, platform
+import os
+import subprocess
+import platform
 import importlib.util
 import sys
+import time
 
 from pathlib import Path
 
@@ -66,7 +65,18 @@ def _ty_to_cpp(ty):
         "fp64": "double",
     }[ty]
 
+def _flatten_signature(sig, output):
+    # Flatten tuples
+    if isinstance(sig, tuple):
+        for x in sig:
+            _flatten_signature(x, output)
+    else:
+        output.append(sig)
+
 def _extracted_type(ty):
+    if isinstance(ty, tuple):
+        val = ','.join(map(_extracted_type, ty))
+        return f"[{val}]"
     if ty[0] == '*':
         return "PyObject*"
     if ty == "constexpr":
@@ -74,6 +84,15 @@ def _extracted_type(ty):
     return _ty_to_cpp(ty)
 
 def _format_of(ty):
+    if isinstance(ty, tuple):
+        val = ''.join(map(_format_of, ty))
+        return f"({val})"
+    if ty[0] == '*':
+        return "O"
+    if ty == "constexpr":
+        return "O"
+    if ty.startswith("tensordesc"):
+        return "O"
     return {
       "PyObject*": "O",
       "constexpr": "O",
@@ -88,15 +107,20 @@ def _format_of(ty):
       "uint16_t": "H",
       "uint32_t": "I",
       "uint64_t": "K",
-    }[ty]
+    }[_ty_to_cpp(ty)]
 
 def _generate_launcher(constants, signature, kernel_name):
-    arg_decls = ', '.join(f"{_ty_to_cpp(ty)} arg{i}" for i, ty in signature.items())
-    args_format = ''.join([_format_of(_extracted_type(ty)) for ty in signature.values()])
+    args_format = ''.join([_format_of(ty) for ty in signature.values()])
     format = "iiiOOOO" + args_format
+
+    flat_signature = []
+    for sig in signature.values():
+        _flatten_signature(sig, flat_signature)
+    signature = {i: s for i, s in enumerate(flat_signature)}
+    arg_decls = ', '.join(f"{_ty_to_cpp(ty)} arg{i}" for i, ty in signature.items())
     args_list = ', ' + ', '.join(f"&_arg{i}" for i, ty in signature.items()) if len(signature) > 0 else ''
 
-    kernel_arg_decls = ', '.join(_ty_to_cpp(ty) if ty[0] != "*" else f"int64_t, void*" for i, ty in signature.items() if ty != "constexpr")
+    kernel_arg_decls = ', '.join(_ty_to_cpp(ty) if ty[0] != "*" else "int64_t, void*" for i, ty in signature.items() if ty != "constexpr")
     kernel_arg_decls += ', ' if kernel_arg_decls else ''
 
     kernel_parameters = ', '.join(f"static_cast<{_ty_to_cpp(ty)}>(arg{i})" if ty[0] != "*" else f"0, &ptr_arg{i}" for i, ty in signature.items() if ty != "constexpr")
@@ -330,7 +354,7 @@ def compile_module(launcher_src, kernel_placeholder_name):
                           libomp_path = next(Path(Path(_get_llvm_bin_path("")).parent).rglob("libomp.so"), None)
 
                           if not libomp_path:
-                              raise Exception(f"libomp.so does not exist.")
+                              raise Exception("libomp.so does not exist.")
 
                           libomp_path = str(libomp_path.parent)
 
@@ -367,7 +391,8 @@ class CPULauncher(object):
         kernel_placeholder_name = "KERNEL_NAME_PLACEHOLDER"
 
         constants = src.constants if hasattr(src, "constants") else dict()
-        cst_key = lambda i: src.fn.arg_names.index(i) if isinstance(i, str) else i
+        def cst_key(i):
+            return src.fn.arg_names.index(i) if isinstance(i, str) else i
         constants = {cst_key(key): value for key, value in constants.items()}
         signature = {cst_key(key): value for key, value in src.signature.items()}
         launcher_src = _generate_launcher(constants, signature, kernel_placeholder_name)
@@ -434,9 +459,56 @@ class CPUDriver(DriverBase):
     def is_active():
         return False
 
+    def do_bench(self, fn, warmup=25, rep=100, grad_to_none=None, quantiles=None, return_mode="mean"):
+        assert return_mode in ["min", "max", "mean", "median", "all"]
+        fn()
+
+        # cache = runtime.driver.active.get_empty_cache_for_benchmark()
+
+        # Estimate the runtime of the function
+        start_event = time.perf_counter()
+        for _ in range(5):
+            # runtime.driver.active.clear_cache(cache)
+            fn()
+        end_event = time.perf_counter()
+        estimate_ms = int((end_event - start_event)*1000 / 5)
+      
+        from triton import knobs
+        verbose = knobs.autotuning.print
+        if verbose:
+            print("CPU estimate ms: ", estimate_ms)
+
+        # compute number of warmup and repeat
+        n_warmup = max(1, int(warmup / estimate_ms))
+        n_repeat = max(5, int(rep / estimate_ms))
+        start_event = [0 for i in range(n_repeat)]
+        end_event = [0 for i in range(n_repeat)]
+        # Warm-up
+        for _ in range(n_warmup):
+            fn()
+        # Benchmark
+        for i in range(n_repeat):
+            # we don't want `fn` to accumulate gradient values
+            # if it contains a backward pass. So we clear the
+            # provided gradients
+            if grad_to_none is not None:
+                for x in grad_to_none:
+                    x.grad = None
+            # we clear the L2 cache before each run
+            # runtime.driver.active.clear_cache(cache)
+            # record time of `fn`
+            start_event[i] = time.perf_counter()
+            fn()
+            end_event[i] = time.perf_counter()
+        times = [(e-s)*1000 for s, e in zip(start_event, end_event)]
+        if verbose: 
+            print("KERNEL TIMES: ", ", ".join(str(t) for t in times))
+
+        from triton.testing import _summarize_statistics
+        return _summarize_statistics(times, quantiles, return_mode)
+
     def get_benchmarker(self):
-        from triton.testing import do_bench
-        return do_bench
+        return self.do_bench
 
     def get_device_capability(self):
         return ("cpu", 0)
diff --git a/include/triton-shared/Analysis/MaskAnalysis.h b/include/triton-shared/Analysis/MaskAnalysis.h
index 8ca8cc4..0ad7c6f 100644
--- a/include/triton-shared/Analysis/MaskAnalysis.h
+++ b/include/triton-shared/Analysis/MaskAnalysis.h
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation, Meta Platforms.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/Analysis/OpFoldResultUtils.h b/include/triton-shared/Analysis/OpFoldResultUtils.h
index 476a8fd..248dfc3 100644
--- a/include/triton-shared/Analysis/OpFoldResultUtils.h
+++ b/include/triton-shared/Analysis/OpFoldResultUtils.h
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation, Meta Platforms.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/Analysis/PtrAnalysis.h b/include/triton-shared/Analysis/PtrAnalysis.h
index 759cc78..5a95ebd 100644
--- a/include/triton-shared/Analysis/PtrAnalysis.h
+++ b/include/triton-shared/Analysis/PtrAnalysis.h
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/Analysis/UseAnalysis.h b/include/triton-shared/Analysis/UseAnalysis.h
index a1c661b..39c3055 100644
--- a/include/triton-shared/Analysis/UseAnalysis.h
+++ b/include/triton-shared/Analysis/UseAnalysis.h
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/AnalysisStructured/PtrAnalysis.h b/include/triton-shared/AnalysisStructured/PtrAnalysis.h
index 306a7b5..b1791aa 100644
--- a/include/triton-shared/AnalysisStructured/PtrAnalysis.h
+++ b/include/triton-shared/AnalysisStructured/PtrAnalysis.h
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation, Meta Platforms.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/Conversion/StructuredToMemref/Passes.h b/include/triton-shared/Conversion/StructuredToMemref/Passes.h
index 1af4f3c..198675b 100644
--- a/include/triton-shared/Conversion/StructuredToMemref/Passes.h
+++ b/include/triton-shared/Conversion/StructuredToMemref/Passes.h
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef TRITON_STRUCTURED_TO_MEMREF_CONVERSION_PASSES_H
 #define TRITON_STRUCTURED_TO_MEMREF_CONVERSION_PASSES_H
 
diff --git a/include/triton-shared/Conversion/StructuredToMemref/Passes.td b/include/triton-shared/Conversion/StructuredToMemref/Passes.td
index be56471..ac318f5 100644
--- a/include/triton-shared/Conversion/StructuredToMemref/Passes.td
+++ b/include/triton-shared/Conversion/StructuredToMemref/Passes.td
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef STRUCTURED_TO_MEMREF_CONVERSION_PASSES
 #define STRUCTURED_TO_MEMREF_CONVERSION_PASSES
 
diff --git a/include/triton-shared/Conversion/StructuredToMemref/StructuredToMemref.h b/include/triton-shared/Conversion/StructuredToMemref/StructuredToMemref.h
index 196cc22..8c67c9e 100644
--- a/include/triton-shared/Conversion/StructuredToMemref/StructuredToMemref.h
+++ b/include/triton-shared/Conversion/StructuredToMemref/StructuredToMemref.h
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef TRITON_CONVERSION_STRUCTUREDTOMEMREF_STRUCTUREDTOMEMREF_H
 #define TRITON_CONVERSION_STRUCTUREDTOMEMREF_STRUCTUREDTOMEMREF_H
 
diff --git a/include/triton-shared/Conversion/TritonArithToLinalg/ConversionPatterns.hpp b/include/triton-shared/Conversion/TritonArithToLinalg/ConversionPatterns.hpp
index 097254c..c509be3 100644
--- a/include/triton-shared/Conversion/TritonArithToLinalg/ConversionPatterns.hpp
+++ b/include/triton-shared/Conversion/TritonArithToLinalg/ConversionPatterns.hpp
@@ -3,7 +3,7 @@
 
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation, Meta Platforms.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
@@ -59,14 +59,14 @@ static Value getScalarValue(Value operand, Location loc,
                   if (auto shapedType = dyn_cast<ShapedType>(resType)) {
                     resType = shapedType.getElementType();
                   }
-                  return rewriter.create<arith::SIToFPOp>(loc, resType, src);
+                  return arith::SIToFPOp::create(rewriter, loc, resType, src);
                 })
                 .Case<arith::TruncFOp>([&](Operation *op) {
                   auto resType = op->getResults()[0].getType();
                   if (auto shapedType = dyn_cast<ShapedType>(resType)) {
                     resType = shapedType.getElementType();
                   }
-                  return rewriter.create<arith::TruncFOp>(loc, resType, src);
+                  return arith::TruncFOp::create(rewriter, loc, resType, src);
                 })
                 .Default([](Operation *op) {
                   llvm_unreachable("unsupported op in generating ");
@@ -134,11 +134,11 @@ static Value getTransposedValue(Value source, const Location loc,
     }
   }
 
-  Value transposeInit = rewriter.create<tensor::EmptyOp>(
-      loc, transposedShape, sourceType.getElementType());
+  Value transposeInit = tensor::EmptyOp::create(rewriter, loc, transposedShape,
+                                                sourceType.getElementType());
 
   Value transpose =
-      rewriter.create<linalg::TransposeOp>(loc, source, transposeInit, perm)
+      linalg::TransposeOp::create(rewriter, loc, source, transposeInit, perm)
           .getResults()[0];
 
   return transpose;
@@ -191,8 +191,8 @@ struct MakeTensorPtrConverter
                              Location loc) const {
     for (auto opnd : ops) {
       if (isa<IntegerType>(opnd.getType())) {
-        auto castOp = rewriter.create<arith::IndexCastOp>(
-            loc, rewriter.getIndexType(), opnd);
+        auto castOp = arith::IndexCastOp::create(rewriter, loc,
+                                                 rewriter.getIndexType(), opnd);
         vec.push_back(castOp.getResult());
       } else {
         assert(isa<IndexType>(opnd.getType()));
@@ -224,8 +224,8 @@ struct MakeTensorPtrConverter
     SmallVector<Value> newOffsets;
     for (auto [offset, stride] :
          llvm::zip(pointerState.offsets, pointerState.strides)) {
-      auto mulOp = rewriter.create<arith::MulIOp>(loc, cast<Value>(offset),
-                                                  cast<Value>(stride));
+      auto mulOp = arith::MulIOp::create(rewriter, loc, cast<Value>(offset),
+                                         cast<Value>(stride));
       newOffsets.push_back(mulOp.getResult());
     }
 
@@ -278,71 +278,67 @@ private:
                               ConversionPatternRewriter &rewriter) const {
 
     auto zero =
-        rewriter.create<arith::ConstantOp>(loc, rewriter.getIndexAttr(0));
+        arith::ConstantOp::create(rewriter, loc, rewriter.getIndexAttr(0));
 
     auto one =
-        rewriter.create<arith::ConstantOp>(loc, rewriter.getIndexAttr(1));
+        arith::ConstantOp::create(rewriter, loc, rewriter.getIndexAttr(1));
 
-    Value block1Row = rewriter.create<memref::DimOp>(loc, block1, 0);
-    Value block1Col = rewriter.create<memref::DimOp>(loc, block1, 1);
+    Value block1Row = memref::DimOp::create(rewriter, loc, block1, 0);
+    Value block1Col = memref::DimOp::create(rewriter, loc, block1, 1);
 
-    Value block2Row = rewriter.create<memref::DimOp>(loc, block2, 0);
-    Value block2Col = rewriter.create<memref::DimOp>(loc, block2, 1);
+    Value block2Row = memref::DimOp::create(rewriter, loc, block2, 0);
+    Value block2Col = memref::DimOp::create(rewriter, loc, block2, 1);
 
-    auto block1Dst =
-        rewriter.create<memref::SubViewOp>(loc, dst, /* offsets */
-                                           ValueRange{zero, zero},
-                                           /* sizes */
-                                           ValueRange{block1Row, block1Col},
-                                           /* strides */
-                                           ValueRange{one, one});
+    auto block1Dst = memref::SubViewOp::create(rewriter, loc, dst, /* offsets */
+                                               ValueRange{zero, zero},
+                                               /* sizes */
+                                               ValueRange{block1Row, block1Col},
+                                               /* strides */
+                                               ValueRange{one, one});
 
-    auto block2Dst =
-        rewriter.create<memref::SubViewOp>(loc, dst,
-                                           /* offsets */
-                                           ValueRange{zero, block1Col},
-                                           /* sizes */
-                                           ValueRange{block2Row, block2Col},
-                                           /* strides */
-                                           ValueRange{one, one});
+    auto block2Dst = memref::SubViewOp::create(rewriter, loc, dst,
+                                               /* offsets */
+                                               ValueRange{zero, block1Col},
+                                               /* sizes */
+                                               ValueRange{block2Row, block2Col},
+                                               /* strides */
+                                               ValueRange{one, one});
 
-    rewriter.create<memref::CopyOp>(loc, block1, block1Dst);
-    rewriter.create<memref::CopyOp>(loc, block2, block2Dst);
+    memref::CopyOp::create(rewriter, loc, block1, block1Dst);
+    memref::CopyOp::create(rewriter, loc, block2, block2Dst);
   }
 
   void createStackedCopies(Value block1, Value block2, Value dst, Location loc,
                            ConversionPatternRewriter &rewriter) const {
 
     auto zero =
-        rewriter.create<arith::ConstantOp>(loc, rewriter.getIndexAttr(0));
+        arith::ConstantOp::create(rewriter, loc, rewriter.getIndexAttr(0));
     auto one =
-        rewriter.create<arith::ConstantOp>(loc, rewriter.getIndexAttr(1));
+        arith::ConstantOp::create(rewriter, loc, rewriter.getIndexAttr(1));
 
-    Value block1Row = rewriter.create<memref::DimOp>(loc, block1, 0);
-    Value block1Col = rewriter.create<memref::DimOp>(loc, block1, 1);
+    Value block1Row = memref::DimOp::create(rewriter, loc, block1, 0);
+    Value block1Col = memref::DimOp::create(rewriter, loc, block1, 1);
 
-    Value block2Row = rewriter.create<memref::DimOp>(loc, block2, 0);
-    Value block2Col = rewriter.create<memref::DimOp>(loc, block2, 1);
+    Value block2Row = memref::DimOp::create(rewriter, loc, block2, 0);
+    Value block2Col = memref::DimOp::create(rewriter, loc, block2, 1);
 
-    auto block1Dst =
-        rewriter.create<memref::SubViewOp>(loc, dst, /* offsets */
-                                           ValueRange{zero, zero},
-                                           /* sizes */
-                                           ValueRange{block1Row, block1Col},
-                                           /* strides */
-                                           ValueRange{one, one});
+    auto block1Dst = memref::SubViewOp::create(rewriter, loc, dst, /* offsets */
+                                               ValueRange{zero, zero},
+                                               /* sizes */
+                                               ValueRange{block1Row, block1Col},
+                                               /* strides */
+                                               ValueRange{one, one});
 
-    auto block2Dst =
-        rewriter.create<memref::SubViewOp>(loc, dst,
-                                           /* offsets */
-                                           ValueRange{block1Row, zero},
-                                           /* sizes */
-                                           ValueRange{block2Row, block2Col},
-                                           /* strides */
-                                           ValueRange{one, one});
+    auto block2Dst = memref::SubViewOp::create(rewriter, loc, dst,
+                                               /* offsets */
+                                               ValueRange{block1Row, zero},
+                                               /* sizes */
+                                               ValueRange{block2Row, block2Col},
+                                               /* strides */
+                                               ValueRange{one, one});
 
-    rewriter.create<memref::CopyOp>(loc, block1, block1Dst);
-    rewriter.create<memref::CopyOp>(loc, block2, block2Dst);
+    memref::CopyOp::create(rewriter, loc, block1, block1Dst);
+    memref::CopyOp::create(rewriter, loc, block2, block2Dst);
   }
 
 public:
@@ -359,8 +355,8 @@ public:
       auto sMemRef = PtrAnalysis::getScalarMemRef(op.getPtr(), adaptor.getPtr(),
                                                   loc, rewriter);
       auto zeroMap = AffineMap::getConstantMap(0, rewriter.getContext());
-      auto loadOp = rewriter.create<affine::AffineLoadOp>(
-          op.getLoc(), sMemRef, zeroMap, ValueRange{});
+      auto loadOp = affine::AffineLoadOp::create(rewriter, op.getLoc(), sMemRef,
+                                                 zeroMap, ValueRange{});
       rewriter.replaceOp(op, loadOp.getResult());
       return success();
     }
@@ -378,8 +374,8 @@ public:
 
     auto tensorType =
         RankedTensorType::get(type.getShape(), type.getElementType());
-    auto alloc = rewriter.create<memref::AllocOp>(
-        loc, MemRefType::get(type.getShape(), type.getElementType()));
+    auto alloc = memref::AllocOp::create(
+        rewriter, loc, MemRefType::get(type.getShape(), type.getElementType()));
 
     if (!mask) {
       assert(!other && "other value used in non-masked load");
@@ -404,11 +400,12 @@ public:
         }
 
       } else {
-        rewriter.create<memref::CopyOp>(loc, ptr, alloc);
+        memref::CopyOp::create(rewriter, loc, ptr, alloc);
       }
 
-      Value tensor = rewriter.create<bufferization::ToTensorOp>(
-          loc, tensorType, alloc, true /* restrict */, true /* writable */);
+      Value tensor = bufferization::ToTensorOp::create(
+          rewriter, loc, tensorType, alloc, true /* restrict */,
+          true /* writable */);
       rewriter.replaceOp(op, tensor);
 
       return success();
@@ -435,31 +432,33 @@ public:
       // the result
       auto shape = type.getShape();
       auto accBase =
-          rewriter.create<arith::ConstantOp>(loc, rewriter.getBoolAttr(false))
+          arith::ConstantOp::create(rewriter, loc, rewriter.getBoolAttr(false))
               .getResult();
       for (size_t i = 0; i < type.getShape().size(); i++) {
-        auto shapei = rewriter.create<arith::ConstantOp>(
-            loc, rewriter.getIndexAttr(shape[i]));
+        auto shapei = arith::ConstantOp::create(
+            rewriter, loc, rewriter.getIndexAttr(shape[i]));
 
         Value dimi = dyn_cast<Value>(mstate.dims[i]);
         if (!dimi) {
-          dimi = rewriter.create<arith::ConstantOp>(
-              loc, cast<IntegerAttr>(cast<Attribute>(mstate.dims[i])));
+          dimi = arith::ConstantOp::create(
+              rewriter, loc,
+              cast<IntegerAttr>(cast<Attribute>(mstate.dims[i])));
         }
 
-        auto cmpOp = rewriter.create<arith::CmpIOp>(
-            loc, arith::CmpIPredicate::slt, dimi, shapei);
-        accBase = rewriter.create<arith::OrIOp>(loc, accBase, cmpOp.getResult())
-                      .getResult();
+        auto cmpOp = arith::CmpIOp::create(
+            rewriter, loc, arith::CmpIPredicate::slt, dimi, shapei);
+        accBase =
+            arith::OrIOp::create(rewriter, loc, accBase, cmpOp.getResult())
+                .getResult();
       }
 
       // condition the memset on the or-accumulation
       // initialize with padding prior to CopyOp
-      rewriter.create<scf::IfOp>(
-          loc, accBase, [&](OpBuilder &builder, Location loc) {
-            builder.create<linalg::FillOp>(loc, ValueRange{scalarOther},
-                                           ValueRange{alloc});
-            builder.create<scf::YieldOp>(loc);
+      scf::IfOp::create(
+          rewriter, loc, accBase, [&](OpBuilder &builder, Location loc) {
+            linalg::FillOp::create(builder, loc, ValueRange{scalarOther},
+                                   ValueRange{alloc});
+            scf::YieldOp::create(builder, loc);
           });
     }
 
@@ -492,11 +491,12 @@ public:
     } else {
       memref::SubViewOp srcSubview = mstate.getSubview(ptr, loc, rewriter);
       memref::SubViewOp dstSubview = mstate.getSubview(alloc, loc, rewriter);
-      rewriter.create<memref::CopyOp>(loc, srcSubview, dstSubview);
+      memref::CopyOp::create(rewriter, loc, srcSubview, dstSubview);
     }
 
-    Value tensor = rewriter.create<bufferization::ToTensorOp>(
-        loc, tensorType, alloc, true /* restrict */, true /* writable */);
+    Value tensor = bufferization::ToTensorOp::create(rewriter, loc, tensorType,
+                                                     alloc, true /* restrict */,
+                                                     true /* writable */);
     rewriter.replaceOp(op, tensor);
 
     return success();
@@ -519,16 +519,16 @@ struct StoreConverter : public OpConversionPattern<triton::StoreOp> {
       auto sMemRef =
           PtrAnalysis::getScalarMemRef(op.getPtr(), ptr, loc, rewriter);
       auto zeroMap = AffineMap::getConstantMap(0, rewriter.getContext());
-      rewriter.create<affine::AffineStoreOp>(loc, val, sMemRef, zeroMap,
-                                             ValueRange{});
+      affine::AffineStoreOp::create(rewriter, loc, val, sMemRef, zeroMap,
+                                    ValueRange{});
       rewriter.eraseOp(op);
       return success();
     }
 
     // 1. Simple case where no mask is used.
     if (!mask) {
-      auto storeOp = rewriter.create<bufferization::MaterializeInDestinationOp>(
-          loc, val, ptr);
+      auto storeOp = bufferization::MaterializeInDestinationOp::create(
+          rewriter, loc, val, ptr);
       storeOp.setWritable(true);
       rewriter.eraseOp(op);
       return success();
@@ -546,8 +546,8 @@ struct StoreConverter : public OpConversionPattern<triton::StoreOp> {
     auto srcSlice = mstate.getExtractSlice(val, loc, rewriter);
     auto dstSubview = mstate.getSubview(ptr, loc, rewriter);
 
-    auto storeOp = rewriter.create<bufferization::MaterializeInDestinationOp>(
-        loc, srcSlice, dstSubview);
+    auto storeOp = bufferization::MaterializeInDestinationOp::create(
+        rewriter, loc, srcSlice, dstSubview);
     storeOp.setWritable(true);
     rewriter.eraseOp(op);
 
@@ -635,13 +635,12 @@ struct SplatConverter : public OpConversionPattern<triton::SplatOp> {
     auto opType = cast<TensorType>(op.getType());
     auto loc = op.getLoc();
 
-    auto init = rewriter.create<tensor::EmptyOp>(loc, opType.getShape(),
-                                                 opType.getElementType());
+    auto init = tensor::EmptyOp::create(rewriter, loc, opType.getShape(),
+                                        opType.getElementType());
 
     auto filledTensor =
-        rewriter
-            .create<linalg::FillOp>(loc, ValueRange{adaptor.getSrc()},
-                                    ValueRange{init})
+        linalg::FillOp::create(rewriter, loc, ValueRange{adaptor.getSrc()},
+                               ValueRange{init})
             .result();
 
     rewriter.replaceOp(op, filledTensor);
@@ -697,16 +696,16 @@ public:
                         rewriter.getMultiDimIdentityMap(resultRank));
 
     assert(op->getNumResults() == 1 && "code assumes single result!");
-    auto init = rewriter.create<tensor::EmptyOp>(loc, resultType.getShape(),
-                                                 elementType);
+    auto init = tensor::EmptyOp::create(rewriter, loc, resultType.getShape(),
+                                        elementType);
 
-    auto linalgOp = rewriter.create<linalg::GenericOp>(
-        loc, op->getResultTypes(), ValueRange{adaptor.getSrc()},
+    auto linalgOp = linalg::GenericOp::create(
+        rewriter, loc, op->getResultTypes(), ValueRange{adaptor.getSrc()},
         ValueRange{init}, indexingMaps, getNParallelLoopsAttrs(resultRank),
         [&](OpBuilder &nestedBuilder, Location nestedLoc,
             ValueRange blockArgs) {
           Value opResult = blockArgs[0];
-          nestedBuilder.create<linalg::YieldOp>(loc, opResult);
+          linalg::YieldOp::create(nestedBuilder, loc, opResult);
         });
 
     linalgOp->setAttr("broadcastDims",
@@ -740,8 +739,8 @@ struct ExpandDimsConverter : public OpConversionPattern<triton::ExpandDimsOp> {
       reassoc.push_back(g);
     }
 
-    auto expandShapeOp = rewriter.create<tensor::ExpandShapeOp>(
-        op.getLoc(), resType, src, reassoc);
+    auto expandShapeOp = tensor::ExpandShapeOp::create(rewriter, op.getLoc(),
+                                                       resType, src, reassoc);
 
     rewriter.replaceOp(op, expandShapeOp.getResult());
     return success();
@@ -781,22 +780,22 @@ struct MakeRangeConverter : public OpConversionPattern<triton::MakeRangeOp> {
         /* dimCount */ 1, /* symbolCount */ 0,
         SmallVector<AffineExpr>{mlir::getAffineDimExpr(0, context)}, context)};
 
-    auto init = rewriter.create<tensor::EmptyOp>(loc, shape, elementType);
-    auto linalgOp = rewriter.create<linalg::GenericOp>(
-        loc, op->getResultTypes(), /* operands */ ValueRange{},
+    auto init = tensor::EmptyOp::create(rewriter, loc, shape, elementType);
+    auto linalgOp = linalg::GenericOp::create(
+        rewriter, loc, op->getResultTypes(), /* operands */ ValueRange{},
         ValueRange{init}, indexingMaps, getNParallelLoopsAttrs(1),
         [&](OpBuilder &nestedBuilder, Location nestedLoc,
             ValueRange blockArgs) {
-          Value index = nestedBuilder.create<linalg::IndexOp>(loc, 0);
-          Value res = nestedBuilder.create<arith::IndexCastOp>(
-              loc, type.getElementType(), index);
+          Value index = linalg::IndexOp::create(nestedBuilder, loc, 0);
+          Value res = arith::IndexCastOp::create(nestedBuilder, loc,
+                                                 type.getElementType(), index);
           if (op.getStart()) {
-            auto start = rewriter.create<mlir::arith::ConstantIntOp>(
-                op.getLoc(), op.getStart(),
+            auto start = arith::ConstantIntOp::create(
+                rewriter, op.getLoc(), op.getStart(),
                 type.getElementType().getIntOrFloatBitWidth());
-            res = nestedBuilder.create<arith::AddIOp>(loc, res, start);
+            res = arith::AddIOp::create(nestedBuilder, loc, res, start);
           }
-          nestedBuilder.create<linalg::YieldOp>(loc, res);
+          linalg::YieldOp::create(nestedBuilder, loc, res);
         });
 
     rewriter.replaceOp(op, linalgOp->getResults());
@@ -819,8 +818,7 @@ struct AssertConverter : public OpConversionPattern<triton::AssertOp> {
     // TritonOps.td. Tensors will always be RankedTensorType.
     if (isa<mlir::IntegerType>(condVal.getType())) {
       // handle scalar case
-      rewriter.create<mlir::cf::AssertOp>(op.getLoc(), condVal,
-                                          assertMessage.str());
+      cf::AssertOp::create(rewriter, op.getLoc(), condVal, assertMessage.str());
     } else if (auto tensorType =
                    dyn_cast<RankedTensorType>(condVal.getType())) {
       // handle tensor case
@@ -834,8 +832,8 @@ struct AssertConverter : public OpConversionPattern<triton::AssertOp> {
       SmallVector<utils::IteratorType, 3> iteratorTypes(
           rank, utils::IteratorType::parallel);
 
-      rewriter.create<linalg::GenericOp>(
-          op.getLoc(), TypeRange{}, condVal, ValueRange{},
+      linalg::GenericOp::create(
+          rewriter, op.getLoc(), TypeRange{}, condVal, ValueRange{},
           ArrayRef<AffineMap>{indexingMaps},
           ArrayRef<utils::IteratorType>{iteratorTypes},
           [&](OpBuilder &b, Location loc, ValueRange args) {
@@ -843,9 +841,9 @@ struct AssertConverter : public OpConversionPattern<triton::AssertOp> {
             Value element = args[0];
 
             // make a cf.assert for the current element
-            b.create<mlir::cf::AssertOp>(loc, element, assertMessage.str());
+            cf::AssertOp::create(b, loc, element, assertMessage.str());
 
-            b.create<linalg::YieldOp>(loc);
+            linalg::YieldOp::create(b, loc);
           });
     } else {
       op.emitError("Unexpected type in triton::AssertOp");
@@ -868,8 +866,8 @@ struct BitcastConverter : public OpConversionPattern<triton::BitcastOp> {
       return failure();
     }
 
-    auto arithBitcast = rewriter.create<arith::BitcastOp>(
-        op.getLoc(), op.getType(), op.getOperand());
+    auto arithBitcast = arith::BitcastOp::create(rewriter, op.getLoc(),
+                                                 op.getType(), op.getOperand());
 
     rewriter.replaceOp(op, arithBitcast.getResult());
     return success();
@@ -908,8 +906,8 @@ struct CallConverter : public OpConversionPattern<triton::CallOp> {
       }
     }
 
-    auto call = rewriter.create<func::CallOp>(op.getLoc(), op.getCallee(),
-                                              op.getResultTypes(), args);
+    auto call = func::CallOp::create(rewriter, op.getLoc(), op.getCallee(),
+                                     op.getResultTypes(), args);
 
     if (!call) {
       op.emitError("Failed to create func::CallOp");
@@ -946,14 +944,14 @@ struct FpToFpConverter : public OpConversionPattern<triton::FpToFpOp> {
            "Not a float-like operand or result");
 
     if (operandWidth.value() > resultWidth.value()) {
-      Value truncatedValue = rewriter.create<arith::TruncFOp>(
-          op.getLoc(), resultType, op.getOperand());
+      Value truncatedValue = arith::TruncFOp::create(
+          rewriter, op.getLoc(), resultType, op.getOperand());
       rewriter.replaceOp(op, truncatedValue);
       return success();
     }
 
-    Value extendedValue = rewriter.create<arith::ExtFOp>(
-        op.getLoc(), resultType, op.getOperand());
+    Value extendedValue = arith::ExtFOp::create(rewriter, op.getLoc(),
+                                                resultType, op.getOperand());
     rewriter.replaceOp(op, extendedValue);
 
     return success();
@@ -975,11 +973,11 @@ struct ClampConverter : public OpConversionPattern<triton::ClampFOp> {
 
     Value clamp;
     if (propagateNan) {
-      Value maxMin = rewriter.create<arith::MaximumFOp>(loc, x, min);
-      clamp = rewriter.create<arith::MinimumFOp>(loc, maxMin, max);
+      Value maxMin = arith::MaximumFOp::create(rewriter, loc, x, min);
+      clamp = arith::MinimumFOp::create(rewriter, loc, maxMin, max);
     } else {
-      Value maxMin = rewriter.create<arith::MaxNumFOp>(loc, x, min);
-      clamp = rewriter.create<arith::MinNumFOp>(loc, maxMin, max);
+      Value maxMin = arith::MaxNumFOp::create(rewriter, loc, x, min);
+      clamp = arith::MinNumFOp::create(rewriter, loc, maxMin, max);
     }
     rewriter.replaceOp(op, clamp);
 
@@ -995,7 +993,7 @@ struct PreciseSqrtConverter
   matchAndRewrite(triton::PreciseSqrtOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     auto replacement =
-        rewriter.create<math::SqrtOp>(op.getLoc(), adaptor.getOperands());
+        math::SqrtOp::create(rewriter, op.getLoc(), adaptor.getOperands());
 
     rewriter.replaceOp(op, replacement);
     return success();
@@ -1009,7 +1007,7 @@ struct PreciseDivConverter : public OpConversionPattern<triton::PreciseDivFOp> {
   matchAndRewrite(triton::PreciseDivFOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
     auto replacement =
-        rewriter.create<arith::DivFOp>(op.getLoc(), adaptor.getOperands());
+        arith::DivFOp::create(rewriter, op.getLoc(), adaptor.getOperands());
 
     rewriter.replaceOp(op, replacement);
     return success();
@@ -1022,8 +1020,8 @@ struct CatConverter : public OpConversionPattern<triton::CatOp> {
   LogicalResult
   matchAndRewrite(triton::CatOp op, OpAdaptor adaptor,
                   ConversionPatternRewriter &rewriter) const override {
-    auto replacement = rewriter.create<tensor::ConcatOp>(
-        op.getLoc(), 0 /* concat dimension */, adaptor.getOperands());
+    auto replacement = tensor::ConcatOp::create(
+        rewriter, op.getLoc(), 0 /* concat dimension */, adaptor.getOperands());
 
     rewriter.replaceOp(op, replacement);
 
@@ -1060,8 +1058,8 @@ struct SplitConverter : public OpConversionPattern<triton::SplitOp> {
 
       offsets.push_back(rewriter.getIndexAttr(i));
       sizes.push_back(rewriter.getIndexAttr(1));
-      Value slice = rewriter.create<tensor::ExtractSliceOp>(
-          loc, resultTensor, input, offsets, sizes, strides);
+      Value slice = tensor::ExtractSliceOp::create(
+          rewriter, loc, resultTensor, input, offsets, sizes, strides);
       results.push_back(slice);
     }
 
@@ -1081,8 +1079,8 @@ struct JoinConverter : public OpConversionPattern<triton::JoinOp> {
     auto resultType = cast<RankedTensorType>(op.getResult().getType());
 
     auto loc = op.getLoc();
-    Value result = rewriter.create<tensor::EmptyOp>(
-        loc, resultType.getShape(), resultType.getElementType());
+    Value result = tensor::EmptyOp::create(rewriter, loc, resultType.getShape(),
+                                           resultType.getElementType());
 
     auto shape = resultType.getShape();
 
@@ -1099,8 +1097,8 @@ struct JoinConverter : public OpConversionPattern<triton::JoinOp> {
 
       offsets.push_back(rewriter.getIndexAttr(i));
       sizes.push_back(rewriter.getIndexAttr(1));
-      result = rewriter.create<tensor::InsertSliceOp>(loc, inputs[i], result,
-                                                      offsets, sizes, strides);
+      result = tensor::InsertSliceOp::create(rewriter, loc, inputs[i], result,
+                                             offsets, sizes, strides);
     }
 
     rewriter.replaceOp(op, result);
@@ -1118,7 +1116,7 @@ struct MulHiUIOpConverter : public OpConversionPattern<triton::MulhiUIOp> {
     Location loc = op.getLoc();
 
     auto mulResult =
-        rewriter.create<arith::MulUIExtendedOp>(loc, adaptor.getOperands());
+        arith::MulUIExtendedOp::create(rewriter, loc, adaptor.getOperands());
     rewriter.replaceOp(op, mulResult.getHigh());
 
     return success();
@@ -1169,29 +1167,28 @@ struct MatmulConverter : public OpConversionPattern<triton::DotOp> {
     bool integers = elementType.isInteger();
     bool skipC = isZeroTensor(opc, integers);
     auto init =
-        rewriter.create<tensor::EmptyOp>(loc, dstType.getShape(), elementType);
+        tensor::EmptyOp::create(rewriter, loc, dstType.getShape(), elementType);
     TypedAttr constantAttr =
         integers
             ? static_cast<TypedAttr>(rewriter.getIntegerAttr(elementType, 0))
             : static_cast<TypedAttr>(rewriter.getFloatAttr(elementType, 0));
 
-    auto zero = rewriter.create<mlir::arith::ConstantOp>(
-        op.getLoc(), elementType, constantAttr);
+    auto zero = arith::ConstantOp::create(rewriter, op.getLoc(), elementType,
+                                          constantAttr);
 
-    auto zeroes =
-        rewriter.create<linalg::FillOp>(loc, ValueRange{zero}, ValueRange{init})
-            .result();
+    auto zeroes = linalg::FillOp::create(rewriter, loc, ValueRange{zero},
+                                         ValueRange{init})
+                      .result();
 
-    auto res = rewriter
-                   .create<linalg::MatmulOp>(loc, ValueRange{opa, opb},
-                                             ValueRange{zeroes})
+    auto res = linalg::MatmulOp::create(rewriter, loc, ValueRange{opa, opb},
+                                        ValueRange{zeroes})
                    .getResult(0);
 
     if (!skipC) {
       if (integers) {
-        res = rewriter.create<arith::AddIOp>(loc, opc, res);
+        res = arith::AddIOp::create(rewriter, loc, opc, res);
       } else {
-        res = rewriter.create<arith::AddFOp>(loc, opc, res);
+        res = arith::AddFOp::create(rewriter, loc, opc, res);
       }
     }
 
@@ -1273,8 +1270,8 @@ private:
               return nullptr;
             });
 
-    return rewriter.create<arith::ConstantOp>(redOp->getLoc(), constantType,
-                                              attr);
+    return arith::ConstantOp::create(rewriter, redOp->getLoc(), constantType,
+                                     attr);
   }
 
   bool requiresF32Conversion(const Type elemType, Operation *redOp) const {
@@ -1291,16 +1288,16 @@ private:
     return llvm::TypeSwitch<Operation *, Value>(redOp)
         .Case<arith::AddFOp, arith::MulFOp>([&](auto redOp) {
           if (convertLhsToF32Precision) {
-            lhs = b.create<arith::ExtFOp>(loc, Float32Type::get(b.getContext()),
-                                          lhs);
+            lhs = arith::ExtFOp::create(b, loc,
+                                        Float32Type::get(b.getContext()), lhs);
           }
-          return b.create<decltype(redOp)>(loc, lhs, rhs);
+          return decltype(redOp)::create(b, loc, lhs, rhs);
         })
         .Case<arith::AddIOp, arith::AndIOp, arith::XOrIOp, arith::MaximumFOp,
               arith::MaxNumFOp, arith::MulIOp, arith::MinimumFOp,
               arith::MinNumFOp, arith::MinSIOp, arith::MinUIOp, arith::MaxSIOp,
               arith::MaxUIOp, arith::OrIOp>([&](auto redOp) {
-          return b.create<decltype(redOp)>(loc, lhs, rhs);
+          return decltype(redOp)::create(b, loc, lhs, rhs);
         })
         .Default([](Operation *op) {
           op->dump();
@@ -1379,40 +1376,40 @@ private:
       // directly instead of EmptyOp so that the subsequent pass can recognize
       // the patterns (EmptyOp is susceptible to being CSE'd away, making it
       // harder to match the patterns correctly).
-      initTensor = rewriter.create<bufferization::AllocTensorOp>(
-          loc, RankedTensorType::get({}, constantType), ValueRange{});
-      initTensor = rewriter.create<tensor::InsertOp>(loc, accBaseConstOp,
-                                                     initTensor, ValueRange{});
+      initTensor = bufferization::AllocTensorOp::create(
+          rewriter, loc, RankedTensorType::get({}, constantType), ValueRange{});
+      initTensor = tensor::InsertOp::create(rewriter, loc, accBaseConstOp,
+                                            initTensor, ValueRange{});
     } else {
-      Value init = rewriter.create<tensor::EmptyOp>(
-          loc, cast<RankedTensorType>(resType).getShape(), constantType);
-      initTensor = rewriter
-                       .create<linalg::FillOp>(loc, ValueRange{accBaseConstOp},
-                                               ValueRange{init})
-                       .result();
+      Value init = tensor::EmptyOp::create(
+          rewriter, loc, cast<RankedTensorType>(resType).getShape(),
+          constantType);
+      initTensor =
+          linalg::FillOp::create(rewriter, loc, ValueRange{accBaseConstOp},
+                                 ValueRange{init})
+              .result();
     }
 
     Value finalResult =
-        rewriter
-            .create<linalg::ReduceOp>(
-                loc, ValueRange{source}, ValueRange{initTensor},
-                SmallVector<int64_t>{axis},
-                [&](OpBuilder &opBuilder, Location loc, ValueRange inputs) {
-                  assert(inputs.size() == 2);
-                  Value result =
-                      getRedElement(inputs[0], inputs[1], loc, rop, opBuilder,
-                                    convertToF32Precision);
-                  opBuilder.create<linalg::YieldOp>(loc, result);
-                })
+        linalg::ReduceOp::create(
+            rewriter, loc, ValueRange{source}, ValueRange{initTensor},
+            SmallVector<int64_t>{axis},
+            [&](OpBuilder &opBuilder, Location loc, ValueRange inputs) {
+              assert(inputs.size() == 2);
+              Value result = getRedElement(inputs[0], inputs[1], loc, rop,
+                                           opBuilder, convertToF32Precision);
+              linalg::YieldOp::create(opBuilder, loc, result);
+            })
             .getResult(0);
 
     if (isVectorReduce) {
       finalResult =
-          rewriter.create<tensor::ExtractOp>(loc, constantType, finalResult);
+          tensor::ExtractOp::create(rewriter, loc, constantType, finalResult);
     }
 
     if (convertToF32Precision) {
-      finalResult = rewriter.create<arith::TruncFOp>(loc, resType, finalResult);
+      finalResult =
+          arith::TruncFOp::create(rewriter, loc, resType, finalResult);
     }
 
     rewriter.replaceOp(op, finalResult);
@@ -1570,10 +1567,9 @@ class ArgMinMaxBaseConverter : public OpConversionPattern<triton::ReduceOp> {
                       ArrayRef<int64_t> shape, Value fillValue,
                       Location loc) const {
     Value initTensor =
-        rewriter.create<tensor::EmptyOp>(loc, shape, fillValue.getType());
-    return rewriter
-        .create<linalg::FillOp>(loc, ValueRange{fillValue},
-                                ValueRange{initTensor})
+        tensor::EmptyOp::create(rewriter, loc, shape, fillValue.getType());
+    return linalg::FillOp::create(rewriter, loc, ValueRange{fillValue},
+                                  ValueRange{initTensor})
         .result();
   }
 
@@ -1648,15 +1644,15 @@ public:
     // the result value to either -inf or +inf depending on
     // whether we're dealing with argmax or argmin
     auto valueType = elemTypes[0];
-    auto valuesAccBaseVal = rewriter.create<arith::ConstantOp>(
-        loc, valueType,
+    auto valuesAccBaseVal = arith::ConstantOp::create(
+        rewriter, loc, valueType,
         rewriter.getFloatAttr(valueType, T::getBaseReductionValue()));
 
     // Set the initial value of the rank-0 tensor containing the index of the
     // min or max value to -1
     auto indexType = elemTypes[1];
-    auto indicesAccBaseVal = rewriter.create<arith::ConstantOp>(
-        loc, indexType, rewriter.getIntegerAttr(indexType, -1));
+    auto indicesAccBaseVal = arith::ConstantOp::create(
+        rewriter, loc, indexType, rewriter.getIntegerAttr(indexType, -1));
 
     // Get the shape of the resulting tensors (both for values and indices). If
     // we are reducing to a single scalar, then the result's type is a tensor of
@@ -1671,8 +1667,8 @@ public:
         getInitTensor(rewriter, reductionResultShape, valuesAccBaseVal, loc),
         getInitTensor(rewriter, reductionResultShape, indicesAccBaseVal, loc)};
 
-    auto linalgOp = rewriter.create<linalg::ReduceOp>(
-        loc, adaptor.getOperands(), outputs,
+    auto linalgOp = linalg::ReduceOp::create(
+        rewriter, loc, adaptor.getOperands(), outputs,
         SmallVector<int64_t>{adaptor.getAxis()},
         [&](OpBuilder &b, Location loc, ValueRange inputs) {
           assert(inputs.size() == 4);
@@ -1690,15 +1686,15 @@ public:
               llvm::map_to_vector(tritonYield->getOperands(), [&](Value val) {
                 return mapping.lookup(val);
               });
-          b.create<linalg::YieldOp>(loc, results);
+          linalg::YieldOp::create(b, loc, results);
         });
 
     if (isScalarReduce) {
       SmallVector<Value> reduceResults{
-          rewriter.create<tensor::ExtractOp>(
-              loc, valueType, linalgOp.getResults()[0], ValueRange{}),
-          rewriter.create<tensor::ExtractOp>(
-              loc, indexType, linalgOp.getResults()[1], ValueRange{})};
+          tensor::ExtractOp::create(rewriter, loc, valueType,
+                                    linalgOp.getResults()[0], ValueRange{}),
+          tensor::ExtractOp::create(rewriter, loc, indexType,
+                                    linalgOp.getResults()[1], ValueRange{})};
       rewriter.replaceOp(op, reduceResults);
     } else {
       rewriter.replaceOp(op, linalgOp);
@@ -1927,8 +1923,9 @@ struct DenseConstantConverter : public OpConversionPattern<arith::ConstantOp> {
     auto splatConst = arith::ConstantOp::materialize(
         rewriter, attr.getSplatValue<Attribute>(), attr.getElementType(), loc);
 
-    auto init = rewriter.create<tensor::EmptyOp>(
-        loc, cast<RankedTensorType>(op.getResult().getType()).getShape(),
+    auto init = tensor::EmptyOp::create(
+        rewriter, loc,
+        cast<RankedTensorType>(op.getResult().getType()).getShape(),
         attr.getElementType());
 
     rewriter.replaceOpWithNewOp<linalg::FillOp>(op, ValueRange{splatConst},
@@ -1996,8 +1993,8 @@ public:
               "= {1, 2} and axis = rank - 1");
     }
 
-    Value init = rewriter.create<tensor::EmptyOp>(op.getLoc(), type.getShape(),
-                                                  type.getElementType());
+    Value init = tensor::EmptyOp::create(rewriter, op.getLoc(), type.getShape(),
+                                         type.getElementType());
 
     rewriter.replaceOpWithNewOp<ttx::CumSumOp>(
         op, input, rewriter.getUI32IntegerAttr(axis), init);
@@ -2032,7 +2029,7 @@ class AddPtrConverter : public OpConversionPattern<triton::AddPtrOp> {
               builder.create(loc, op->getName().getIdentifier(),
                              regionArgs.take_front(op->getNumOperands()),
                              resultTypes, op->getAttrs());
-          builder.create<linalg::YieldOp>(loc, scalarOp->getResults());
+          linalg::YieldOp::create(builder, loc, scalarOp->getResults());
         });
     return success();
   }
@@ -2063,8 +2060,8 @@ class TensorOpConverter : public OpConversionPattern<OpType> {
         rewriter.getMultiDimIdentityMap(rank));
     SmallVector<utils::IteratorType> iteratorTypes(
         rank, utils::IteratorType::parallel);
-    SmallVector<Value> outputs = {rewriter.create<tensor::EmptyOp>(
-        op->getLoc(), resultTensorType.getShape(),
+    SmallVector<Value> outputs = {tensor::EmptyOp::create(
+        rewriter, op->getLoc(), resultTensorType.getShape(),
         resultTensorType.getElementType())};
     rewriter.replaceOpWithNewOp<linalg::GenericOp>(
         op, op->getResultTypes(), op->getOperands(), outputs, indexingMaps,
@@ -2078,7 +2075,7 @@ class TensorOpConverter : public OpConversionPattern<OpType> {
               builder.create(loc, op->getName().getIdentifier(),
                              regionArgs.take_front(op->getNumOperands()),
                              resultTypes, op->getAttrs());
-          builder.create<linalg::YieldOp>(loc, scalarOp->getResults());
+          linalg::YieldOp::create(builder, loc, scalarOp->getResults());
         });
     return success();
   }
@@ -2116,7 +2113,7 @@ class StorePtrToLinalgConverter : public OpConversionPattern<triton::StoreOp> {
               builder.create(loc, op->getName().getIdentifier(),
                              regionArgs.take_front(op->getNumOperands()),
                              resultTypes, op->getAttrs());
-          builder.create<linalg::YieldOp>(loc, scalarOp->getResults());
+          linalg::YieldOp::create(builder, loc, scalarOp->getResults());
         });
     return success();
   }
@@ -2154,8 +2151,8 @@ public:
 
     ArrayRef<int64_t> outputShape = outputType.getShape();
 
-    auto shape = rewriter.create<arith::ConstantOp>(
-        loc, rewriter.getI64TensorAttr(outputShape));
+    auto shape = arith::ConstantOp::create(
+        rewriter, loc, rewriter.getI64TensorAttr(outputShape));
     rewriter.replaceOpWithNewOp<tensor::ReshapeOp>(op, outputType, input,
                                                    shape);
 
diff --git a/include/triton-shared/Conversion/TritonArithToLinalg/ConversionTools.h b/include/triton-shared/Conversion/TritonArithToLinalg/ConversionTools.h
index a0f5047..b3b42c1 100644
--- a/include/triton-shared/Conversion/TritonArithToLinalg/ConversionTools.h
+++ b/include/triton-shared/Conversion/TritonArithToLinalg/ConversionTools.h
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef TRITON_CONVERSION_TRITONARITHTOLINALG_CONVERSIONTOOLS_H
 #define TRITON_CONVERSION_TRITONARITHTOLINALG_CONVERSIONTOOLS_H
 
diff --git a/include/triton-shared/Conversion/TritonArithToLinalg/Passes.h b/include/triton-shared/Conversion/TritonArithToLinalg/Passes.h
index e748c4c..b95cbde 100644
--- a/include/triton-shared/Conversion/TritonArithToLinalg/Passes.h
+++ b/include/triton-shared/Conversion/TritonArithToLinalg/Passes.h
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef TRITON_ARITH_TO_LINALG_CONVERSION_PASSES_H
 #define TRITON_ARITH_TO_LINALG_CONVERSION_PASSES_H
 
diff --git a/include/triton-shared/Conversion/TritonArithToLinalg/Passes.td b/include/triton-shared/Conversion/TritonArithToLinalg/Passes.td
index 1a55303..d1f0f63 100644
--- a/include/triton-shared/Conversion/TritonArithToLinalg/Passes.td
+++ b/include/triton-shared/Conversion/TritonArithToLinalg/Passes.td
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef TRITON_ARITH_TO_LINALG_CONVERSION_PASSES
 #define TRITON_ARITH_TO_LINALG_CONVERSION_PASSES
 
diff --git a/include/triton-shared/Conversion/TritonArithToLinalg/TritonArithToLinalg.h b/include/triton-shared/Conversion/TritonArithToLinalg/TritonArithToLinalg.h
index 45867e4..22dc652 100644
--- a/include/triton-shared/Conversion/TritonArithToLinalg/TritonArithToLinalg.h
+++ b/include/triton-shared/Conversion/TritonArithToLinalg/TritonArithToLinalg.h
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef TRITON_CONVERSION_TRITONARITHTOLINALG_TRITONARITHTOLINALG_H
 #define TRITON_CONVERSION_TRITONARITHTOLINALG_TRITONARITHTOLINALG_H
 
diff --git a/include/triton-shared/Conversion/TritonPtrToMemref/Passes.h b/include/triton-shared/Conversion/TritonPtrToMemref/Passes.h
index f0df4ab..e1f6f33 100644
--- a/include/triton-shared/Conversion/TritonPtrToMemref/Passes.h
+++ b/include/triton-shared/Conversion/TritonPtrToMemref/Passes.h
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef TRITON_PTR_TO_MEMREF_CONVERSION_PASSES_H
 #define TRITON_PTR_TO_MEMREF_CONVERSION_PASSES_H
 
diff --git a/include/triton-shared/Conversion/TritonPtrToMemref/Passes.td b/include/triton-shared/Conversion/TritonPtrToMemref/Passes.td
index 0aaec4c..c027b09 100644
--- a/include/triton-shared/Conversion/TritonPtrToMemref/Passes.td
+++ b/include/triton-shared/Conversion/TritonPtrToMemref/Passes.td
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef TRITON_PTR_TO_MEMREF_CONVERSION_PASSES
 #define TRITON_PTR_TO_MEMREF_CONVERSION_PASSES
 
diff --git a/include/triton-shared/Conversion/TritonPtrToMemref/TritonPtrToMemref.h b/include/triton-shared/Conversion/TritonPtrToMemref/TritonPtrToMemref.h
index c2df315..4476f7d 100644
--- a/include/triton-shared/Conversion/TritonPtrToMemref/TritonPtrToMemref.h
+++ b/include/triton-shared/Conversion/TritonPtrToMemref/TritonPtrToMemref.h
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef TRITON_CONVERSION_TRITON_PTR_TO_MEMREF_TRITON_PTR_TO_MEMREF_H
 #define TRITON_CONVERSION_TRITON_PTR_TO_MEMREF_TRITON_PTR_TO_MEMREF_H
 
diff --git a/include/triton-shared/Conversion/TritonToLinalgExperimental/CollapseShape.h b/include/triton-shared/Conversion/TritonToLinalgExperimental/CollapseShape.h
index 2c44951..064528e 100644
--- a/include/triton-shared/Conversion/TritonToLinalgExperimental/CollapseShape.h
+++ b/include/triton-shared/Conversion/TritonToLinalgExperimental/CollapseShape.h
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/Conversion/TritonToLinalgExperimental/Passes.h b/include/triton-shared/Conversion/TritonToLinalgExperimental/Passes.h
index fa5333f..5aa650a 100644
--- a/include/triton-shared/Conversion/TritonToLinalgExperimental/Passes.h
+++ b/include/triton-shared/Conversion/TritonToLinalgExperimental/Passes.h
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/Conversion/TritonToLinalgExperimental/Passes.td b/include/triton-shared/Conversion/TritonToLinalgExperimental/Passes.td
index 8ffae62..09d6655 100644
--- a/include/triton-shared/Conversion/TritonToLinalgExperimental/Passes.td
+++ b/include/triton-shared/Conversion/TritonToLinalgExperimental/Passes.td
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/Conversion/TritonToLinalgExperimental/ReconcilePtrCasts.h b/include/triton-shared/Conversion/TritonToLinalgExperimental/ReconcilePtrCasts.h
index 01f0229..bea24e8 100644
--- a/include/triton-shared/Conversion/TritonToLinalgExperimental/ReconcilePtrCasts.h
+++ b/include/triton-shared/Conversion/TritonToLinalgExperimental/ReconcilePtrCasts.h
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/Conversion/TritonToLinalgExperimental/TritonToLinalgExperimental.h b/include/triton-shared/Conversion/TritonToLinalgExperimental/TritonToLinalgExperimental.h
index f8a5f06..59b7b23 100644
--- a/include/triton-shared/Conversion/TritonToLinalgExperimental/TritonToLinalgExperimental.h
+++ b/include/triton-shared/Conversion/TritonToLinalgExperimental/TritonToLinalgExperimental.h
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/Conversion/TritonToLinalgExperimental/TritonToPtr.h b/include/triton-shared/Conversion/TritonToLinalgExperimental/TritonToPtr.h
index 8a3831b..305d5aa 100644
--- a/include/triton-shared/Conversion/TritonToLinalgExperimental/TritonToPtr.h
+++ b/include/triton-shared/Conversion/TritonToLinalgExperimental/TritonToPtr.h
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/Conversion/TritonToStructured/Passes.h b/include/triton-shared/Conversion/TritonToStructured/Passes.h
index 31bc380..3c3b81c 100644
--- a/include/triton-shared/Conversion/TritonToStructured/Passes.h
+++ b/include/triton-shared/Conversion/TritonToStructured/Passes.h
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef TRITON_TO_STRUCTURED_CONVERSION_PASSES_H
 #define TRITON_TO_STRUCTURED_CONVERSION_PASSES_H
 
diff --git a/include/triton-shared/Conversion/TritonToStructured/Passes.td b/include/triton-shared/Conversion/TritonToStructured/Passes.td
index 07ad788..ef61f0f 100644
--- a/include/triton-shared/Conversion/TritonToStructured/Passes.td
+++ b/include/triton-shared/Conversion/TritonToStructured/Passes.td
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef TRITON_TO_STRUCTURED_CONVERSION_PASSES
 #define TRITON_TO_STRUCTURED_CONVERSION_PASSES
 
diff --git a/include/triton-shared/Conversion/TritonToStructured/TritonToStructured.h b/include/triton-shared/Conversion/TritonToStructured/TritonToStructured.h
index d919cbb..bbf730e 100644
--- a/include/triton-shared/Conversion/TritonToStructured/TritonToStructured.h
+++ b/include/triton-shared/Conversion/TritonToStructured/TritonToStructured.h
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef TRITON_CONVERSION_TRITONTOSTRUCTURED_TRITONTOSTRUCTURED_H
 #define TRITON_CONVERSION_TRITONTOSTRUCTURED_TRITONTOSTRUCTURED_H
 
diff --git a/include/triton-shared/Conversion/TritonToUnstructured/Passes.h b/include/triton-shared/Conversion/TritonToUnstructured/Passes.h
index d0380b9..a2016c7 100644
--- a/include/triton-shared/Conversion/TritonToUnstructured/Passes.h
+++ b/include/triton-shared/Conversion/TritonToUnstructured/Passes.h
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef TRITON_TO_UNSTRUCTURED_CONVERSION_PASSES_H
 #define TRITON_TO_UNSTRUCTURED_CONVERSION_PASSES_H
 
diff --git a/include/triton-shared/Conversion/TritonToUnstructured/Passes.td b/include/triton-shared/Conversion/TritonToUnstructured/Passes.td
index cd7397c..542d087 100644
--- a/include/triton-shared/Conversion/TritonToUnstructured/Passes.td
+++ b/include/triton-shared/Conversion/TritonToUnstructured/Passes.td
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef TRITON_TO_UNSTRUCTURED_CONVERSION_PASSES
 #define TRITON_TO_UNSTRUCTURED_CONVERSION_PASSES
 
diff --git a/include/triton-shared/Conversion/TritonToUnstructured/TritonToUnstructured.h b/include/triton-shared/Conversion/TritonToUnstructured/TritonToUnstructured.h
index a36a343..03ccdcd 100644
--- a/include/triton-shared/Conversion/TritonToUnstructured/TritonToUnstructured.h
+++ b/include/triton-shared/Conversion/TritonToUnstructured/TritonToUnstructured.h
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef TRITON_CONVERSION_TRITON_TO_UNSTRUCTURED_TRITON_TO_UNSTRUCTURED_H
 #define TRITON_CONVERSION_TRITON_TO_UNSTRUCTURED_TRITON_TO_UNSTRUCTURED_H
 
diff --git a/include/triton-shared/Conversion/UnstructuredToMemref/Passes.h b/include/triton-shared/Conversion/UnstructuredToMemref/Passes.h
index db664ef..f2d7117 100644
--- a/include/triton-shared/Conversion/UnstructuredToMemref/Passes.h
+++ b/include/triton-shared/Conversion/UnstructuredToMemref/Passes.h
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/Conversion/UnstructuredToMemref/Passes.td b/include/triton-shared/Conversion/UnstructuredToMemref/Passes.td
index 4a7b382..a0bf316 100644
--- a/include/triton-shared/Conversion/UnstructuredToMemref/Passes.td
+++ b/include/triton-shared/Conversion/UnstructuredToMemref/Passes.td
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/Conversion/UnstructuredToMemref/UnstructuredToMemref.h b/include/triton-shared/Conversion/UnstructuredToMemref/UnstructuredToMemref.h
index 4ad6d0a..ad0f5c4 100644
--- a/include/triton-shared/Conversion/UnstructuredToMemref/UnstructuredToMemref.h
+++ b/include/triton-shared/Conversion/UnstructuredToMemref/UnstructuredToMemref.h
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/Dialect/TPtr/IR/TPtrDialect.h b/include/triton-shared/Dialect/TPtr/IR/TPtrDialect.h
index f810b40..c7e5933 100644
--- a/include/triton-shared/Dialect/TPtr/IR/TPtrDialect.h
+++ b/include/triton-shared/Dialect/TPtr/IR/TPtrDialect.h
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef MLIR_DIALECT_TPTR_IR_TPTR_DIALECT_H_
 #define MLIR_DIALECT_TPTR_IR_TPTR_DIALECT_H_
 
diff --git a/include/triton-shared/Dialect/TPtr/IR/TPtrDialect.td b/include/triton-shared/Dialect/TPtr/IR/TPtrDialect.td
index f724f78..04d894f 100644
--- a/include/triton-shared/Dialect/TPtr/IR/TPtrDialect.td
+++ b/include/triton-shared/Dialect/TPtr/IR/TPtrDialect.td
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef TPTR_DIALECT
 #define TPTR_DIALECT
 
diff --git a/include/triton-shared/Dialect/TritonStructured/IR/TritonStructuredDialect.h b/include/triton-shared/Dialect/TritonStructured/IR/TritonStructuredDialect.h
index af72fbe..fbead0c 100644
--- a/include/triton-shared/Dialect/TritonStructured/IR/TritonStructuredDialect.h
+++ b/include/triton-shared/Dialect/TritonStructured/IR/TritonStructuredDialect.h
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef MLIR_DIALECT_TRITON_STRUCTURED_IR_TRITON_STRUCTURED_DIALECT_H_
 #define MLIR_DIALECT_TRITON_STRUCTURED_IR_TRITON_STRUCTURED_DIALECT_H_
 
diff --git a/include/triton-shared/Dialect/TritonStructured/IR/TritonStructuredDialect.td b/include/triton-shared/Dialect/TritonStructured/IR/TritonStructuredDialect.td
index e5c81b2..17c3ce9 100644
--- a/include/triton-shared/Dialect/TritonStructured/IR/TritonStructuredDialect.td
+++ b/include/triton-shared/Dialect/TritonStructured/IR/TritonStructuredDialect.td
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef TRITON_STRUCTURED_DIALECT
 #define TRITON_STRUCTURED_DIALECT
 
diff --git a/include/triton-shared/Dialect/TritonTilingExt/IR/TritonTilingExtDialect.h b/include/triton-shared/Dialect/TritonTilingExt/IR/TritonTilingExtDialect.h
index 5cf8492..53e031d 100644
--- a/include/triton-shared/Dialect/TritonTilingExt/IR/TritonTilingExtDialect.h
+++ b/include/triton-shared/Dialect/TritonTilingExt/IR/TritonTilingExtDialect.h
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/Dialect/TritonTilingExt/IR/TritonTilingExtInterfaces.td b/include/triton-shared/Dialect/TritonTilingExt/IR/TritonTilingExtInterfaces.td
index 567d95e..e74fbb6 100644
--- a/include/triton-shared/Dialect/TritonTilingExt/IR/TritonTilingExtInterfaces.td
+++ b/include/triton-shared/Dialect/TritonTilingExt/IR/TritonTilingExtInterfaces.td
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/Dialect/TritonTilingExt/IR/TritonTilingExtOps.td b/include/triton-shared/Dialect/TritonTilingExt/IR/TritonTilingExtOps.td
index d589aab..d3a4268 100644
--- a/include/triton-shared/Dialect/TritonTilingExt/IR/TritonTilingExtOps.td
+++ b/include/triton-shared/Dialect/TritonTilingExt/IR/TritonTilingExtOps.td
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/Transform/AddLLVMDebugInfo/AddLLVMDebugInfo.h b/include/triton-shared/Transform/AddLLVMDebugInfo/AddLLVMDebugInfo.h
index d2fc8f4..e350e8a 100644
--- a/include/triton-shared/Transform/AddLLVMDebugInfo/AddLLVMDebugInfo.h
+++ b/include/triton-shared/Transform/AddLLVMDebugInfo/AddLLVMDebugInfo.h
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/Transform/AddLLVMDebugInfo/Passes.h b/include/triton-shared/Transform/AddLLVMDebugInfo/Passes.h
index 69b127e..9c94ac4 100644
--- a/include/triton-shared/Transform/AddLLVMDebugInfo/Passes.h
+++ b/include/triton-shared/Transform/AddLLVMDebugInfo/Passes.h
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/Transform/AddLLVMDebugInfo/Passes.td b/include/triton-shared/Transform/AddLLVMDebugInfo/Passes.td
index 1d606d7..e391eb2 100644
--- a/include/triton-shared/Transform/AddLLVMDebugInfo/Passes.td
+++ b/include/triton-shared/Transform/AddLLVMDebugInfo/Passes.td
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/include/triton-shared/Utils/Utils.h b/include/triton-shared/Utils/Utils.h
index dfc693a..e484bee 100644
--- a/include/triton-shared/Utils/Utils.h
+++ b/include/triton-shared/Utils/Utils.h
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #ifndef TRITON_SHARED_UTILITY_H
 #define TRITON_SHARED_UTILITY_H
 
diff --git a/lib/Analysis/MaskAnalysis.cpp b/lib/Analysis/MaskAnalysis.cpp
index 10794bb..5b84859 100644
--- a/lib/Analysis/MaskAnalysis.cpp
+++ b/lib/Analysis/MaskAnalysis.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
@@ -65,11 +65,10 @@ tensor::ExtractSliceOp MaskState::getExtractSlice(Value source,
   SmallVector<OpFoldResult> offsets(getRank(), builder.getIndexAttr(0));
   SmallVector<OpFoldResult> strides(getRank(), builder.getIndexAttr(1));
 
-  auto dstType = tensor::ExtractSliceOp::inferResultType(sourceType, offsets,
-                                                         dims, strides);
+  auto dstType = tensor::ExtractSliceOp::inferResultType(sourceType, dims);
 
-  return builder.create<tensor::ExtractSliceOp>(loc, dstType, source, offsets,
-                                                dims, strides);
+  return tensor::ExtractSliceOp::create(builder, loc, dstType, source, offsets,
+                                        dims, strides);
 }
 
 memref::SubViewOp MaskState::getSubview(Value source, const Location loc,
@@ -80,8 +79,8 @@ memref::SubViewOp MaskState::getSubview(Value source, const Location loc,
   auto dstType =
       memref::SubViewOp::inferResultType(sourceType, offsets, dims, strides);
 
-  return builder.create<memref::SubViewOp>(loc, cast<MemRefType>(dstType),
-                                           source, offsets, dims, strides);
+  return memref::SubViewOp::create(builder, loc, cast<MemRefType>(dstType),
+                                   source, offsets, dims, strides);
 }
 
 static memref::SubViewOp createSubview(Value src, Location loc, OpBuilder &b,
@@ -91,8 +90,8 @@ static memref::SubViewOp createSubview(Value src, Location loc, OpBuilder &b,
   auto srcType = cast<MemRefType>(src.getType());
   auto dstType =
       memref::SubViewOp::inferResultType(srcType, offsets, sizes, strides);
-  return b.create<memref::SubViewOp>(loc, cast<MemRefType>(dstType), src,
-                                     offsets, sizes, strides);
+  return memref::SubViewOp::create(b, loc, cast<MemRefType>(dstType), src,
+                                   offsets, sizes, strides);
 }
 
 // Assume block1 wraps around and the remainder is block2.
@@ -155,7 +154,8 @@ MaskState::getSideBySideSubviews(Value block1, Value block2, const Location loc,
                                  OpBuilder &builder) const {
   OpFoldResult subviewRowFull = dims[0];
   OpFoldResult subviewColFull = dims[1];
-  OpFoldResult col1 = builder.create<memref::DimOp>(loc, block1, 1).getResult();
+  OpFoldResult col1 =
+      memref::DimOp::create(builder, loc, block1, 1).getResult();
   OpFoldResult subviewCol1 = minOFRs(col1, subviewColFull, loc, builder);
   OpFoldResult subviewCol2 = subOFRs(subviewColFull, subviewCol1, loc, builder);
 
@@ -174,7 +174,8 @@ MaskState::getStackedSubviews(Value block1, Value block2, const Location loc,
                               OpBuilder &builder) const {
   OpFoldResult subviewRowFull = dims[0];
   OpFoldResult subviewColFull = dims[1];
-  OpFoldResult row1 = builder.create<memref::DimOp>(loc, block1, 0).getResult();
+  OpFoldResult row1 =
+      memref::DimOp::create(builder, loc, block1, 0).getResult();
   OpFoldResult subviewRow1 = minOFRs(row1, subviewRowFull, loc, builder);
   OpFoldResult subviewRow2 = subOFRs(subviewRowFull, subviewRow1, loc, builder);
 
@@ -314,8 +315,8 @@ LogicalResult MaskState::parseIntScalar(Value scalar, const Location loc,
   if (scalar.getType().isInteger(1)) {
     this->scalar = scalar;
   } else {
-    auto castOp =
-        builder.create<arith::IndexCastOp>(loc, builder.getIndexType(), scalar);
+    auto castOp = arith::IndexCastOp::create(builder, loc,
+                                             builder.getIndexType(), scalar);
     this->scalar = castOp.getResult();
   }
   return success();
@@ -407,18 +408,17 @@ LogicalResult MaskState::parseAnd(arith::AndIOp andOp, const Location loc,
             }
             auto targetTensorType =
                 RankedTensorType::get({size}, builder.getI32Type());
-            Value range =
-                builder
-                    .create<triton::MakeRangeOp>(loc, targetTensorType, 0, size)
-                    .getResult();
+            Value range = triton::MakeRangeOp::create(builder, loc,
+                                                      targetTensorType, 0, size)
+                              .getResult();
             Value v = ofrToIndexValue(ofr, loc, builder);
-            v = builder
-                    .create<arith::IndexCastUIOp>(loc, builder.getI32Type(), v)
+            v = arith::IndexCastUIOp::create(builder, loc, builder.getI32Type(),
+                                             v)
                     .getResult();
-            v = builder.create<triton::SplatOp>(loc, targetTensorType, v)
+            v = triton::SplatOp::create(builder, loc, targetTensorType, v)
                     .getResult();
-            return builder
-                .create<arith::CmpIOp>(loc, arith::CmpIPredicate::ult, range, v)
+            return arith::CmpIOp::create(builder, loc,
+                                         arith::CmpIPredicate::ult, range, v)
                 .getResult();
           };
           if (!lhsV) {
@@ -436,7 +436,7 @@ LogicalResult MaskState::parseAnd(arith::AndIOp andOp, const Location loc,
             continue;
           }
           // And the mask.
-          masks.push_back(builder.create<arith::AndIOp>(loc, lhsV, rhsV));
+          masks.push_back(arith::AndIOp::create(builder, loc, lhsV, rhsV));
         }
       }
       // Only support one unstructured mask.
@@ -494,8 +494,8 @@ LogicalResult MaskState::parseCmp(arith::CmpIOp cmpOp, const Location loc,
         SmallVector<ReassociationIndices> reassociation =
             *maybeReassociationMap;
         // Set masks.
-        unstructuredMask = builder.create<tensor::CollapseShapeOp>(
-            loc, flatType, cmpOp, reassociation);
+        unstructuredMask = tensor::CollapseShapeOp::create(
+            builder, loc, flatType, cmpOp, reassociation);
       }
       masks[cmpOpDim] = unstructuredMask;
     }
diff --git a/lib/Analysis/OpFoldResultUtils.cpp b/lib/Analysis/OpFoldResultUtils.cpp
index 6c1b5ca..85d7842 100644
--- a/lib/Analysis/OpFoldResultUtils.cpp
+++ b/lib/Analysis/OpFoldResultUtils.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation, Meta Platforms.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
@@ -59,7 +59,7 @@ Value ofrToValue(const OpFoldResult ofr, const Location loc, OpBuilder &b) {
 
   auto attr = dyn_cast<Attribute>(ofr);
   auto typedAttr = dyn_cast<TypedAttr>(attr);
-  return b.create<arith::ConstantOp>(loc, typedAttr);
+  return arith::ConstantOp::create(b, loc, typedAttr);
 }
 
 Value ofrToIndexValue(const OpFoldResult ofr, const Location loc,
@@ -67,14 +67,14 @@ Value ofrToIndexValue(const OpFoldResult ofr, const Location loc,
   if (Value val = dyn_cast<Value>(ofr)) {
     assert(val.getType().isIntOrIndex());
     if (!val.getType().isIndex()) {
-      val = b.create<arith::IndexCastOp>(loc, b.getIndexType(), val);
+      val = arith::IndexCastOp::create(b, loc, b.getIndexType(), val);
     }
     return val;
   }
 
   auto intVal = getIntAttr(ofr);
   if (intVal.has_value()) {
-    return b.create<arith::ConstantOp>(loc, b.getIndexAttr(intVal.value()));
+    return arith::ConstantOp::create(b, loc, b.getIndexAttr(intVal.value()));
   }
   llvm_unreachable("Unexpected OpFoldResult state");
   return nullptr;
@@ -93,14 +93,14 @@ Value indexTypeCast(Value v, Type targetTy, const Location loc, OpBuilder &b) {
   if (isa<IndexType>(targetTy) || isa<IndexType>(ty)) {
     assert((isa<IntegerType>(targetTy) || isa<IntegerType>(ty)) &&
            "Only cast between index type and integer type");
-    return b.create<arith::IndexCastOp>(loc, targetTy, v).getResult();
+    return arith::IndexCastOp::create(b, loc, targetTy, v).getResult();
   } else {
     auto targetIntTy = cast<IntegerType>(targetTy);
     auto intTy = cast<IntegerType>(ty);
     if (targetIntTy.getWidth() > intTy.getWidth())
-      return b.create<arith::ExtSIOp>(loc, targetTy, v).getResult();
+      return arith::ExtSIOp::create(b, loc, targetTy, v).getResult();
     else
-      return b.create<arith::TruncIOp>(loc, targetTy, v).getResult();
+      return arith::TruncIOp::create(b, loc, targetTy, v).getResult();
   }
 }
 
@@ -114,8 +114,8 @@ OpFoldResult expandOFRIndex(OpFoldResult ofr, OpFoldResult targetForTy,
 
   Value v = dyn_cast<Value>(ofr);
   if (!v)
-    v = b.create<arith::ConstantOp>(loc,
-                                    cast<IntegerAttr>(cast<Attribute>(ofr)));
+    v = arith::ConstantOp::create(b, loc,
+                                  cast<IntegerAttr>(cast<Attribute>(ofr)));
 
   Type ty = v.getType();
   if (targetTy == ty)
@@ -127,7 +127,7 @@ OpFoldResult expandOFRIndex(OpFoldResult ofr, OpFoldResult targetForTy,
     // cast to target element type first.
     if (targetEltTy != ty)
       v = indexTypeCast(v, targetEltTy, loc, b);
-    return b.create<triton::SplatOp>(loc, targetTy, v).getResult();
+    return triton::SplatOp::create(b, loc, targetTy, v).getResult();
   } else if (targetShapedTy && shapedTy) {
     Type targetEltTy = targetShapedTy.getElementType();
     Type eltTy = shapedTy.getElementType();
@@ -148,26 +148,26 @@ OpFoldResult expandOFRIndex(OpFoldResult ofr, OpFoldResult targetForTy,
       SmallVector<Value> shapeValues;
       for (auto dim : targetShapedTy.getShape()) {
         shapeValues.push_back(
-            b.create<arith::ConstantOp>(loc, b.getIndexAttr(dim)));
+            arith::ConstantOp::create(b, loc, b.getIndexAttr(dim)));
       }
       RankedTensorType targetShapeTensorTy = RankedTensorType::get(
           targetShapedTy.getShape().size(), b.getIndexType());
-      auto shapeTensor = b.create<tensor::FromElementsOp>(
-          loc, targetShapeTensorTy, shapeValues);
-      return b.create<triton::ReshapeOp>(loc, targetTy, v, shapeTensor)
+      auto shapeTensor = tensor::FromElementsOp::create(
+          b, loc, targetShapeTensorTy, shapeValues);
+      return triton::ReshapeOp::create(b, loc, targetTy, v, shapeTensor)
           .getResult();
     }
     if (isa<IndexType>(targetEltTy) || isa<IndexType>(eltTy)) {
       assert((isa<IntegerType>(targetEltTy) || isa<IntegerType>(eltTy)) &&
              "Only cast between index type and integer type");
-      return b.create<arith::IndexCastOp>(loc, targetTy, v).getResult();
+      return arith::IndexCastOp::create(b, loc, targetTy, v).getResult();
     } else {
       auto targetIntTy = cast<IntegerType>(targetEltTy);
       auto intTy = cast<IntegerType>(eltTy);
       if (targetIntTy.getWidth() > intTy.getWidth())
-        return b.create<arith::ExtSIOp>(loc, targetTy, v).getResult();
+        return arith::ExtSIOp::create(b, loc, targetTy, v).getResult();
       else
-        return b.create<arith::TruncIOp>(loc, targetTy, v).getResult();
+        return arith::TruncIOp::create(b, loc, targetTy, v).getResult();
     }
   } else {
     assert(!shapedTy && "src type rank should be >= target type rank");
@@ -194,18 +194,18 @@ OpFoldResult addOFRs(const OpFoldResult lhs, const OpFoldResult rhs,
   auto lhsValue = dyn_cast<Value>(lhs);
   if (lhsIntAttr) {
     auto lhsOp =
-        b.create<arith::ConstantOp>(loc, b.getIndexAttr(lhsIntAttr.value()));
+        arith::ConstantOp::create(b, loc, b.getIndexAttr(lhsIntAttr.value()));
     lhsValue = lhsOp.getResult();
   }
 
   auto rhsValue = dyn_cast<Value>(rhs);
   if (rhsIntAttr) {
     auto rhsOp =
-        b.create<arith::ConstantOp>(loc, b.getIndexAttr(rhsIntAttr.value()));
+        arith::ConstantOp::create(b, loc, b.getIndexAttr(rhsIntAttr.value()));
     rhsValue = rhsOp.getResult();
   }
 
-  return b.create<arith::AddIOp>(loc, lhsValue, rhsValue).getResult();
+  return arith::AddIOp::create(b, loc, lhsValue, rhsValue).getResult();
 }
 
 OpFoldResult subOFRs(const OpFoldResult lhs, const OpFoldResult rhs,
@@ -225,18 +225,18 @@ OpFoldResult subOFRs(const OpFoldResult lhs, const OpFoldResult rhs,
   auto lhsValue = dyn_cast<Value>(lhs);
   if (lhsIntAttr) {
     auto lhsOp =
-        b.create<arith::ConstantOp>(loc, b.getIndexAttr(lhsIntAttr.value()));
+        arith::ConstantOp::create(b, loc, b.getIndexAttr(lhsIntAttr.value()));
     lhsValue = lhsOp.getResult();
   }
 
   auto rhsValue = dyn_cast<Value>(rhs);
   if (rhsIntAttr) {
     auto rhsOp =
-        b.create<arith::ConstantOp>(loc, b.getIndexAttr(rhsIntAttr.value()));
+        arith::ConstantOp::create(b, loc, b.getIndexAttr(rhsIntAttr.value()));
     rhsValue = rhsOp.getResult();
   }
 
-  auto sumOp = b.create<arith::SubIOp>(loc, lhsValue, rhsValue);
+  auto sumOp = arith::SubIOp::create(b, loc, lhsValue, rhsValue);
   return sumOp.getResult();
 }
 
@@ -280,17 +280,17 @@ OpFoldResult mulOFRs(const OpFoldResult lhs, const OpFoldResult rhs,
   // otherwise, need to create instructions to calculate new attribute value
   if (lhsIntAttr) {
     auto lhsOp =
-        b.create<arith::ConstantOp>(loc, b.getIndexAttr(lhsIntAttr.value()));
+        arith::ConstantOp::create(b, loc, b.getIndexAttr(lhsIntAttr.value()));
     lhsValue = lhsOp.getResult();
   }
 
   if (rhsIntAttr) {
     auto rhsOp =
-        b.create<arith::ConstantOp>(loc, b.getIndexAttr(rhsIntAttr.value()));
+        arith::ConstantOp::create(b, loc, b.getIndexAttr(rhsIntAttr.value()));
     rhsValue = rhsOp.getResult();
   }
 
-  return b.create<arith::MulIOp>(loc, lhsValue, rhsValue).getResult();
+  return arith::MulIOp::create(b, loc, lhsValue, rhsValue).getResult();
 }
 
 OpFoldResult minOFRs(const OpFoldResult lhs, const OpFoldResult rhs,
@@ -306,18 +306,18 @@ OpFoldResult minOFRs(const OpFoldResult lhs, const OpFoldResult rhs,
   auto lhsValue = dyn_cast<Value>(lhs);
   if (lhsIntAttr) {
     auto lhsOp =
-        b.create<arith::ConstantOp>(loc, b.getIndexAttr(lhsIntAttr.value()));
+        arith::ConstantOp::create(b, loc, b.getIndexAttr(lhsIntAttr.value()));
     lhsValue = lhsOp.getResult();
   }
 
   auto rhsValue = dyn_cast<Value>(rhs);
   if (rhsIntAttr) {
     auto rhsOp =
-        b.create<arith::ConstantOp>(loc, b.getIndexAttr(rhsIntAttr.value()));
+        arith::ConstantOp::create(b, loc, b.getIndexAttr(rhsIntAttr.value()));
     rhsValue = rhsOp.getResult();
   }
 
-  auto minOp = b.create<arith::MinSIOp>(loc, lhsValue, rhsValue);
+  auto minOp = arith::MinSIOp::create(b, loc, lhsValue, rhsValue);
   return minOp.getResult();
 }
 
@@ -334,18 +334,18 @@ OpFoldResult maxOFRs(const OpFoldResult lhs, const OpFoldResult rhs,
   auto lhsValue = dyn_cast<Value>(lhs);
   if (lhsIntAttr) {
     auto lhsOp =
-        b.create<arith::ConstantOp>(loc, b.getIndexAttr(lhsIntAttr.value()));
+        arith::ConstantOp::create(b, loc, b.getIndexAttr(lhsIntAttr.value()));
     lhsValue = lhsOp.getResult();
   }
 
   auto rhsValue = dyn_cast<Value>(rhs);
   if (rhsIntAttr) {
     auto rhsOp =
-        b.create<arith::ConstantOp>(loc, b.getIndexAttr(rhsIntAttr.value()));
+        arith::ConstantOp::create(b, loc, b.getIndexAttr(rhsIntAttr.value()));
     rhsValue = rhsOp.getResult();
   }
 
-  auto maxOp = b.create<arith::MaxSIOp>(loc, lhsValue, rhsValue);
+  auto maxOp = arith::MaxSIOp::create(b, loc, lhsValue, rhsValue);
   return maxOp.getResult();
 }
 
@@ -359,7 +359,7 @@ OpFoldResult selectOFRs(const OpFoldResult condOFR, const OpFoldResult trueOFR,
          "Condition for selectOp must be a bool type");
 
   auto selectOp =
-      b.create<arith::SelectOp>(loc, condValue, trueValue, falseValue);
+      arith::SelectOp::create(b, loc, condValue, trueValue, falseValue);
   return selectOp.getResult();
 }
 
@@ -398,7 +398,7 @@ OpFoldResult compareOFRs(const OpFoldResult lhs, const OpFoldResult rhs,
   auto lhsValue = ofrToIndexValue(lhs, loc, b);
   auto rhsValue = ofrToIndexValue(rhs, loc, b);
 
-  auto cmpOp = b.create<arith::CmpIOp>(loc, pred, lhsValue, rhsValue);
+  auto cmpOp = arith::CmpIOp::create(b, loc, pred, lhsValue, rhsValue);
   return selectOFRs(cmpOp.getResult(), trueOFR, falseOFR, loc, b);
 }
 
diff --git a/lib/Analysis/PtrAnalysis.cpp b/lib/Analysis/PtrAnalysis.cpp
index 9ebd54d..1a17567 100644
--- a/lib/Analysis/PtrAnalysis.cpp
+++ b/lib/Analysis/PtrAnalysis.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
@@ -82,7 +82,7 @@ void PtrState::addState(const PtrState &lhsState, const PtrState &rhsState,
 
   if (lhsState.scalar && rhsState.scalar) {
     auto addOp =
-        rewriter.create<arith::AddIOp>(loc, lhsState.scalar, rhsState.scalar);
+        arith::AddIOp::create(rewriter, loc, lhsState.scalar, rhsState.scalar);
     scalar = addOp.getResult();
   } else if (lhsState.getRank() == 0) { // both lhs and rhs are scalars
     scalar = lhsState.scalar ? lhsState.scalar : rhsState.scalar;
@@ -210,28 +210,28 @@ PtrState::createStackedCastOps(ArrayRef<int64_t> resultShape,
   Value strideRow = ofrToIndexValue(strides[0], loc, rewriter);
   Value strideCol = ofrToIndexValue(strides[1], loc, rewriter);
 
-  Value modRow = rewriter.create<arith::IndexCastOp>(
-      loc, rewriter.getIndexType(), modulos[0]->size);
+  Value modRow = arith::IndexCastOp::create(
+      rewriter, loc, rewriter.getIndexType(), modulos[0]->size);
 
   // First chunk
   Value wrappedAroundOff =
-      rewriter.create<arith::RemSIOp>(loc, targetOffset, strideRow);
-  Value clampedOff = rewriter.create<arith::MulIOp>(loc, modRow, strideRow);
+      arith::RemSIOp::create(rewriter, loc, targetOffset, strideRow);
+  Value clampedOff = arith::MulIOp::create(rewriter, loc, modRow, strideRow);
   clampedOff =
-      rewriter.create<arith::AddIOp>(loc, clampedOff, wrappedAroundOff);
-  Value d1 = rewriter.create<arith::SubIOp>(loc, clampedOff, targetOffset);
-  d1 = rewriter.create<arith::DivSIOp>(loc, d1, strideRow);
+      arith::AddIOp::create(rewriter, loc, clampedOff, wrappedAroundOff);
+  Value d1 = arith::SubIOp::create(rewriter, loc, clampedOff, targetOffset);
+  d1 = arith::DivSIOp::create(rewriter, loc, d1, strideRow);
 
   SmallVector<Value> sizes1{d1, colSize};
-  memref::ReinterpretCastOp cast1 = rewriter.create<memref::ReinterpretCastOp>(
-      loc, resultType, source, targetOffset, sizes1,
+  memref::ReinterpretCastOp cast1 = memref::ReinterpretCastOp::create(
+      rewriter, loc, resultType, source, targetOffset, sizes1,
       ValueRange{strideRow, strideCol});
 
   // Second chunk
-  Value d2 = rewriter.create<arith::SubIOp>(loc, rowSize, d1);
+  Value d2 = arith::SubIOp::create(rewriter, loc, rowSize, d1);
   SmallVector<Value> sizes2{d2, colSize};
-  memref::ReinterpretCastOp cast2 = rewriter.create<memref::ReinterpretCastOp>(
-      loc, resultType, source, wrappedAroundOff, sizes2,
+  memref::ReinterpretCastOp cast2 = memref::ReinterpretCastOp::create(
+      rewriter, loc, resultType, source, wrappedAroundOff, sizes2,
       ValueRange{strideRow, strideCol});
 
   return {cast1, cast2};
@@ -299,29 +299,29 @@ PtrState::createSideBySideCastOps(ArrayRef<int64_t> resultShape,
   Value rowSize = ofrToIndexValue(sizes[0], loc, rewriter);
   Value colSize = ofrToIndexValue(sizes[1], loc, rewriter);
 
-  Value modN = rewriter.create<arith::IndexCastOp>(loc, rewriter.getIndexType(),
-                                                   modulos[1]->size);
+  Value modN = arith::IndexCastOp::create(
+      rewriter, loc, rewriter.getIndexType(), modulos[1]->size);
 
-  Value x = rewriter.create<arith::RemSIOp>(loc, targetOffset, modN);
-  Value y = rewriter.create<arith::SubIOp>(loc, targetOffset, x);
+  Value x = arith::RemSIOp::create(rewriter, loc, targetOffset, modN);
+  Value y = arith::SubIOp::create(rewriter, loc, targetOffset, x);
 
   SmallVector<Value> strideVals = ofrsToIndexValues(strides, loc, rewriter);
 
   // First chunk
-  Value nextOffset = rewriter.create<arith::AddIOp>(loc, x, colSize);
-  Value clampedOffset = rewriter.create<arith::MinSIOp>(loc, nextOffset, modN);
-  Value d1 = rewriter.create<arith::SubIOp>(loc, clampedOffset, x);
+  Value nextOffset = arith::AddIOp::create(rewriter, loc, x, colSize);
+  Value clampedOffset = arith::MinSIOp::create(rewriter, loc, nextOffset, modN);
+  Value d1 = arith::SubIOp::create(rewriter, loc, clampedOffset, x);
   SmallVector<Value> sizes1{rowSize, d1};
 
-  auto cast1 = rewriter.create<memref::ReinterpretCastOp>(
-      loc, resultType, source, targetOffset, sizes1, strideVals);
+  auto cast1 = memref::ReinterpretCastOp::create(
+      rewriter, loc, resultType, source, targetOffset, sizes1, strideVals);
 
   // Second chunk
-  Value d2 = rewriter.create<arith::SubIOp>(loc, colSize, d1);
+  Value d2 = arith::SubIOp::create(rewriter, loc, colSize, d1);
   SmallVector<Value> sizes2{rowSize, d2};
 
-  auto cast2 = rewriter.create<memref::ReinterpretCastOp>(
-      loc, resultType, source, y, sizes2, strideVals);
+  auto cast2 = memref::ReinterpretCastOp::create(rewriter, loc, resultType,
+                                                 source, y, sizes2, strideVals);
 
   return {cast1, cast2};
 }
@@ -341,8 +341,8 @@ PtrState::createCastOp(ArrayRef<int64_t> resultShape, const Location loc,
       getResultMemrefType(rewriter.getContext(), staticOffset[0], resultShape);
 
   // Create reinterpret cast
-  return rewriter.create<memref::ReinterpretCastOp>(
-      loc, resultType, source, targetOffset, sizes, strides);
+  return memref::ReinterpretCastOp::create(rewriter, loc, resultType, source,
+                                           targetOffset, sizes, strides);
 }
 
 void PtrAnalysis::visitOperandAdd(
@@ -648,8 +648,8 @@ void PtrAnalysis::visitOperand(
   }
 
   if (isa<IntegerType>(operand.getType())) {
-    auto castOp = rewriter.create<arith::IndexCastOp>(
-        loc, rewriter.getIndexType(), operand);
+    auto castOp = arith::IndexCastOp::create(rewriter, loc,
+                                             rewriter.getIndexType(), operand);
     state.scalar = castOp.getResult();
     return;
   }
@@ -805,10 +805,10 @@ void PtrAnalysis::rewriteAddptrOp(
         rewriter.getContext(), ShapedType::kDynamic, resultShape);
 
     UnrealizedConversionCastOp combinedCast =
-        rewriter.create<UnrealizedConversionCastOp>(
-            op.getLoc(), resultType,
-            ValueRange{casts[0].getResult(), casts[1].getResult(),
-                       op.getResult()});
+        UnrealizedConversionCastOp::create(rewriter, op.getLoc(), resultType,
+                                           ValueRange{casts[0].getResult(),
+                                                      casts[1].getResult(),
+                                                      op.getResult()});
 
     combinedCast->setAttr(ModuloState::WraparoundAttr,
                           rewriter.getStringAttr(type));
@@ -858,18 +858,18 @@ void PtrAnalysis::rewriteAdvanceOp(
        llvm::zip(incrementOffsets, ptrState.offsets, ptrState.strides)) {
     Value offsetValue;
     if (auto offsetIntAttr = getIntAttr(offset)) {
-      auto constOp = rewriter.create<arith::ConstantOp>(
-          op.getLoc(), rewriter.getIndexAttr(0));
+      auto constOp = arith::ConstantOp::create(rewriter, op.getLoc(),
+                                               rewriter.getIndexAttr(0));
       offsetValue = constOp.getResult();
     } else {
       offsetValue = cast<Value>(offset);
     }
-    auto castOp = rewriter.create<arith::IndexCastOp>(
-        loc, rewriter.getIndexType(), increment);
-    auto mulOp = rewriter.create<arith::MulIOp>(loc, castOp.getResult(),
-                                                cast<Value>(stride));
+    auto castOp = arith::IndexCastOp::create(
+        rewriter, loc, rewriter.getIndexType(), increment);
+    auto mulOp = arith::MulIOp::create(rewriter, loc, castOp.getResult(),
+                                       cast<Value>(stride));
     auto addOp =
-        rewriter.create<arith::AddIOp>(loc, mulOp.getResult(), offsetValue);
+        arith::AddIOp::create(rewriter, loc, mulOp.getResult(), offsetValue);
     newOffsets.push_back(addOp.getResult());
   }
 
@@ -995,8 +995,8 @@ void PtrAnalysis::rewriteYieldOp(
       // zeroes.
       if (auto sIntAttr = getIntAttr(s)) {
         assert(sIntAttr.value() == 0 && "attribute offsets should be zeroes");
-        auto constOp = rewriter.create<arith::ConstantOp>(
-            op.getLoc(), rewriter.getIndexAttr(0));
+        auto constOp = arith::ConstantOp::create(rewriter, op.getLoc(),
+                                                 rewriter.getIndexAttr(0));
         operands.push_back(constOp.getResult());
       } else {
         operands.push_back(cast<Value>(s));
@@ -1166,8 +1166,8 @@ void PtrAnalysis::rewriteForOp(
     for (auto [j, s] : llvm::enumerate(state.offsets)) {
       auto sIntAttr = getIntAttr(s);
       if (sIntAttr) {
-        auto constOp = rewriter.create<arith::ConstantOp>(
-            op.getLoc(), rewriter.getIndexAttr(sIntAttr.value()));
+        auto constOp = arith::ConstantOp::create(
+            rewriter, op.getLoc(), rewriter.getIndexAttr(sIntAttr.value()));
         newInitArgs.push_back(constOp.getResult());
         state.offsets[j] = constOp.getResult();
       } else {
@@ -1178,8 +1178,8 @@ void PtrAnalysis::rewriteForOp(
     for (auto [j, s] : llvm::enumerate(state.strides)) {
       auto sIntAttr = getIntAttr(s);
       if (sIntAttr) {
-        auto constOp = rewriter.create<arith::ConstantOp>(
-            op.getLoc(), rewriter.getIndexAttr(sIntAttr.value()));
+        auto constOp = arith::ConstantOp::create(
+            rewriter, op.getLoc(), rewriter.getIndexAttr(sIntAttr.value()));
         newInitArgs.push_back(constOp.getResult());
         state.strides[j] = constOp.getResult();
       } else {
@@ -1240,9 +1240,10 @@ void PtrAnalysis::rewriteForOp(
   rewriter.restoreInsertionPoint(origIp);
 
   // Create a new scf::ForOp that uses updated init args and same loop body
-  auto newOp = rewriter.create<scf::ForOp>(
-      op.getLoc(), op.getLowerBound(), op.getUpperBound(), op.getStep(),
-      newInitArgs, [&](OpBuilder &b, Location loc, Value iv, ValueRange args) {
+  auto newOp = scf::ForOp::create(
+      rewriter, op.getLoc(), op.getLowerBound(), op.getUpperBound(),
+      op.getStep(), newInitArgs,
+      [&](OpBuilder &b, Location loc, Value iv, ValueRange args) {
         IRMapping mapping;
         mapping.map(op.getInductionVar(), iv);
         mapping.map(op.getInitArgs(), newInitArgs);
@@ -1266,7 +1267,7 @@ void PtrAnalysis::rewriteForOp(
         b.setInsertionPointToStart(b.getBlock());
 
         Value zero =
-            rewriter.create<arith::ConstantOp>(loc, rewriter.getIndexAttr(0));
+            arith::ConstantOp::create(rewriter, loc, rewriter.getIndexAttr(0));
 
         for (auto &[unrealizedCastOp, chunkData, state] : moduloStates) {
           SmallVector<Value> newReinterpretCasts;
@@ -1274,9 +1275,9 @@ void PtrAnalysis::rewriteForOp(
             newReinterpretCasts.push_back(args[chunk.initArgIndex]);
           }
 
-          auto combinedCast = b.create<UnrealizedConversionCastOp>(
-              loc, unrealizedCastOp.getResult(0).getType(), newReinterpretCasts,
-              unrealizedCastOp->getAttrs());
+          auto combinedCast = UnrealizedConversionCastOp::create(
+              b, loc, unrealizedCastOp.getResult(0).getType(),
+              newReinterpretCasts, unrealizedCastOp->getAttrs());
 
           args[chunkData[0].initArgIndex].replaceUsesWithIf(
               combinedCast.getResult(0), [](OpOperand &operand) {
diff --git a/lib/Analysis/UseAnalysis.cpp b/lib/Analysis/UseAnalysis.cpp
index 7bde980..62e4508 100644
--- a/lib/Analysis/UseAnalysis.cpp
+++ b/lib/Analysis/UseAnalysis.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/lib/AnalysisStructured/PtrAnalysis.cpp b/lib/AnalysisStructured/PtrAnalysis.cpp
index befcfbd..e5b6ece 100644
--- a/lib/AnalysisStructured/PtrAnalysis.cpp
+++ b/lib/AnalysisStructured/PtrAnalysis.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation, Meta Platforms.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
@@ -63,29 +63,26 @@ static Value applyUnstructuredMask(Operation *op, Value ptr,
       return nullptr;
     }
 
-    ptr =
-        builder
-            .create<tts::MakeGatherScatterTensorPtrOp>(
-                loc, gatherScatterPtr.getBase(),
-                gatherScatterPtr.getGatherScatterOffset(), unstructuredMask,
-                gatherScatterPtr.getGatherScatterDim(),
-                gatherScatterPtr.getSizes(), gatherScatterPtr.getMixedStrides(),
-                gatherScatterPtr.getMixedOffsets())
-            .getResult();
+    ptr = tts::MakeGatherScatterTensorPtrOp::create(
+              builder, loc, gatherScatterPtr.getBase(),
+              gatherScatterPtr.getGatherScatterOffset(), unstructuredMask,
+              gatherScatterPtr.getGatherScatterDim(),
+              gatherScatterPtr.getSizes(), gatherScatterPtr.getMixedStrides(),
+              gatherScatterPtr.getMixedOffsets())
+              .getResult();
   } else if (auto structuredPtr = ptr.getDefiningOp<tts::MakeTensorPtrOp>()) {
     auto ofrToI32Value = [&](OpFoldResult ofr) {
       Value v = dyn_cast<Value>(ofr);
       if (!v) {
-        v = builder
-                .create<arith::ConstantOp>(
-                    loc, cast<TypedAttr>(cast<Attribute>(ofr)))
+        v = arith::ConstantOp::create(builder, loc,
+                                      cast<TypedAttr>(cast<Attribute>(ofr)))
                 .getResult();
       }
       if (isa<IndexType>(v.getType())) {
-        v = builder.create<arith::IndexCastOp>(loc, builder.getI32Type(), v)
+        v = arith::IndexCastOp::create(builder, loc, builder.getI32Type(), v)
                 .getResult();
       } else if (v.getType().isInteger(64)) {
-        v = builder.create<arith::TruncIOp>(loc, builder.getI32Type(), v)
+        v = arith::TruncIOp::create(builder, loc, builder.getI32Type(), v)
                 .getResult();
       }
 
@@ -100,22 +97,20 @@ static Value applyUnstructuredMask(Operation *op, Value ptr,
     // Divide stride since offset of tts::MakeTensorPtrOp already include the
     // stride, but gatherScatterOffset of tts::MakeGatherScatterTensorPtrOp
     // should not include stride.
-    offset = builder.create<arith::DivUIOp>(loc, offset, stride);
+    offset = arith::DivUIOp::create(builder, loc, offset, stride);
 
     Value gatherScatterOffset =
-        builder.create<tensor::SplatOp>(loc, offsetRowType, offset).getResult();
-    Value range = builder
-                      .create<triton::MakeRangeOp>(
-                          loc, offsetRowType, 0, structuredPtr.getSizes()[dim])
+        tensor::SplatOp::create(builder, loc, offsetRowType, offset)
+            .getResult();
+    Value range = triton::MakeRangeOp::create(builder, loc, offsetRowType, 0,
+                                              structuredPtr.getSizes()[dim])
                       .getResult();
     gatherScatterOffset =
-        builder.create<arith::AddIOp>(loc, gatherScatterOffset, range);
-    ptr = builder
-              .create<tts::MakeGatherScatterTensorPtrOp>(
-                  loc, structuredPtr.getBase(), gatherScatterOffset,
-                  unstructuredMask, dim, structuredPtr.getSizes(),
-                  structuredPtr.getMixedStrides(),
-                  structuredPtr.getMixedOffsets())
+        arith::AddIOp::create(builder, loc, gatherScatterOffset, range);
+    ptr = tts::MakeGatherScatterTensorPtrOp::create(
+              builder, loc, structuredPtr.getBase(), gatherScatterOffset,
+              unstructuredMask, dim, structuredPtr.getSizes(),
+              structuredPtr.getMixedStrides(), structuredPtr.getMixedOffsets())
               .getResult();
   } else {
     return nullptr;
@@ -291,7 +286,7 @@ LogicalResult PtrState::addState(const PtrState &lhsState,
 
   if (lhsState.scalar && rhsState.scalar) {
     auto addOp =
-        builder.create<arith::AddIOp>(loc, lhsState.scalar, rhsState.scalar);
+        arith::AddIOp::create(builder, loc, lhsState.scalar, rhsState.scalar);
     scalar = addOp.getResult();
   } else if (lhsState.getRank() == 0) { // both lhs and rhs are scalars
     scalar = lhsState.scalar ? lhsState.scalar : rhsState.scalar;
@@ -569,7 +564,7 @@ LogicalResult PtrState::mulState(const PtrState &lhsState,
 
   if (lhsState.scalar && rhsState.scalar) {
     scalar =
-        builder.create<arith::MulIOp>(loc, lhsState.scalar, rhsState.scalar);
+        arith::MulIOp::create(builder, loc, lhsState.scalar, rhsState.scalar);
   }
 
   auto indexTy = IndexType::get(op->getContext());
@@ -662,8 +657,8 @@ tts::MakeTensorPtrOp PtrState::createTTSMakeTensorPtrOp(OpBuilder &builder,
     staticSizes.push_back(s.value());
   }
 
-  auto op = builder.create<mlir::tts::MakeTensorPtrOp>(
-      loc, source, staticSizes, strides, offsets, shape, order);
+  auto op = tts::MakeTensorPtrOp::create(builder, loc, source, staticSizes,
+                                         strides, offsets, shape, order);
   LLVM_DEBUG({
     llvm::dbgs() << "creating tts::make_tensor_ptr:\n";
     op->dump();
@@ -700,16 +695,15 @@ PtrState::createTTSMakeGatherScatterTensorPtrOp(OpBuilder &builder,
     auto collapseTy =
         RankedTensorType::get({offsetSize}, offsetTy.getElementType());
     nonContinuousOffset =
-        builder
-            .create<tensor::CollapseShapeOp>(
-                loc, collapseTy, nonContinuousOffset, reassociationMap)
+        tensor::CollapseShapeOp::create(builder, loc, collapseTy,
+                                        nonContinuousOffset, reassociationMap)
             .getResult();
     offsets[nonContinuousDim] = nonContinuousOffset;
   }
   // Generate tts::make_gather_scatter_tensor_ptr.
-  auto op = builder.create<mlir::tts::MakeGatherScatterTensorPtrOp>(
-      loc, source, nonContinuousOffset, nonContinuousDim, staticSizes, strides,
-      offsets);
+  auto op = tts::MakeGatherScatterTensorPtrOp::create(
+      builder, loc, source, nonContinuousOffset, nonContinuousDim, staticSizes,
+      strides, offsets);
   LLVM_DEBUG({
     llvm::dbgs() << "creating tts::make_gather_scatter_tensor_ptr:\n";
     op->dump();
@@ -1135,19 +1129,19 @@ PtrAnalysis::visitOperandMakeTensorPtr(triton::MakeTensorPtrOp makeTPtrOp,
   for (int64_t i = 0; i < pointeeType.getRank(); i++) {
     state.sizes.push_back(builder.getIndexAttr(shape[i]));
 
-    auto strideCst = builder.create<arith::IndexCastOp>(
-        loc, builder.getIndexType(), makeTPtrOp.getStrides()[i]);
+    auto strideCst = arith::IndexCastOp::create(
+        builder, loc, builder.getIndexType(), makeTPtrOp.getStrides()[i]);
     state.strides.push_back(strideCst.getResult());
 
-    auto offsetCst = builder.create<arith::IndexCastOp>(
-        loc, builder.getIndexType(), makeTPtrOp.getOffsets()[i]);
+    auto offsetCst = arith::IndexCastOp::create(
+        builder, loc, builder.getIndexType(), makeTPtrOp.getOffsets()[i]);
 
-    auto scaledOffset = builder.create<arith::MulIOp>(
-        loc, offsetCst.getResult(), strideCst.getResult());
+    auto scaledOffset = arith::MulIOp::create(
+        builder, loc, offsetCst.getResult(), strideCst.getResult());
     state.offsets.push_back(scaledOffset.getResult());
 
-    auto shapeCst = builder.create<arith::IndexCastOp>(
-        loc, builder.getIndexType(), makeTPtrOp.getShape()[i]);
+    auto shapeCst = arith::IndexCastOp::create(
+        builder, loc, builder.getIndexType(), makeTPtrOp.getShape()[i]);
     state.shape.push_back(shapeCst.getResult());
   }
   state.order = SmallVector<int32_t>(makeTPtrOp.getOrder());
@@ -1219,8 +1213,8 @@ LogicalResult PtrAnalysis::visitOperand(Value operand, PtrState &state,
     if (!isa<BlockArgument>(operand) && operand.getDefiningOp()) {
       builder.setInsertionPointAfter(operand.getDefiningOp());
     }
-    auto castOp = builder.create<arith::IndexCastOp>(
-        loc, builder.getIndexType(), operand);
+    auto castOp = arith::IndexCastOp::create(builder, loc,
+                                             builder.getIndexType(), operand);
     state.scalar = castOp.getResult();
     return success();
   } else if (isa<IndexType>(operand.getType())) {
@@ -1390,18 +1384,18 @@ LogicalResult PtrAnalysis::rewriteAdvanceOp(triton::AdvanceOp op) {
        llvm::zip(incrementOffsets, state.offsets, state.strides)) {
     Value offsetValue;
     if (auto offsetIntAttr = getIntAttr(offset)) {
-      auto constOp = builder.create<arith::ConstantOp>(
-          loc, builder.getIndexAttr(offsetIntAttr.value()));
+      auto constOp = arith::ConstantOp::create(
+          builder, loc, builder.getIndexAttr(offsetIntAttr.value()));
       offsetValue = constOp.getResult();
     } else {
       offsetValue = cast<Value>(offset);
     }
-    auto castOp = builder.create<arith::IndexCastOp>(
-        loc, builder.getIndexType(), increment);
-    auto mulOp = builder.create<arith::MulIOp>(loc, castOp.getResult(),
-                                               cast<Value>(stride));
+    auto castOp = arith::IndexCastOp::create(builder, loc,
+                                             builder.getIndexType(), increment);
+    auto mulOp = arith::MulIOp::create(builder, loc, castOp.getResult(),
+                                       cast<Value>(stride));
     auto addOp =
-        builder.create<arith::AddIOp>(loc, mulOp.getResult(), offsetValue);
+        arith::AddIOp::create(builder, loc, mulOp.getResult(), offsetValue);
     newOffsets.push_back(addOp.getResult());
   }
 
@@ -1637,15 +1631,15 @@ PtrAnalysis::rewriteGetStructuredStateOp(tts::GetStructuredStateOp op) {
       // This operand is a pointer directly from the kernel arguments.
       // Use offset 0.
       assert(!tritonValue.getDefiningOp());
-      replacements.push_back(builder.create<arith::ConstantOp>(
-          op.getLoc(), builder.getIndexAttr(0)));
+      replacements.push_back(arith::ConstantOp::create(
+          builder, op.getLoc(), builder.getIndexAttr(0)));
     }
   } else {
     for (auto [j, s] : llvm::enumerate(state.offsets)) {
       auto sIntAttr = getIntAttr(s);
       if (sIntAttr) {
-        auto constOp = builder.create<arith::ConstantOp>(
-            op.getLoc(), builder.getIndexAttr(sIntAttr.value()));
+        auto constOp = arith::ConstantOp::create(
+            builder, op.getLoc(), builder.getIndexAttr(sIntAttr.value()));
         replacements.push_back(constOp.getResult());
       } else {
         replacements.push_back(cast<Value>(s));
@@ -1655,8 +1649,8 @@ PtrAnalysis::rewriteGetStructuredStateOp(tts::GetStructuredStateOp op) {
     for (auto [j, s] : llvm::enumerate(state.strides)) {
       auto sIntAttr = getIntAttr(s);
       if (sIntAttr) {
-        auto constOp = builder.create<arith::ConstantOp>(
-            op.getLoc(), builder.getIndexAttr(sIntAttr.value()));
+        auto constOp = arith::ConstantOp::create(
+            builder, op.getLoc(), builder.getIndexAttr(sIntAttr.value()));
         replacements.push_back(constOp.getResult());
       } else {
         replacements.push_back(cast<Value>(s));
@@ -1720,7 +1714,7 @@ LogicalResult PtrAnalysis::rewriteLoadOp(triton::LoadOp op,
     }
   }
 
-  auto loadOp = builder.create<tts::LoadOp>(loc, ptr, dims, scalarOther);
+  auto loadOp = tts::LoadOp::create(builder, loc, ptr, dims, scalarOther);
 
   LLVM_DEBUG({
     llvm::dbgs() << "creating tts::load:\n";
@@ -1850,7 +1844,7 @@ LogicalResult PtrAnalysis::rewriteStoreOp(triton::StoreOp op,
     dims = mstate.dims;
   }
 
-  auto storeOp = builder.create<tts::StoreOp>(loc, ptr, val, dims);
+  auto storeOp = tts::StoreOp::create(builder, loc, ptr, val, dims);
 
   LLVM_DEBUG({
     llvm::dbgs() << "creating tts::store:\n";
diff --git a/lib/Conversion/StructuredToMemref/StructuredToMemref.cpp b/lib/Conversion/StructuredToMemref/StructuredToMemref.cpp
index 7619d82..6a8ca9e 100644
--- a/lib/Conversion/StructuredToMemref/StructuredToMemref.cpp
+++ b/lib/Conversion/StructuredToMemref/StructuredToMemref.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation, Meta Platforms.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
@@ -56,8 +56,8 @@ static memref::SubViewOp getSubview(int rank, ArrayRef<OpFoldResult> dims,
   auto dstType =
       memref::SubViewOp::inferResultType(sourceType, offsets, dims, strides);
 
-  return b.create<memref::SubViewOp>(loc, cast<MemRefType>(dstType), source,
-                                     offsets, dims, strides);
+  return memref::SubViewOp::create(b, loc, cast<MemRefType>(dstType), source,
+                                   offsets, dims, strides);
 }
 
 static Type getElementTypeStructuredPtr(tts::MakeTensorPtrOp op) {
@@ -182,8 +182,9 @@ static Value rewriteGatherScatterPtrElement(
   SmallVector<Value> dynSizes; // sizes are always static
   auto sizes = mlir::getMixedValues(staticSizes, dynSizes, rewriter);
 
-  auto castOp = rewriter.create<memref::ReinterpretCastOp>(
-      op.getLoc(), resultType, basePtr, targetOffset, sizes, mixedStrides);
+  auto castOp = memref::ReinterpretCastOp::create(
+      rewriter, op.getLoc(), resultType, basePtr, targetOffset, sizes,
+      mixedStrides);
 
   return castOp.getResult();
 }
@@ -198,28 +199,28 @@ static void fillWithValue(Location loc, Value alloc, Value other,
   // For each dimension check if dims[i] < shape[i], or-accumulate
   // the result
   auto accBase =
-      rewriter.create<arith::ConstantOp>(loc, rewriter.getBoolAttr(false))
+      arith::ConstantOp::create(rewriter, loc, rewriter.getBoolAttr(false))
           .getResult();
   for (size_t i = 0; i < shape.size(); i++) {
-    auto shapei = rewriter.create<arith::ConstantOp>(
-        loc, rewriter.getIndexAttr(shape[i]));
+    auto shapei = arith::ConstantOp::create(rewriter, loc,
+                                            rewriter.getIndexAttr(shape[i]));
 
     Value dimi = dyn_cast<Value>(mixedDims[i]);
     if (!dimi) {
-      dimi = rewriter.create<arith::ConstantOp>(
-          loc, rewriter.getIndexAttr(staticMaskDims[i]));
+      dimi = arith::ConstantOp::create(
+          rewriter, loc, rewriter.getIndexAttr(staticMaskDims[i]));
     }
 
-    Value cmp = rewriter.create<arith::CmpIOp>(loc, arith::CmpIPredicate::slt,
-                                               dimi, shapei);
-    accBase = rewriter.create<arith::OrIOp>(loc, accBase, cmp);
+    Value cmp = arith::CmpIOp::create(rewriter, loc, arith::CmpIPredicate::slt,
+                                      dimi, shapei);
+    accBase = arith::OrIOp::create(rewriter, loc, accBase, cmp);
   }
 
   // condition the memset on the or-accumulation
   // initialize with padding prior to CopyOp
-  rewriter.create<scf::IfOp>(loc, accBase, [&](OpBuilder &b, Location loc) {
-    b.create<linalg::FillOp>(loc, ValueRange{other}, ValueRange{alloc});
-    b.create<scf::YieldOp>(loc);
+  scf::IfOp::create(rewriter, loc, accBase, [&](OpBuilder &b, Location loc) {
+    linalg::FillOp::create(b, loc, ValueRange{other}, ValueRange{alloc});
+    scf::YieldOp::create(b, loc);
   });
 }
 
@@ -321,35 +322,36 @@ private:
             // wrapping around.
             ShapedType::kDynamic});
 
-    Value rowSize = rewriter.create<arith::ConstantOp>(
-        loc, rewriter.getIndexAttr(op.getSizes()[0]));
-    Value colSize = rewriter.create<arith::ConstantOp>(
-        loc, rewriter.getIndexAttr(op.getSizes()[1]));
+    Value rowSize = arith::ConstantOp::create(
+        rewriter, loc, rewriter.getIndexAttr(op.getSizes()[0]));
+    Value colSize = arith::ConstantOp::create(
+        rewriter, loc, rewriter.getIndexAttr(op.getSizes()[1]));
 
     Value modN = ofrToIndexValue(op.getMixedShape()[1], loc, rewriter);
 
-    Value x = rewriter.create<arith::RemSIOp>(loc, targetOffset, modN);
-    Value y = rewriter.create<arith::SubIOp>(loc, targetOffset, x);
+    Value x = arith::RemSIOp::create(rewriter, loc, targetOffset, modN);
+    Value y = arith::SubIOp::create(rewriter, loc, targetOffset, x);
 
     SmallVector<Value> strideVals =
         ofrsToIndexValues(op.getMixedStrides(), loc, rewriter);
 
     // First chunk
-    Value nextOffset = rewriter.create<arith::AddIOp>(loc, x, colSize);
+    Value nextOffset = arith::AddIOp::create(rewriter, loc, x, colSize);
     Value clampedOffset =
-        rewriter.create<arith::MinSIOp>(loc, nextOffset, modN);
-    Value d1 = rewriter.create<arith::SubIOp>(loc, clampedOffset, x);
+        arith::MinSIOp::create(rewriter, loc, nextOffset, modN);
+    Value d1 = arith::SubIOp::create(rewriter, loc, clampedOffset, x);
     SmallVector<Value> sizes1{rowSize, d1};
 
-    auto cast1 = rewriter.create<memref::ReinterpretCastOp>(
-        loc, resultType, adaptor.getBase(), targetOffset, sizes1, strideVals);
+    auto cast1 = memref::ReinterpretCastOp::create(
+        rewriter, loc, resultType, adaptor.getBase(), targetOffset, sizes1,
+        strideVals);
 
     // Second chunk
-    Value d2 = rewriter.create<arith::SubIOp>(loc, colSize, d1);
+    Value d2 = arith::SubIOp::create(rewriter, loc, colSize, d1);
     SmallVector<Value> sizes2{rowSize, d2};
 
-    auto cast2 = rewriter.create<memref::ReinterpretCastOp>(
-        loc, resultType, adaptor.getBase(), y, sizes2, strideVals);
+    auto cast2 = memref::ReinterpretCastOp::create(
+        rewriter, loc, resultType, adaptor.getBase(), y, sizes2, strideVals);
 
     return {cast1, cast2};
   }
@@ -450,10 +452,10 @@ private:
             // allow this anymore. So we put dynamic instead.
             ShapedType::kDynamic});
 
-    Value rowSize = rewriter.create<arith::ConstantOp>(
-        loc, rewriter.getIndexAttr(op.getSizes()[0]));
-    Value colSize = rewriter.create<arith::ConstantOp>(
-        loc, rewriter.getIndexAttr(op.getSizes()[1]));
+    Value rowSize = arith::ConstantOp::create(
+        rewriter, loc, rewriter.getIndexAttr(op.getSizes()[0]));
+    Value colSize = arith::ConstantOp::create(
+        rewriter, loc, rewriter.getIndexAttr(op.getSizes()[1]));
 
     Value strideRow = ofrToIndexValue(op.getMixedStrides()[0], loc, rewriter);
     Value strideCol = ofrToIndexValue(op.getMixedStrides()[1], loc, rewriter);
@@ -462,26 +464,24 @@ private:
 
     // First chunk
     Value wrappedAroundOff =
-        rewriter.create<arith::RemSIOp>(loc, targetOffset, strideRow);
+        arith::RemSIOp::create(rewriter, loc, targetOffset, strideRow);
     Value clampedOff =
-        rewriter.create<arith::AddIOp>(loc, modRow, wrappedAroundOff);
-    Value d1 = rewriter.create<arith::SubIOp>(loc, clampedOff, targetOffset);
-    d1 = rewriter.create<arith::DivSIOp>(loc, d1, strideRow);
-    d1 = rewriter.create<arith::MinSIOp>(loc, d1, rowSize);
+        arith::AddIOp::create(rewriter, loc, modRow, wrappedAroundOff);
+    Value d1 = arith::SubIOp::create(rewriter, loc, clampedOff, targetOffset);
+    d1 = arith::DivSIOp::create(rewriter, loc, d1, strideRow);
+    d1 = arith::MinSIOp::create(rewriter, loc, d1, rowSize);
 
     SmallVector<Value> sizes1{d1, colSize};
-    memref::ReinterpretCastOp cast1 =
-        rewriter.create<memref::ReinterpretCastOp>(
-            loc, resultType, adaptor.getBase(), targetOffset, sizes1,
-            ValueRange{strideRow, strideCol});
+    memref::ReinterpretCastOp cast1 = memref::ReinterpretCastOp::create(
+        rewriter, loc, resultType, adaptor.getBase(), targetOffset, sizes1,
+        ValueRange{strideRow, strideCol});
 
     // Second chunk
-    Value d2 = rewriter.create<arith::SubIOp>(loc, rowSize, d1);
+    Value d2 = arith::SubIOp::create(rewriter, loc, rowSize, d1);
     SmallVector<Value> sizes2{d2, colSize};
-    memref::ReinterpretCastOp cast2 =
-        rewriter.create<memref::ReinterpretCastOp>(
-            loc, resultType, adaptor.getBase(), wrappedAroundOff, sizes2,
-            ValueRange{strideRow, strideCol});
+    memref::ReinterpretCastOp cast2 = memref::ReinterpretCastOp::create(
+        rewriter, loc, resultType, adaptor.getBase(), wrappedAroundOff, sizes2,
+        ValueRange{strideRow, strideCol});
 
     return {cast1, cast2};
   }
@@ -515,8 +515,8 @@ private:
       llvm_unreachable("Unexpected split pointer shape");
     }
 
-    auto combinedCast = rewriter.create<UnrealizedConversionCastOp>(
-        op.getLoc(), op.getType(), casts);
+    auto combinedCast = UnrealizedConversionCastOp::create(
+        rewriter, op.getLoc(), op.getType(), casts);
 
     combinedCast->setAttr(wrapType, rewriter.getUnitAttr());
 
@@ -541,8 +541,8 @@ private:
         op, staticTargetOffset.value_or(ShapedType::kDynamic), staticStrides,
         resultShape);
 
-    auto castOp = rewriter.create<memref::ReinterpretCastOp>(
-        op.getLoc(), resultType, adaptor.getBase(), targetOffset,
+    auto castOp = memref::ReinterpretCastOp::create(
+        rewriter, op.getLoc(), resultType, adaptor.getBase(), targetOffset,
         op.getMixedSizes(), mixedStrides);
 
     rewriter.replaceOp(op, castOp);
@@ -626,71 +626,67 @@ private:
                               ConversionPatternRewriter &rewriter) const {
 
     auto zero =
-        rewriter.create<arith::ConstantOp>(loc, rewriter.getIndexAttr(0));
+        arith::ConstantOp::create(rewriter, loc, rewriter.getIndexAttr(0));
 
     auto one =
-        rewriter.create<arith::ConstantOp>(loc, rewriter.getIndexAttr(1));
-
-    Value block1Row = rewriter.create<memref::DimOp>(loc, block1, 0);
-    Value block1Col = rewriter.create<memref::DimOp>(loc, block1, 1);
-
-    Value block2Row = rewriter.create<memref::DimOp>(loc, block2, 0);
-    Value block2Col = rewriter.create<memref::DimOp>(loc, block2, 1);
-
-    auto block1Dst =
-        rewriter.create<memref::SubViewOp>(loc, dst, /* offsets */
-                                           ValueRange{zero, zero},
-                                           /* sizes */
-                                           ValueRange{block1Row, block1Col},
-                                           /* strides */
-                                           ValueRange{one, one});
-
-    auto block2Dst =
-        rewriter.create<memref::SubViewOp>(loc, dst,
-                                           /* offsets */
-                                           ValueRange{zero, block1Col},
-                                           /* sizes */
-                                           ValueRange{block2Row, block2Col},
-                                           /* strides */
-                                           ValueRange{one, one});
-
-    rewriter.create<memref::CopyOp>(loc, block1, block1Dst);
-    rewriter.create<memref::CopyOp>(loc, block2, block2Dst);
+        arith::ConstantOp::create(rewriter, loc, rewriter.getIndexAttr(1));
+
+    Value block1Row = memref::DimOp::create(rewriter, loc, block1, 0);
+    Value block1Col = memref::DimOp::create(rewriter, loc, block1, 1);
+
+    Value block2Row = memref::DimOp::create(rewriter, loc, block2, 0);
+    Value block2Col = memref::DimOp::create(rewriter, loc, block2, 1);
+
+    auto block1Dst = memref::SubViewOp::create(rewriter, loc, dst, /* offsets */
+                                               ValueRange{zero, zero},
+                                               /* sizes */
+                                               ValueRange{block1Row, block1Col},
+                                               /* strides */
+                                               ValueRange{one, one});
+
+    auto block2Dst = memref::SubViewOp::create(rewriter, loc, dst,
+                                               /* offsets */
+                                               ValueRange{zero, block1Col},
+                                               /* sizes */
+                                               ValueRange{block2Row, block2Col},
+                                               /* strides */
+                                               ValueRange{one, one});
+
+    memref::CopyOp::create(rewriter, loc, block1, block1Dst);
+    memref::CopyOp::create(rewriter, loc, block2, block2Dst);
   }
 
   void createStackedCopies(Value block1, Value block2, Value dst, Location loc,
                            ConversionPatternRewriter &rewriter) const {
 
     auto zero =
-        rewriter.create<arith::ConstantOp>(loc, rewriter.getIndexAttr(0));
+        arith::ConstantOp::create(rewriter, loc, rewriter.getIndexAttr(0));
     auto one =
-        rewriter.create<arith::ConstantOp>(loc, rewriter.getIndexAttr(1));
-
-    Value block1Row = rewriter.create<memref::DimOp>(loc, block1, 0);
-    Value block1Col = rewriter.create<memref::DimOp>(loc, block1, 1);
-
-    Value block2Row = rewriter.create<memref::DimOp>(loc, block2, 0);
-    Value block2Col = rewriter.create<memref::DimOp>(loc, block2, 1);
-
-    auto block1Dst =
-        rewriter.create<memref::SubViewOp>(loc, dst, /* offsets */
-                                           ValueRange{zero, zero},
-                                           /* sizes */
-                                           ValueRange{block1Row, block1Col},
-                                           /* strides */
-                                           ValueRange{one, one});
-
-    auto block2Dst =
-        rewriter.create<memref::SubViewOp>(loc, dst,
-                                           /* offsets */
-                                           ValueRange{block1Row, zero},
-                                           /* sizes */
-                                           ValueRange{block2Row, block2Col},
-                                           /* strides */
-                                           ValueRange{one, one});
-
-    rewriter.create<memref::CopyOp>(loc, block1, block1Dst);
-    rewriter.create<memref::CopyOp>(loc, block2, block2Dst);
+        arith::ConstantOp::create(rewriter, loc, rewriter.getIndexAttr(1));
+
+    Value block1Row = memref::DimOp::create(rewriter, loc, block1, 0);
+    Value block1Col = memref::DimOp::create(rewriter, loc, block1, 1);
+
+    Value block2Row = memref::DimOp::create(rewriter, loc, block2, 0);
+    Value block2Col = memref::DimOp::create(rewriter, loc, block2, 1);
+
+    auto block1Dst = memref::SubViewOp::create(rewriter, loc, dst, /* offsets */
+                                               ValueRange{zero, zero},
+                                               /* sizes */
+                                               ValueRange{block1Row, block1Col},
+                                               /* strides */
+                                               ValueRange{one, one});
+
+    auto block2Dst = memref::SubViewOp::create(rewriter, loc, dst,
+                                               /* offsets */
+                                               ValueRange{block1Row, zero},
+                                               /* sizes */
+                                               ValueRange{block2Row, block2Col},
+                                               /* strides */
+                                               ValueRange{one, one});
+
+    memref::CopyOp::create(rewriter, loc, block1, block1Dst);
+    memref::CopyOp::create(rewriter, loc, block2, block2Dst);
   }
 
   memref::SubViewOp createSubview(Value src, ArrayRef<OpFoldResult> offsets,
@@ -700,8 +696,8 @@ private:
     auto srcType = cast<MemRefType>(src.getType());
     auto dstType =
         memref::SubViewOp::inferResultType(srcType, offsets, sizes, strides);
-    return rewriter.create<memref::SubViewOp>(loc, cast<MemRefType>(dstType),
-                                              src, offsets, sizes, strides);
+    return memref::SubViewOp::create(rewriter, loc, cast<MemRefType>(dstType),
+                                     src, offsets, sizes, strides);
   }
 
   std::pair<memref::SubViewOp, memref::SubViewOp>
@@ -711,9 +707,9 @@ private:
     OpFoldResult subviewRowFull = dims[0];
     OpFoldResult subviewColFull = dims[1];
     OpFoldResult subviewCol1 =
-        rewriter.create<memref::DimOp>(loc, block1, 1).getResult();
+        memref::DimOp::create(rewriter, loc, block1, 1).getResult();
     OpFoldResult subviewCol2 =
-        rewriter.create<memref::DimOp>(loc, block2, 1).getResult();
+        memref::DimOp::create(rewriter, loc, block2, 1).getResult();
 
     SmallVector<OpFoldResult> offsets(dims.size(), rewriter.getIndexAttr(0));
     SmallVector<OpFoldResult> strides(dims.size(), rewriter.getIndexAttr(1));
@@ -732,9 +728,9 @@ private:
     OpFoldResult subviewRowFull = dims[0];
     OpFoldResult subviewColFull = dims[1];
     OpFoldResult subviewRow1 =
-        rewriter.create<memref::DimOp>(loc, block1, 0).getResult();
+        memref::DimOp::create(rewriter, loc, block1, 0).getResult();
     OpFoldResult subviewRow2 =
-        rewriter.create<memref::DimOp>(loc, block2, 0).getResult();
+        memref::DimOp::create(rewriter, loc, block2, 0).getResult();
 
     SmallVector<OpFoldResult> offsets(dims.size(), rewriter.getIndexAttr(0));
     SmallVector<OpFoldResult> strides(dims.size(), rewriter.getIndexAttr(1));
@@ -757,8 +753,8 @@ private:
     auto tensorType = cast<RankedTensorType>(op.getType());
     auto elemType = tensorType.getElementType();
 
-    auto alloc = rewriter.create<memref::AllocOp>(
-        loc, MemRefType::get(tensorType.getShape(), elemType));
+    auto alloc = memref::AllocOp::create(
+        rewriter, loc, MemRefType::get(tensorType.getShape(), elemType));
 
     // No mask
     assert(!other && "other value used in non-masked load");
@@ -781,11 +777,12 @@ private:
         llvm_unreachable("unexpected wraparound type");
       }
     } else {
-      rewriter.create<memref::CopyOp>(loc, ptr, alloc);
+      memref::CopyOp::create(rewriter, loc, ptr, alloc);
     }
 
-    Value tensor = rewriter.create<bufferization::ToTensorOp>(
-        loc, tensorType, alloc, true /* restrict */, true /* writable */);
+    Value tensor = bufferization::ToTensorOp::create(rewriter, loc, tensorType,
+                                                     alloc, true /* restrict */,
+                                                     true /* writable */);
     rewriter.replaceOp(op, tensor);
 
     return success();
@@ -801,8 +798,8 @@ private:
     auto tensorType = cast<RankedTensorType>(op.getType());
     auto elemType = tensorType.getElementType();
 
-    auto alloc = rewriter.create<memref::AllocOp>(
-        loc, MemRefType::get(tensorType.getShape(), elemType));
+    auto alloc = memref::AllocOp::create(
+        rewriter, loc, MemRefType::get(tensorType.getShape(), elemType));
 
     SmallVector<OpFoldResult> mixedDims = op.getMixedMaskDims();
 
@@ -842,11 +839,12 @@ private:
           getSubview(tensorType.getRank(), mixedDims, ptr, loc, rewriter);
       memref::SubViewOp dstSubview =
           getSubview(tensorType.getRank(), mixedDims, alloc, loc, rewriter);
-      rewriter.create<memref::CopyOp>(loc, srcSubview, dstSubview);
+      memref::CopyOp::create(rewriter, loc, srcSubview, dstSubview);
     }
 
-    Value tensor = rewriter.create<bufferization::ToTensorOp>(
-        loc, tensorType, alloc, true /* restrict */, true /* writable */);
+    Value tensor = bufferization::ToTensorOp::create(rewriter, loc, tensorType,
+                                                     alloc, true /* restrict */,
+                                                     true /* writable */);
     rewriter.replaceOp(op, tensor);
 
     return success();
@@ -864,7 +862,7 @@ private:
     auto indexOffsetTy = RankedTensorType::get(offsetShapedType.getShape(),
                                                rewriter.getIndexType());
     gatherOffset =
-        rewriter.create<arith::IndexCastOp>(loc, indexOffsetTy, gatherOffset)
+        arith::IndexCastOp::create(rewriter, loc, indexOffsetTy, gatherOffset)
             .getResult();
 
     int gatherDim = ptr.getGatherScatterDim();
@@ -881,7 +879,7 @@ private:
     auto resultType = dyn_cast<RankedTensorType>(op.getResult().getType());
     auto allocType =
         MemRefType::get(resultType.getShape(), resultType.getElementType());
-    auto alloc = rewriter.create<memref::AllocOp>(loc, allocType);
+    auto alloc = memref::AllocOp::create(rewriter, loc, allocType);
 
     auto allocStrides = mlir::getMixedValues(
         allocType.getStridesAndOffset().first, dynSizes, rewriter);
@@ -892,9 +890,9 @@ private:
     }
 
     // Create loop to iterate every offset in gatherOffset.
-    auto lowerBound = rewriter.create<arith::ConstantIndexOp>(loc, 0);
+    auto lowerBound = arith::ConstantIndexOp::create(rewriter, loc, 0);
     Value upperBound =
-        rewriter.create<arith::ConstantIndexOp>(loc, offsetSize).getResult();
+        arith::ConstantIndexOp::create(rewriter, loc, offsetSize).getResult();
     if (op.hasMask()) {
       SmallVector<OpFoldResult> mixedDims = op.getMixedMaskDims();
       OpFoldResult gatherMaskDim = mixedDims[gatherDim];
@@ -911,26 +909,26 @@ private:
           gatherMaskDimValue = offsetSize;
         }
         offsetSize = std::min(offsetSize, gatherMaskDimValue);
-        upperBound = rewriter.create<arith::ConstantIndexOp>(loc, offsetSize)
+        upperBound = arith::ConstantIndexOp::create(rewriter, loc, offsetSize)
                          .getResult();
       } else {
         // Use arith::MinSIOp to get the minimum value of gatherMaskDim
         // and offsetSize.
         auto gatherMaskDimVal = cast<Value>(gatherMaskDim);
         auto offsetSizeVal =
-            rewriter.create<arith::ConstantIndexOp>(loc, offsetSize);
-        upperBound =
-            rewriter
-                .create<arith::MinSIOp>(loc, gatherMaskDimVal, offsetSizeVal)
-                .getResult();
+            arith::ConstantIndexOp::create(rewriter, loc, offsetSize);
+        upperBound = arith::MinSIOp::create(rewriter, loc, gatherMaskDimVal,
+                                            offsetSizeVal)
+                         .getResult();
       }
     }
-    auto step = rewriter.create<arith::ConstantIndexOp>(loc, 1);
-    auto loop = rewriter.create<scf::ForOp>(loc, lowerBound, upperBound, step);
+    auto step = arith::ConstantIndexOp::create(rewriter, loc, 1);
+    auto loop = scf::ForOp::create(rewriter, loc, lowerBound, upperBound, step);
 
     // Create tensor from alloc and use it as the result to replace op.
-    Value tensor = rewriter.create<bufferization::ToTensorOp>(
-        loc, op.getType(), alloc, true /* restrict */, true /* writable */);
+    Value tensor = bufferization::ToTensorOp::create(
+        rewriter, loc, op.getType(), alloc, true /* restrict */,
+        true /* writable */);
     rewriter.replaceOp(op, tensor);
 
     // Build loop body.
@@ -941,15 +939,15 @@ private:
     if (Value unstructuredMask = ptr.getGatherScatterMask()) {
       // If the gather scatter mask is present, we need to use it to guard the
       // load.
-      auto maskValue = rewriter.create<tensor::ExtractOp>(
-          loc, unstructuredMask, ValueRange{inductionVar});
-      auto ifOp = rewriter.create<scf::IfOp>(loc, maskValue);
+      auto maskValue = tensor::ExtractOp::create(
+          rewriter, loc, unstructuredMask, ValueRange{inductionVar});
+      auto ifOp = scf::IfOp::create(rewriter, loc, maskValue);
       rewriter.setInsertionPointToStart(&ifOp.getThenRegion().front());
     }
 
     // Load the offsetElt first.
-    auto gatherOffsetElt = rewriter.create<tensor::ExtractOp>(
-        loc, gatherOffset, ValueRange{inductionVar});
+    auto gatherOffsetElt = tensor::ExtractOp::create(
+        rewriter, loc, gatherOffset, ValueRange{inductionVar});
 
     // reinterpret_cast to current row as memRefPtr[gatherOffsetElt].
     Value srcPtr = rewriteGatherScatterPtrElement(staticSizes, ptr, memRefPtr,
@@ -971,11 +969,10 @@ private:
       // Use oneStrides for subview.
       auto dstSubViewType = memref::SubViewOp::inferResultType(
           cast<MemRefType>(srcPtr.getType()), maskOffsets, sizes, oneStrides);
-      srcPtr =
-          rewriter
-              .create<memref::SubViewOp>(loc, cast<MemRefType>(dstSubViewType),
+      srcPtr = memref::SubViewOp::create(rewriter, loc,
+                                         cast<MemRefType>(dstSubViewType),
                                          srcPtr, maskOffsets, sizes, oneStrides)
-              .getResult();
+                   .getResult();
     }
 
     // alloc[inductionVar]
@@ -983,11 +980,11 @@ private:
     allocOffsets[gatherDim] = inductionVar;
     auto dstAllocType = memref::SubViewOp::inferResultType(
         allocType, allocOffsets, sizes, oneStrides);
-    auto dstSubview = rewriter.create<memref::SubViewOp>(
-        loc, cast<MemRefType>(dstAllocType), alloc, allocOffsets, sizes,
-        oneStrides);
+    auto dstSubview =
+        memref::SubViewOp::create(rewriter, loc, cast<MemRefType>(dstAllocType),
+                                  alloc, allocOffsets, sizes, oneStrides);
     // Copy srcPtr to alloc[inductionVar].
-    rewriter.create<memref::CopyOp>(loc, srcPtr, dstSubview);
+    memref::CopyOp::create(rewriter, loc, srcPtr, dstSubview);
 
     return success();
   }
@@ -1024,11 +1021,10 @@ private:
     SmallVector<OpFoldResult> offsets(rank, b.getIndexAttr(0));
     SmallVector<OpFoldResult> strides(rank, b.getIndexAttr(1));
 
-    auto dstType = tensor::ExtractSliceOp::inferResultType(sourceType, offsets,
-                                                           dims, strides);
+    auto dstType = tensor::ExtractSliceOp::inferResultType(sourceType, dims);
 
-    return b.create<tensor::ExtractSliceOp>(loc, dstType, source, offsets, dims,
-                                            strides);
+    return tensor::ExtractSliceOp::create(b, loc, dstType, source, offsets,
+                                          dims, strides);
   }
 
   LogicalResult rewriteScatter(tts::MakeGatherScatterTensorPtrOp ptr,
@@ -1043,7 +1039,7 @@ private:
     auto indexOffsetTy = RankedTensorType::get(offsetShapedType.getShape(),
                                                rewriter.getIndexType());
     gatherOffset =
-        rewriter.create<arith::IndexCastOp>(loc, indexOffsetTy, gatherOffset)
+        arith::IndexCastOp::create(rewriter, loc, indexOffsetTy, gatherOffset)
             .getResult();
 
     int gatherDim = ptr.getGatherScatterDim();
@@ -1057,9 +1053,9 @@ private:
     auto sizes = mlir::getMixedValues(staticSizes, dynSizes, rewriter);
 
     // Create loop to iterate every offset in gatherOffset.
-    auto lowerBound = rewriter.create<arith::ConstantIndexOp>(loc, 0);
+    auto lowerBound = arith::ConstantIndexOp::create(rewriter, loc, 0);
     Value upperBound =
-        rewriter.create<arith::ConstantIndexOp>(loc, offsetSize).getResult();
+        arith::ConstantIndexOp::create(rewriter, loc, offsetSize).getResult();
     if (op.hasMask()) {
       SmallVector<OpFoldResult> mixedDims = op.getMixedMaskDims();
       OpFoldResult gatherMaskDim = mixedDims[gatherDim];
@@ -1076,22 +1072,21 @@ private:
           gatherMaskDimValue = offsetSize;
         }
         offsetSize = std::min(offsetSize, gatherMaskDimValue);
-        upperBound = rewriter.create<arith::ConstantIndexOp>(loc, offsetSize)
+        upperBound = arith::ConstantIndexOp::create(rewriter, loc, offsetSize)
                          .getResult();
       } else {
         // Use arith::MinSIOp to get the minimum value of gatherMaskDim
         // and offsetSize.
         auto gatherMaskDimVal = cast<Value>(gatherMaskDim);
         auto offsetSizeVal =
-            rewriter.create<arith::ConstantIndexOp>(loc, offsetSize);
-        upperBound =
-            rewriter
-                .create<arith::MinSIOp>(loc, gatherMaskDimVal, offsetSizeVal)
-                .getResult();
+            arith::ConstantIndexOp::create(rewriter, loc, offsetSize);
+        upperBound = arith::MinSIOp::create(rewriter, loc, gatherMaskDimVal,
+                                            offsetSizeVal)
+                         .getResult();
       }
     }
-    auto step = rewriter.create<arith::ConstantIndexOp>(loc, 1);
-    auto loop = rewriter.create<scf::ForOp>(loc, lowerBound, upperBound, step);
+    auto step = arith::ConstantIndexOp::create(rewriter, loc, 1);
+    auto loop = scf::ForOp::create(rewriter, loc, lowerBound, upperBound, step);
 
     // Build loop body.
     rewriter.setInsertionPointToStart(loop.getBody());
@@ -1101,15 +1096,15 @@ private:
     if (Value unstructuredMask = ptr.getGatherScatterMask()) {
       // If the gather scatter mask is present, we need to use it to guard the
       // store.
-      auto maskValue = rewriter.create<tensor::ExtractOp>(
-          loc, unstructuredMask, ValueRange{inductionVar});
-      auto ifOp = rewriter.create<scf::IfOp>(loc, maskValue);
+      auto maskValue = tensor::ExtractOp::create(
+          rewriter, loc, unstructuredMask, ValueRange{inductionVar});
+      auto ifOp = scf::IfOp::create(rewriter, loc, maskValue);
       rewriter.setInsertionPointToStart(&ifOp.getThenRegion().front());
     }
 
     // Load the offsetElt first.
-    auto gatherOffsetElt = rewriter.create<tensor::ExtractOp>(
-        loc, gatherOffset, ValueRange{inductionVar});
+    auto gatherOffsetElt = tensor::ExtractOp::create(
+        rewriter, loc, gatherOffset, ValueRange{inductionVar});
 
     // Create extract_slice stVal[inductionVar].
     unsigned rank = ptr.getSizes().size();
@@ -1125,8 +1120,8 @@ private:
     }
     // The subview should not apply an additional stride to the source.
     SmallVector<OpFoldResult> oneStrides(rank, OpFoldResult(step));
-    auto slice = rewriter.create<tensor::ExtractSliceOp>(
-        loc, stVal, stValOffsets, sizes, oneStrides);
+    auto slice = tensor::ExtractSliceOp::create(
+        rewriter, loc, stVal, stValOffsets, sizes, oneStrides);
 
     // reinterpret_cast to current row as memRefPtr[gatherOffsetElt].
     Value dstPtr = rewriteGatherScatterPtrElement(staticSizes, ptr, memRefPtr,
@@ -1141,14 +1136,13 @@ private:
           cast<MemRefType>(dstPtr.getType()), maskOffsets, sizes, oneStrides);
 
       dstPtr =
-          rewriter
-              .create<memref::SubViewOp>(loc, cast<MemRefType>(dstType), dstPtr,
-                                         maskOffsets, sizes, oneStrides)
+          memref::SubViewOp::create(rewriter, loc, cast<MemRefType>(dstType),
+                                    dstPtr, maskOffsets, sizes, oneStrides)
               .getResult();
     }
     // store slice to dstPtr.
-    auto storeOp = rewriter.create<bufferization::MaterializeInDestinationOp>(
-        loc, slice, dstPtr);
+    auto storeOp = bufferization::MaterializeInDestinationOp::create(
+        rewriter, loc, slice, dstPtr);
     storeOp.setWritable(true);
 
     rewriter.eraseOp(op);
@@ -1182,12 +1176,12 @@ public:
           getExtractSlice(rank, mixedDims, storeValue, loc, rewriter);
       auto dstSubview = getSubview(rank, mixedDims, ptr, loc, rewriter);
 
-      auto storeOp = rewriter.create<bufferization::MaterializeInDestinationOp>(
-          loc, srcSlice, dstSubview);
+      auto storeOp = bufferization::MaterializeInDestinationOp::create(
+          rewriter, loc, srcSlice, dstSubview);
       storeOp.setWritable(true);
     } else {
-      auto storeOp = rewriter.create<bufferization::MaterializeInDestinationOp>(
-          loc, storeValue, ptr);
+      auto storeOp = bufferization::MaterializeInDestinationOp::create(
+          rewriter, loc, storeValue, ptr);
       storeOp.setWritable(true);
     }
 
diff --git a/lib/Conversion/StructuredToMemref/StructuredToMemrefPass.cpp b/lib/Conversion/StructuredToMemref/StructuredToMemrefPass.cpp
index 049b3f6..2a393e8 100644
--- a/lib/Conversion/StructuredToMemref/StructuredToMemrefPass.cpp
+++ b/lib/Conversion/StructuredToMemref/StructuredToMemrefPass.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation, Meta Platforms.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
@@ -51,13 +51,15 @@ public:
     addTargetMaterialization([&](OpBuilder &builder,
                                  UnrankedMemRefType resultType,
                                  ValueRange inputs, Location loc) -> Value {
-      return builder.create<UnrealizedConversionCastOp>(loc, resultType, inputs)
+      return UnrealizedConversionCastOp::create(builder, loc, resultType,
+                                                inputs)
           .getResult(0);
     });
 
     addSourceMaterialization([&](OpBuilder &builder, Type resultType,
                                  ValueRange inputs, Location loc) -> Value {
-      return builder.create<UnrealizedConversionCastOp>(loc, resultType, inputs)
+      return UnrealizedConversionCastOp::create(builder, loc, resultType,
+                                                inputs)
           .getResult(0);
     });
   }
diff --git a/lib/Conversion/TritonArithToLinalg/TritonArithToLinalg.cpp b/lib/Conversion/TritonArithToLinalg/TritonArithToLinalg.cpp
index 22142c0..44ecd72 100644
--- a/lib/Conversion/TritonArithToLinalg/TritonArithToLinalg.cpp
+++ b/lib/Conversion/TritonArithToLinalg/TritonArithToLinalg.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation, Meta Platforms.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/lib/Conversion/TritonArithToLinalg/TritonArithToLinalgPass.cpp b/lib/Conversion/TritonArithToLinalg/TritonArithToLinalgPass.cpp
index 42661af..39cf0ba 100644
--- a/lib/Conversion/TritonArithToLinalg/TritonArithToLinalgPass.cpp
+++ b/lib/Conversion/TritonArithToLinalg/TritonArithToLinalgPass.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation, Meta Platforms.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
@@ -215,7 +215,8 @@ public:
         func.getAllArgAttrs(argAttrs);
         func.getAllResultAttrs(resAttrs);
 
-        auto funcFunc = builder.create<func::FuncOp>(func.getLoc(), name, type);
+        auto funcFunc =
+            func::FuncOp::create(builder, func.getLoc(), name, type);
         // Preserve the visibility attribute
         funcFunc.setVisibility(func.getVisibility());
         funcFunc.setAllArgAttrs(argAttrs);
@@ -234,7 +235,7 @@ public:
           // considered terminators.
           if (isa<triton::ReturnOp>(term)) {
             builder.setInsertionPoint(term);
-            builder.create<func::ReturnOp>(func.getLoc(), term->getOperands());
+            func::ReturnOp::create(builder, func.getLoc(), term->getOperands());
             term->erase();
           }
         }
diff --git a/lib/Conversion/TritonPtrToMemref/TritonPtrToMemrefPass.cpp b/lib/Conversion/TritonPtrToMemref/TritonPtrToMemrefPass.cpp
index 689cf1b..09d8c23 100644
--- a/lib/Conversion/TritonPtrToMemref/TritonPtrToMemrefPass.cpp
+++ b/lib/Conversion/TritonPtrToMemref/TritonPtrToMemrefPass.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
@@ -61,7 +61,8 @@ public:
 
     auto createUnrealizedCast = [&](OpBuilder &builder, Type resultType,
                                     ValueRange inputs, Location loc) -> Value {
-      return builder.create<UnrealizedConversionCastOp>(loc, resultType, inputs)
+      return UnrealizedConversionCastOp::create(builder, loc, resultType,
+                                                inputs)
           .getResult(0);
     };
     addSourceMaterialization(createUnrealizedCast);
diff --git a/lib/Conversion/TritonToLinalgExperimental/CollapseShape.cpp b/lib/Conversion/TritonToLinalgExperimental/CollapseShape.cpp
index 495518e..0130ede 100644
--- a/lib/Conversion/TritonToLinalgExperimental/CollapseShape.cpp
+++ b/lib/Conversion/TritonToLinalgExperimental/CollapseShape.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
@@ -83,8 +83,8 @@ struct CollapseFill : public OpRewritePattern<linalg::FillOp> {
     }
     auto elementType = resultType.getElementType();
 
-    auto output = rewriter.create<memref::CollapseShapeOp>(
-        loc,
+    auto output = memref::CollapseShapeOp::create(
+        rewriter, loc,
         MemRefType::get(llvm::ArrayRef<int64_t>{resultType.getNumElements()},
                         elementType),
         result, reassociationMap);
@@ -112,14 +112,15 @@ struct CollapseFill : public OpRewritePattern<linalg::FillOp> {
       reassociationMap[0].push_back(rewriter.getAffineDimExpr(i));
     }
 
-    auto init = rewriter.create<tensor::CollapseShapeOp>(
-        loc, RankedTensorType::get({resultType.getNumElements()}, elementType),
+    auto init = tensor::CollapseShapeOp::create(
+        rewriter, loc,
+        RankedTensorType::get({resultType.getNumElements()}, elementType),
         op.getOutputs()[0], reassociationMap);
     auto fillOp =
-        rewriter.create<linalg::FillOp>(loc, op.getInputs(), ValueRange{init});
+        linalg::FillOp::create(rewriter, loc, op.getInputs(), ValueRange{init});
 
-    auto expandOp = rewriter.create<tensor::ExpandShapeOp>(
-        loc, result.getType(), fillOp.getResult(0), reassociationMap);
+    auto expandOp = tensor::ExpandShapeOp::create(
+        rewriter, loc, result.getType(), fillOp.getResult(0), reassociationMap);
 
     rewriter.replaceOp(op, expandOp.getResult());
     return success();
@@ -224,8 +225,8 @@ struct CollapseTranspose : public OpRewritePattern<linalg::TransposeOp> {
 
     auto loc = op.getLoc();
     sourceType = RankedTensorType::get(collapseShapeInput, elementType);
-    source = rewriter.create<tensor::CollapseShapeOp>(loc, sourceType, source,
-                                                      reassociationMap);
+    source = tensor::CollapseShapeOp::create(rewriter, loc, sourceType, source,
+                                             reassociationMap);
 
     SmallVector<ReassociationExprs> reassociationMapRe(reassociationMap.size());
     int idx = 0;
@@ -235,12 +236,12 @@ struct CollapseTranspose : public OpRewritePattern<linalg::TransposeOp> {
       }
     }
 
-    Value transposeInit = rewriter.create<tensor::CollapseShapeOp>(
-        loc, RankedTensorType::get(transposedShape, elementType), op.getInit(),
-        reassociationMapRe);
+    Value transposeInit = tensor::CollapseShapeOp::create(
+        rewriter, loc, RankedTensorType::get(transposedShape, elementType),
+        op.getInit(), reassociationMapRe);
 
     Value transpose =
-        rewriter.create<linalg::TransposeOp>(loc, source, transposeInit, perm)
+        linalg::TransposeOp::create(rewriter, loc, source, transposeInit, perm)
             .getResults()[0];
 
     rewriter.replaceOpWithNewOp<tensor::ExpandShapeOp>(
@@ -337,8 +338,8 @@ struct CollapseBroadCast : public OpRewritePattern<linalg::GenericOp> {
 
     auto loc = op.getLoc();
     sourceType = RankedTensorType::get(collapseShapeInput, elementType);
-    input = rewriter.create<tensor::CollapseShapeOp>(loc, sourceType, input,
-                                                     reassociationMap);
+    input = tensor::CollapseShapeOp::create(rewriter, loc, sourceType, input,
+                                            reassociationMap);
     resultType = RankedTensorType::get(collapseShapeOutput, elementType);
     size_t resultRank = resultType.getRank();
 
@@ -351,13 +352,14 @@ struct CollapseBroadCast : public OpRewritePattern<linalg::GenericOp> {
 
     assert(op->getNumResults() == 1 && "code assumes single result!");
 
-    auto init = rewriter.create<tensor::CollapseShapeOp>(
-        loc, RankedTensorType::get(resultType.getShape(), elementType),
+    auto init = tensor::CollapseShapeOp::create(
+        rewriter, loc,
+        RankedTensorType::get(resultType.getShape(), elementType),
         op.getOutputs()[0], reassociationMap);
 
-    auto linalgOp = rewriter.create<linalg::GenericOp>(
-        loc, init->getResultTypes(), ValueRange{input}, ValueRange{init},
-        indexingMaps, getNParallelLoopsAttrs(resultRank));
+    auto linalgOp = linalg::GenericOp::create(
+        rewriter, loc, init->getResultTypes(), ValueRange{input},
+        ValueRange{init}, indexingMaps, getNParallelLoopsAttrs(resultRank));
     rewriter.cloneRegionBefore(op.getRegion(), linalgOp.getRegion(),
                                linalgOp.getRegion().begin());
     linalgOp->setAttr("broadcastDims",
@@ -453,8 +455,8 @@ struct CollapseReduce : public OpRewritePattern<linalg::ReduceOp> {
     auto elementType = inputType.getElementType();
     auto loc = op.getLoc();
     auto newInputType = RankedTensorType::get(collapseShapeInput, elementType);
-    input = rewriter.create<tensor::CollapseShapeOp>(loc, newInputType, input,
-                                                     reassociationMap);
+    input = tensor::CollapseShapeOp::create(rewriter, loc, newInputType, input,
+                                            reassociationMap);
 
     SmallVector<ReassociationExprs> reassociationMapOutput;
     int idx = 0;
@@ -469,12 +471,12 @@ struct CollapseReduce : public OpRewritePattern<linalg::ReduceOp> {
             rewriter.getAffineDimExpr(idx++));
       }
     }
-    auto init = rewriter.create<tensor::CollapseShapeOp>(
-        loc, RankedTensorType::get(collapseShapeOutput, elementType),
+    auto init = tensor::CollapseShapeOp::create(
+        rewriter, loc, RankedTensorType::get(collapseShapeOutput, elementType),
         op.getInits()[0], reassociationMapOutput);
-    auto newReduce = rewriter.create<linalg::ReduceOp>(
-        loc, init->getResultTypes(), ValueRange{input}, ValueRange{init},
-        newDims);
+    auto newReduce =
+        linalg::ReduceOp::create(rewriter, loc, init->getResultTypes(),
+                                 ValueRange{input}, ValueRange{init}, newDims);
     rewriter.cloneRegionBefore(op.getRegion(), newReduce.getRegion(),
                                newReduce.getRegion().begin());
 
diff --git a/lib/Conversion/TritonToLinalgExperimental/ReconcilePtrCastsPass.cpp b/lib/Conversion/TritonToLinalgExperimental/ReconcilePtrCastsPass.cpp
index 02caff0..0f7eda6 100644
--- a/lib/Conversion/TritonToLinalgExperimental/ReconcilePtrCastsPass.cpp
+++ b/lib/Conversion/TritonToLinalgExperimental/ReconcilePtrCastsPass.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
@@ -61,8 +61,8 @@ struct SimplifyUnrealizedCast
       }
 
       auto prevInput = unrealizedCast.getInputs().front();
-      auto newCast = rewriter.create<UnrealizedConversionCastOp>(
-          op->getLoc(), op->getResultTypes(), ValueRange{prevInput});
+      auto newCast = UnrealizedConversionCastOp::create(
+          rewriter, op->getLoc(), op->getResultTypes(), ValueRange{prevInput});
 
       rewriter.replaceOp(op, newCast);
       return success();
@@ -90,11 +90,11 @@ struct FromMemrefConverter
     if (unrankedInput && isa<triton::PointerType, ptr::PtrType>(outType)) {
       // from_memref only takes ranked memref, cast the unranked memref to
       // ranked memref first.
-      auto rankedMemref = rewriter.create<memref::CastOp>(
-          op.getLoc(), MemRefType::get({1}, unrankedInput.getElementType()),
-          input);
-      auto memrefToPtr = rewriter.create<tptr::FromMemrefOp>(
-          op->getLoc(),
+      auto rankedMemref = memref::CastOp::create(
+          rewriter, op.getLoc(),
+          MemRefType::get({1}, unrankedInput.getElementType()), input);
+      auto memrefToPtr = tptr::FromMemrefOp::create(
+          rewriter, op->getLoc(),
           ptr::PtrType::get(
               rewriter.getContext(),
               tptr::DefaultMemorySpaceAttr::get(rewriter.getContext())),
@@ -128,14 +128,15 @@ struct ToMemrefConverter : public OpRewritePattern<UnrealizedConversionCastOp> {
       // to_memref can only cast to ranked static shape memref, we have to cast
       // the resulting memref back to unranked
       auto elemType = outUnrankedMemrefType.getElementType();
-      auto ptrToMemref = rewriter.create<tptr::ToMemrefOp>(
-          op->getLoc(), MemRefType::get({1}, elemType), input);
+      auto ptrToMemref = tptr::ToMemrefOp::create(
+          rewriter, op->getLoc(), MemRefType::get({1}, elemType), input);
 
       SmallVector<OpFoldResult> sizes = {rewriter.getIndexAttr(1)};
       SmallVector<OpFoldResult> newStrides = {rewriter.getIndexAttr(1)};
-      auto newUnrankedMemref = rewriter.create<memref::ReinterpretCastOp>(
-          op->getLoc(), MemRefType::get({ShapedType::kDynamic}, elemType),
-          ptrToMemref, rewriter.getIndexAttr(0), sizes, newStrides);
+      auto newUnrankedMemref = memref::ReinterpretCastOp::create(
+          rewriter, op->getLoc(),
+          MemRefType::get({ShapedType::kDynamic}, elemType), ptrToMemref,
+          rewriter.getIndexAttr(0), sizes, newStrides);
 
       rewriter.replaceAllUsesWith(output, newUnrankedMemref);
       rewriter.eraseOp(op);
diff --git a/lib/Conversion/TritonToLinalgExperimental/TritonToLinalgExperimentalPass.cpp b/lib/Conversion/TritonToLinalgExperimental/TritonToLinalgExperimentalPass.cpp
index eafc5a0..dbfb274 100644
--- a/lib/Conversion/TritonToLinalgExperimental/TritonToLinalgExperimentalPass.cpp
+++ b/lib/Conversion/TritonToLinalgExperimental/TritonToLinalgExperimentalPass.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/lib/Conversion/TritonToLinalgExperimental/TritonToPtrPass.cpp b/lib/Conversion/TritonToLinalgExperimental/TritonToPtrPass.cpp
index 60dd0d5..6e3cc9d 100644
--- a/lib/Conversion/TritonToLinalgExperimental/TritonToPtrPass.cpp
+++ b/lib/Conversion/TritonToLinalgExperimental/TritonToPtrPass.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
@@ -182,9 +182,9 @@ struct AddPtrConverter : public OpConversionPattern<triton::AddPtrOp> {
     auto pointeeType = cast<triton::PointerType>(op.getType()).getPointeeType();
     auto offsetType = op.getOffset().getType();
     auto pointeeSizeInBytes =
-        rewriter.create<tptr::TypeOffsetOp>(loc, offsetType, pointeeType);
-    auto scaledOffset =
-        rewriter.create<arith::MulIOp>(loc, op.getOffset(), pointeeSizeInBytes);
+        tptr::TypeOffsetOp::create(rewriter, loc, offsetType, pointeeType);
+    auto scaledOffset = arith::MulIOp::create(rewriter, loc, op.getOffset(),
+                                              pointeeSizeInBytes);
     rewriter.replaceOpWithNewOp<tptr::PtrAddOp>(
         op,
         ptr::PtrType::get(
@@ -214,36 +214,37 @@ struct LoadConverter : public OpConversionPattern<triton::LoadOp> {
     auto pointeeType =
         cast<triton::PointerType>(ptr.getType()).getPointeeType();
 
-    auto memref = rewriter.create<tptr::ToMemrefOp>(
-        op->getLoc(), MemRefType::get({1}, pointeeType), adaptor.getPtr());
+    auto memref = tptr::ToMemrefOp::create(rewriter, op->getLoc(),
+                                           MemRefType::get({1}, pointeeType),
+                                           adaptor.getPtr());
 
-    auto zero = rewriter.create<arith::ConstantIndexOp>(op.getLoc(), 0);
+    auto zero = arith::ConstantIndexOp::create(rewriter, op.getLoc(), 0);
 
     if (op.getMask()) {
-      auto ifOp = rewriter.create<scf::IfOp>(
-          op->getLoc(), op.getMask(),
+      auto ifOp = scf::IfOp::create(
+          rewriter, op->getLoc(), op.getMask(),
           [&](OpBuilder &b, Location loc) {
             // Truthy case, load from the index.
-            Value memrefLoad = rewriter.create<memref::LoadOp>(
-                op->getLoc(), memref, ValueRange{zero});
-            b.create<scf::YieldOp>(loc, memrefLoad);
+            Value memrefLoad = memref::LoadOp::create(rewriter, op->getLoc(),
+                                                      memref, ValueRange{zero});
+            scf::YieldOp::create(b, loc, memrefLoad);
           },
           [&](OpBuilder &b, Location loc) {
             // Falsy case, yield `other` or 0 as the default value.
             if (op.getOther()) {
-              b.create<scf::YieldOp>(loc, op.getOther());
+              scf::YieldOp::create(b, loc, op.getOther());
             } else {
               auto elemType = op.getType();
               auto zeroAttr = b.getZeroAttr(elemType);
               assert(zeroAttr && "unexpected element type");
-              Value val = b.create<arith::ConstantOp>(loc, zeroAttr);
-              b.create<scf::YieldOp>(loc, val);
+              Value val = arith::ConstantOp::create(b, loc, zeroAttr);
+              scf::YieldOp::create(b, loc, val);
             }
           });
       rewriter.replaceOp(op, ifOp);
     } else {
-      auto memrefLoad = rewriter.create<memref::LoadOp>(op->getLoc(), memref,
-                                                        ValueRange{zero});
+      auto memrefLoad = memref::LoadOp::create(rewriter, op->getLoc(), memref,
+                                               ValueRange{zero});
 
       rewriter.replaceOp(op, memrefLoad);
     }
@@ -272,18 +273,19 @@ struct StoreConverter : public OpConversionPattern<triton::StoreOp> {
 
     IRRewriter::InsertionGuard g(rewriter);
     if (op.getMask()) {
-      auto ifOp = rewriter.create<scf::IfOp>(op->getLoc(), op.getMask(),
-                                             /*withElseRegion*/ false);
+      auto ifOp = scf::IfOp::create(rewriter, op->getLoc(), op.getMask(),
+                                    /*withElseRegion*/ false);
       rewriter.setInsertionPointToStart(
           &ifOp.getThenRegion().getBlocks().front());
     }
 
-    auto memref = rewriter.create<tptr::ToMemrefOp>(
-        op->getLoc(), MemRefType::get({1}, pointeeType), adaptor.getPtr());
-    auto zero = rewriter.create<arith::ConstantIndexOp>(op.getLoc(), 0);
+    auto memref = tptr::ToMemrefOp::create(rewriter, op->getLoc(),
+                                           MemRefType::get({1}, pointeeType),
+                                           adaptor.getPtr());
+    auto zero = arith::ConstantIndexOp::create(rewriter, op.getLoc(), 0);
 
-    rewriter.create<memref::StoreOp>(op->getLoc(), op.getValue(), memref,
-                                     ValueRange{zero});
+    memref::StoreOp::create(rewriter, op->getLoc(), op.getValue(), memref,
+                            ValueRange{zero});
 
     rewriter.eraseOp(op);
 
@@ -353,9 +355,10 @@ struct LinalgPtrConverter : public OpConversionPattern<linalg::GenericOp> {
       return failure();
     }
 
-    auto replacement = rewriter.create<linalg::GenericOp>(
-        op.getLoc(), convertedTypes, adaptor.getInputs(), adaptor.getOutputs(),
-        op.getIndexingMapsArray(), op.getIteratorTypesArray());
+    auto replacement = linalg::GenericOp::create(
+        rewriter, op.getLoc(), convertedTypes, adaptor.getInputs(),
+        adaptor.getOutputs(), op.getIndexingMapsArray(),
+        op.getIteratorTypesArray());
 
     Region &region = op.getRegion();
     Block &block = region.front();
@@ -431,7 +434,8 @@ public:
     });
     auto createCast = [&](OpBuilder &builder, Type resultType,
                           ValueRange inputs, Location loc) -> Value {
-      return builder.create<UnrealizedConversionCastOp>(loc, resultType, inputs)
+      return UnrealizedConversionCastOp::create(builder, loc, resultType,
+                                                inputs)
           .getResult(0);
     };
     addTargetMaterialization(createCast);
diff --git a/lib/Conversion/TritonToStructured/TritonToStructuredPass.cpp b/lib/Conversion/TritonToStructured/TritonToStructuredPass.cpp
index 1de70e9..ec80fcd 100644
--- a/lib/Conversion/TritonToStructured/TritonToStructuredPass.cpp
+++ b/lib/Conversion/TritonToStructured/TritonToStructuredPass.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation, Meta Platforms.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
@@ -137,20 +137,20 @@ public:
     // result is still being used by another tt.load or tt.store.
     converter.addSourceMaterialization([](OpBuilder &builder, Type resultType,
                                           ValueRange inputs, Location loc) {
-      return builder.create<UnrealizedConversionCastOp>(loc, resultType, inputs)
+      return UnrealizedConversionCastOp::create(builder, loc, resultType,
+                                                inputs)
           .getResult(0);
     });
 
     // Compute the target materialization, given a value with the pointer type,
     // convert that value to a tuple type.
-    converter.addTargetMaterialization([](OpBuilder &builder,
-                                          TypeRange resultTypes,
-                                          ValueRange inputs,
-                                          Location loc) -> SmallVector<Value> {
-      return builder
-          .create<UnrealizedConversionCastOp>(loc, resultTypes, inputs.front())
-          ->getResults();
-    });
+    converter.addTargetMaterialization(
+        [](OpBuilder &builder, TypeRange resultTypes, ValueRange inputs,
+           Location loc) -> SmallVector<Value> {
+          return UnrealizedConversionCastOp::create(builder, loc, resultTypes,
+                                                    inputs.front())
+              ->getResults();
+        });
 
     ConversionTarget target(getContext());
     scf::populateSCFStructuralTypeConversionsAndLegality(converter, patterns,
@@ -200,8 +200,8 @@ public:
     // during reconcile-unrealized-conversion-casts.
     converter.addSourceMaterialization([](OpBuilder &builder, Type resultType,
                                           ValueRange inputs, Location loc) {
-      return builder
-          .create<UnrealizedConversionCastOp>(loc, resultType, inputs[0])
+      return UnrealizedConversionCastOp::create(builder, loc, resultType,
+                                                inputs[0])
           ->getResult(0);
     });
 
@@ -214,8 +214,8 @@ public:
     converter.addTargetMaterialization([](OpBuilder &builder,
                                           TypeRange resultTypes,
                                           ValueRange inputs, Location loc) {
-      auto placeholder = builder.create<tts::GetStructuredStateOp>(
-          loc, inputs.front().getDefiningOp()->getOperand(0));
+      auto placeholder = tts::GetStructuredStateOp::create(
+          builder, loc, inputs.front().getDefiningOp()->getOperand(0));
       assert(llvm::equal(placeholder.getResultTypes(), resultTypes));
       return placeholder.getResults();
     });
diff --git a/lib/Conversion/TritonToUnstructured/TritonToUnstructuredPass.cpp b/lib/Conversion/TritonToUnstructured/TritonToUnstructuredPass.cpp
index 377f082..867ebd4 100644
--- a/lib/Conversion/TritonToUnstructured/TritonToUnstructuredPass.cpp
+++ b/lib/Conversion/TritonToUnstructured/TritonToUnstructuredPass.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
@@ -252,8 +252,8 @@ public:
         }
 
         OpBuilder b(func->getRegion(0));
-        Value zero = b.create<arith::ConstantOp>(
-            arg.getLoc(),
+        Value zero = arith::ConstantOp::create(
+            b, arg.getLoc(),
             b.getIntegerAttr(IntegerType::get(&getContext(), defaultBitWidth),
                              0));
 
@@ -271,8 +271,8 @@ public:
       }
       auto res = op.getResult();
       OpBuilder b(op);
-      Value zero = b.create<arith::ConstantOp>(
-          op.getLoc(),
+      Value zero = arith::ConstantOp::create(
+          b, op.getLoc(),
           b.getIntegerAttr(IntegerType::get(&getContext(), defaultBitWidth),
                            0));
 
@@ -304,8 +304,8 @@ public:
                   // We are converting a pointer to an integer here,
                   // materialized the pointer using the accumulated offset
                   // that we have stored so far.
-                  auto materializedAddPtr = b.create<triton::AddPtrOp>(
-                      op->getLoc(), offsetInfo.ptrType, offsetInfo.ptr,
+                  auto materializedAddPtr = triton::AddPtrOp::create(
+                      b, op->getLoc(), offsetInfo.ptrType, offsetInfo.ptr,
                       offsetInfo.offset);
 
                   // Change the op to use the "simplified" pointer above.
@@ -337,19 +337,19 @@ public:
                   auto resWidth = std::max(lhsWidth, rhsWidth);
 
                   if (lhsWidth < resWidth) {
-                    prevOff = b.create<arith::ExtSIOp>(
-                        loc, getPtrOffsetType(offsetInfo.ptrType, resWidth),
+                    prevOff = arith::ExtSIOp::create(
+                        b, loc, getPtrOffsetType(offsetInfo.ptrType, resWidth),
                         prevOff);
                   }
 
                   if (rhsWidth < resWidth) {
-                    off = b.create<arith::ExtSIOp>(
-                        loc, getPtrOffsetType(offsetInfo.ptrType, resWidth),
+                    off = arith::ExtSIOp::create(
+                        b, loc, getPtrOffsetType(offsetInfo.ptrType, resWidth),
                         off);
                   }
 
-                  auto accumulatedOff = b.create<arith::AddIOp>(
-                      loc, getPtrOffsetType(addptr.getType(), resWidth),
+                  auto accumulatedOff = arith::AddIOp::create(
+                      b, loc, getPtrOffsetType(addptr.getType(), resWidth),
                       prevOff, off);
 
                   PtrOffset newOffsetInfo{offsetInfo.ptr, addptr.getType(),
@@ -491,8 +491,8 @@ public:
                   }
                 }
 
-                auto gather = b.create<tts::GatherOp>(
-                    loc, load.getType(), offsetInfo.ptr, offsetInfo.offset,
+                auto gather = tts::GatherOp::create(
+                    b, loc, load.getType(), offsetInfo.ptr, offsetInfo.offset,
                     load.getMask(), other);
 
                 load->replaceAllUsesWith(gather->getResults());
@@ -501,8 +501,9 @@ public:
               })
               .Case<triton::StoreOp>([&](triton::StoreOp store) {
                 auto offsetInfo = offsetMap.at(store.getPtr());
-                b.create<tts::ScatterOp>(loc, offsetInfo.ptr, offsetInfo.offset,
-                                         store.getValue(), store.getMask());
+                tts::ScatterOp::create(b, loc, offsetInfo.ptr,
+                                       offsetInfo.offset, store.getValue(),
+                                       store.getMask());
                 store->erase();
                 return success();
               })
@@ -526,26 +527,26 @@ public:
 
                 if (baseOffType != currOffType) {
                   if (currOffType.isIndex()) {
-                    baseOffset = b.create<arith::IndexCastOp>(
-                        loc, b.getIndexType(), baseOffset);
+                    baseOffset = arith::IndexCastOp::create(
+                        b, loc, b.getIndexType(), baseOffset);
                   } else if (currOffType.isInteger()) {
                     if (baseOffType.getIntOrFloatBitWidth() <
                         currOffType.getIntOrFloatBitWidth()) {
-                      baseOffset = b.create<arith::ExtSIOp>(loc, currOffType,
-                                                            baseOffset);
+                      baseOffset = arith::ExtSIOp::create(b, loc, currOffType,
+                                                          baseOffset);
                     } else {
                       // MakeTensorPtrOp only takes i32 offsets, so we need
                       // to truncate if the offsets were already in i64
                       makeTensorPtr.emitWarning(
                           "truncating offsets which may result in data loss");
-                      baseOffset = b.create<arith::TruncIOp>(loc, currOffType,
-                                                             baseOffset);
+                      baseOffset = arith::TruncIOp::create(b, loc, currOffType,
+                                                           baseOffset);
                     }
                   }
                 }
 
-                auto accumulatedOffset = b.create<arith::AddIOp>(
-                    loc, currOffset.getType(), baseOffset, currOffset);
+                auto accumulatedOffset = arith::AddIOp::create(
+                    b, loc, currOffset.getType(), baseOffset, currOffset);
 
                 offsetOpnd.set(accumulatedOffset);
 
diff --git a/lib/Conversion/UnstructuredToMemref/UnstructuredToMemrefPass.cpp b/lib/Conversion/UnstructuredToMemref/UnstructuredToMemrefPass.cpp
index 68cb18a..df97ac6 100644
--- a/lib/Conversion/UnstructuredToMemref/UnstructuredToMemrefPass.cpp
+++ b/lib/Conversion/UnstructuredToMemref/UnstructuredToMemrefPass.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
@@ -53,7 +53,8 @@ public:
     addTargetMaterialization([&](OpBuilder &builder,
                                  UnrankedMemRefType resultType,
                                  ValueRange inputs, Location loc) -> Value {
-      return builder.create<UnrealizedConversionCastOp>(loc, resultType, inputs)
+      return UnrealizedConversionCastOp::create(builder, loc, resultType,
+                                                inputs)
           .getResult(0);
     });
   }
@@ -89,11 +90,11 @@ struct ScalarLoadConverter : public OpConversionPattern<tts::GatherOp> {
     auto basePtr = adaptor.getPtr();
     auto offset = adaptor.getOffset();
 
-    Value loadIndex = rewriter.create<arith::IndexCastOp>(
-        loc, rewriter.getIndexType(), offset);
+    Value loadIndex = arith::IndexCastOp::create(
+        rewriter, loc, rewriter.getIndexType(), offset);
 
-    auto memref = rewriter.create<memref::ReinterpretCastOp>(
-        loc,
+    auto memref = memref::ReinterpretCastOp::create(
+        rewriter, loc,
         getMemrefTypeForScalarPtr(
             cast<triton::PointerType>(gatherOp.getPtr().getType()),
             rewriter.getContext()),
@@ -103,8 +104,8 @@ struct ScalarLoadConverter : public OpConversionPattern<tts::GatherOp> {
 
     auto zeroMap = AffineMap::getConstantMap(0, rewriter.getContext());
 
-    auto scalarLoadOp = rewriter.create<affine::AffineLoadOp>(
-        loc, memref, zeroMap, ValueRange{});
+    auto scalarLoadOp = affine::AffineLoadOp::create(rewriter, loc, memref,
+                                                     zeroMap, ValueRange{});
 
     rewriter.replaceOp(gatherOp, scalarLoadOp.getResult());
 
@@ -134,11 +135,11 @@ struct ScalarStoreConverter : public OpConversionPattern<tts::ScatterOp> {
     auto basePtr = adaptor.getPtr();
     auto offset = adaptor.getOffset();
 
-    Value storeIndex = rewriter.create<arith::IndexCastOp>(
-        loc, rewriter.getIndexType(), offset);
+    Value storeIndex = arith::IndexCastOp::create(
+        rewriter, loc, rewriter.getIndexType(), offset);
 
-    auto memref = rewriter.create<memref::ReinterpretCastOp>(
-        loc,
+    auto memref = memref::ReinterpretCastOp::create(
+        rewriter, loc,
         getMemrefTypeForScalarPtr(
             cast<triton::PointerType>(scatterOp.getPtr().getType()),
             rewriter.getContext()),
@@ -149,8 +150,8 @@ struct ScalarStoreConverter : public OpConversionPattern<tts::ScatterOp> {
     auto storeVal = scatterOp.getValue();
     auto zeroMap = AffineMap::getConstantMap(0, rewriter.getContext());
 
-    rewriter.create<affine::AffineStoreOp>(loc, storeVal, memref, zeroMap,
-                                           ValueRange{});
+    affine::AffineStoreOp::create(rewriter, loc, storeVal, memref, zeroMap,
+                                  ValueRange{});
     rewriter.eraseOp(scatterOp);
 
     return success();
@@ -186,22 +187,19 @@ struct GatherConverter : public OpConversionPattern<tts::GatherOp> {
 
     // Treat the base pointer (memref) as 1D because the offsets are all
     // relative to a single base pointer (already collapsed).
-    auto baseMemref = rewriter
-                          .create<memref::CastOp>(
-                              loc,
-                              MemRefType::get({ShapedType::kDynamic},
-                                              resultType.getElementType()),
-                              ptr)
-                          .getResult();
+    auto baseMemref =
+        memref::CastOp::create(rewriter, loc,
+                               MemRefType::get({ShapedType::kDynamic},
+                                               resultType.getElementType()),
+                               ptr)
+            .getResult();
 
     auto baseTensor =
-        rewriter
-            .create<bufferization::ToTensorOp>(
-                loc,
-                RankedTensorType::get(
-                    SmallVector<int64_t>(1, ShapedType::kDynamic),
-                    resultType.getElementType()),
-                baseMemref, true /* restrict */, false /* writable */)
+        bufferization::ToTensorOp::create(
+            rewriter, loc,
+            RankedTensorType::get(SmallVector<int64_t>(1, ShapedType::kDynamic),
+                                  resultType.getElementType()),
+            baseMemref, true /* restrict */, false /* writable */)
             .getResult();
 
     // The linalg.generic op should have the following inputs:
@@ -213,10 +211,10 @@ struct GatherConverter : public OpConversionPattern<tts::GatherOp> {
       inputs.push_back(gatherOp.getMask());
     }
 
-    auto emptyTensor = rewriter
-                           .create<tensor::EmptyOp>(loc, resultType.getShape(),
-                                                    resultType.getElementType())
-                           .getResult();
+    auto emptyTensor =
+        tensor::EmptyOp::create(rewriter, loc, resultType.getShape(),
+                                resultType.getElementType())
+            .getResult();
 
     // Affine maps for the inputs and one additional output.
     SmallVector<AffineMap> affineMaps(
@@ -227,16 +225,17 @@ struct GatherConverter : public OpConversionPattern<tts::GatherOp> {
     SmallVector<utils::IteratorType> iteratorTypes(
         resultType.getRank(), utils::IteratorType::parallel);
 
-    auto genericOp = rewriter.create<linalg::GenericOp>(
-        loc, TypeRange{resultType}, inputs, ValueRange{emptyTensor}, affineMaps,
-        iteratorTypes, [&](OpBuilder &b, Location loc, ValueRange args) {
+    auto genericOp = linalg::GenericOp::create(
+        rewriter, loc, TypeRange{resultType}, inputs, ValueRange{emptyTensor},
+        affineMaps, iteratorTypes,
+        [&](OpBuilder &b, Location loc, ValueRange args) {
           auto getValueAtIndex = [baseTensor](OpBuilder &b, Location loc,
                                               Value index) -> Value {
             Value index0 =
-                b.create<arith::IndexCastOp>(loc, b.getIndexType(), index);
+                arith::IndexCastOp::create(b, loc, b.getIndexType(), index);
 
-            return b.create<tensor::ExtractOp>(loc, baseTensor,
-                                               ValueRange{index0});
+            return tensor::ExtractOp::create(b, loc, baseTensor,
+                                             ValueRange{index0});
           };
 
           auto offset = args[0];
@@ -245,34 +244,34 @@ struct GatherConverter : public OpConversionPattern<tts::GatherOp> {
             // If there is no mask, simply extract the current element from the
             // base tensor and use it as the yield value.
             auto loadValue = getValueAtIndex(b, loc, offset);
-            b.create<linalg::YieldOp>(loc, loadValue);
+            linalg::YieldOp::create(b, loc, loadValue);
           } else {
             // If the mask value is truthy, the current element is loaded from
             // the base tensor using its offset. Otherwise, if `other` is
             // present, yield `other`. If `other` is not present, a default
             // value of 0 is used.
             auto mask = args[1];
-            auto ifOp = b.create<scf::IfOp>(
-                loc, mask,
+            auto ifOp = scf::IfOp::create(
+                b, loc, mask,
                 [&](OpBuilder &b, Location loc) {
                   // Truthy case, load from the index.
                   auto value = getValueAtIndex(b, loc, offset);
-                  b.create<scf::YieldOp>(loc, value);
+                  scf::YieldOp::create(b, loc, value);
                 },
                 [&](OpBuilder &b, Location loc) {
                   // Falsy case, yield `other` or 0 as the default value.
                   if (gatherOp.getOther()) {
-                    b.create<scf::YieldOp>(loc, gatherOp.getOther());
+                    scf::YieldOp::create(b, loc, gatherOp.getOther());
                   } else {
                     auto elemType = resultType.getElementType();
                     auto zeroAttr = b.getZeroAttr(elemType);
                     assert(zeroAttr && "unexpected element type");
-                    Value extract = b.create<arith::ConstantOp>(loc, zeroAttr);
-                    b.create<scf::YieldOp>(loc, extract);
+                    Value extract = arith::ConstantOp::create(b, loc, zeroAttr);
+                    scf::YieldOp::create(b, loc, extract);
                   }
                 });
 
-            b.create<linalg::YieldOp>(loc, ifOp->getResult(0));
+            linalg::YieldOp::create(b, loc, ifOp->getResult(0));
           }
         });
 
@@ -312,11 +311,10 @@ struct ScatterConverter : public OpConversionPattern<tts::ScatterOp> {
     // Treat the base pointer (memref) as 1D because the offsets are all
     // relative to a single base pointer (already collapsed).
     auto baseMemref =
-        rewriter
-            .create<memref::CastOp>(loc,
-                                    MemRefType::get({ShapedType::kDynamic},
-                                                    valueType.getElementType()),
-                                    ptr)
+        memref::CastOp::create(
+            rewriter, loc,
+            MemRefType::get({ShapedType::kDynamic}, valueType.getElementType()),
+            ptr)
             .getResult();
 
     // The linalg.generic op should have the following inputs:
@@ -339,16 +337,16 @@ struct ScatterConverter : public OpConversionPattern<tts::ScatterOp> {
 
     rewriter.setInsertionPoint(scatterOp);
 
-    auto genericOp = rewriter.create<linalg::GenericOp>(
-        loc, TypeRange{}, inputs, ValueRange{}, affineMaps, iteratorTypes,
-        [&](OpBuilder &b, Location loc, ValueRange args) {
+    auto genericOp = linalg::GenericOp::create(
+        rewriter, loc, TypeRange{}, inputs, ValueRange{}, affineMaps,
+        iteratorTypes, [&](OpBuilder &b, Location loc, ValueRange args) {
           auto storeValueAtIndex = [baseMemref](OpBuilder &b, Location loc,
                                                 Value index, Value value) {
             Value index0 =
-                b.create<arith::IndexCastOp>(loc, b.getIndexType(), index);
+                arith::IndexCastOp::create(b, loc, b.getIndexType(), index);
 
-            b.create<memref::StoreOp>(loc, value, baseMemref,
-                                      ValueRange{index0});
+            memref::StoreOp::create(b, loc, value, baseMemref,
+                                    ValueRange{index0});
           };
 
           auto offset = args[0];
@@ -362,14 +360,14 @@ struct ScatterConverter : public OpConversionPattern<tts::ScatterOp> {
             // If the mask value is truthy, insert the current value to the
             // the base memref using its offset. Otherwise, noop.
             auto mask = args[2];
-            auto ifOp =
-                b.create<scf::IfOp>(loc, mask, [&](OpBuilder &b, Location loc) {
+            auto ifOp = scf::IfOp::create(
+                b, loc, mask, [&](OpBuilder &b, Location loc) {
                   storeValueAtIndex(b, loc, offset, value);
-                  b.create<scf::YieldOp>(loc);
+                  scf::YieldOp::create(b, loc);
                 });
           }
 
-          b.create<linalg::YieldOp>(loc);
+          linalg::YieldOp::create(b, loc);
         });
 
     rewriter.eraseOp(scatterOp);
diff --git a/lib/Dialect/TPtr/IR/TPtrDialect.cpp b/lib/Dialect/TPtr/IR/TPtrDialect.cpp
index 6db2c63..c4b8b7c 100644
--- a/lib/Dialect/TPtr/IR/TPtrDialect.cpp
+++ b/lib/Dialect/TPtr/IR/TPtrDialect.cpp
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #include "mlir/IR/Builders.h"
 
 #include "triton-shared/Dialect/TPtr/IR/TPtrDialect.h"
@@ -58,27 +51,30 @@ void mlir::tptr::TPtrDialect::initialize() {
 }
 
 bool tptr::DefaultMemorySpaceAttr::isValidLoad(
-    Type type, mlir::ptr::AtomicOrdering ordering, IntegerAttr alignment,
+    Type type, mlir::ptr::AtomicOrdering ordering,
+    std::optional<int64_t> alignment, const ::mlir::DataLayout *dataLayout,
     llvm::function_ref<InFlightDiagnostic()> emitError) const {
   return true;
 }
 
 bool tptr::DefaultMemorySpaceAttr::isValidStore(
-    Type type, mlir::ptr::AtomicOrdering ordering, IntegerAttr alignment,
+    Type type, mlir::ptr::AtomicOrdering ordering,
+    std::optional<int64_t> alignment, const ::mlir::DataLayout *dataLayout,
     llvm::function_ref<InFlightDiagnostic()> emitError) const {
   return true;
 }
 
 bool tptr::DefaultMemorySpaceAttr::isValidAtomicOp(
     mlir::ptr::AtomicBinOp binOp, Type type, mlir::ptr::AtomicOrdering ordering,
-    IntegerAttr alignment,
+    std::optional<int64_t> alignment, const ::mlir::DataLayout *dataLayout,
     llvm::function_ref<InFlightDiagnostic()> emitError) const {
   return true;
 }
 
 bool tptr::DefaultMemorySpaceAttr::isValidAtomicXchg(
     Type type, mlir::ptr::AtomicOrdering successOrdering,
-    mlir::ptr::AtomicOrdering failureOrdering, IntegerAttr alignment,
+    mlir::ptr::AtomicOrdering failureOrdering, std::optional<int64_t> alignment,
+    const ::mlir::DataLayout *dataLayout,
     llvm::function_ref<InFlightDiagnostic()> emitError) const {
   return true;
 }
diff --git a/lib/Dialect/TPtr/IR/TPtrOps.cpp b/lib/Dialect/TPtr/IR/TPtrOps.cpp
index 06bffd5..dad589e 100644
--- a/lib/Dialect/TPtr/IR/TPtrOps.cpp
+++ b/lib/Dialect/TPtr/IR/TPtrOps.cpp
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #include "mlir/Bytecode/BytecodeOpInterface.h"
 #include "mlir/Interfaces/SideEffectInterfaces.h" // Required for IR/TPtrOps.h.inc
 
diff --git a/lib/Dialect/TritonStructured/IR/TritonStructuredDialect.cpp b/lib/Dialect/TritonStructured/IR/TritonStructuredDialect.cpp
index 0a9ce77..2af19b8 100644
--- a/lib/Dialect/TritonStructured/IR/TritonStructuredDialect.cpp
+++ b/lib/Dialect/TritonStructured/IR/TritonStructuredDialect.cpp
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #include "triton-shared/Dialect/TritonStructured/IR/TritonStructuredDialect.h"
 
 using namespace mlir;
diff --git a/lib/Dialect/TritonStructured/IR/TritonStructuredOps.cpp b/lib/Dialect/TritonStructured/IR/TritonStructuredOps.cpp
index daa9cf1..150e926 100644
--- a/lib/Dialect/TritonStructured/IR/TritonStructuredOps.cpp
+++ b/lib/Dialect/TritonStructured/IR/TritonStructuredOps.cpp
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #include "triton/Dialect/Triton/IR/Dialect.h"
 #include "triton/Dialect/Triton/IR/Types.h"
 
@@ -53,14 +46,14 @@ Value getScalarValue(Value operand, Location loc, OpBuilder &builder) {
                   if (auto shapedType = dyn_cast<ShapedType>(resType)) {
                     resType = shapedType.getElementType();
                   }
-                  return builder.create<arith::SIToFPOp>(loc, resType, src);
+                  return arith::SIToFPOp::create(builder, loc, resType, src);
                 })
                 .Case<arith::TruncFOp>([&](Operation *op) {
                   auto resType = op->getResults()[0].getType();
                   if (auto shapedType = dyn_cast<ShapedType>(resType)) {
                     resType = shapedType.getElementType();
                   }
-                  return builder.create<arith::TruncFOp>(loc, resType, src);
+                  return arith::TruncFOp::create(builder, loc, resType, src);
                 })
                 .Default([](Operation *op) {
                   llvm_unreachable("unsupported op in generating ");
diff --git a/lib/Dialect/TritonTilingExt/IR/BufferizableOpInterfaceImpl.cpp b/lib/Dialect/TritonTilingExt/IR/BufferizableOpInterfaceImpl.cpp
index d992f2a..d98d763 100644
--- a/lib/Dialect/TritonTilingExt/IR/BufferizableOpInterfaceImpl.cpp
+++ b/lib/Dialect/TritonTilingExt/IR/BufferizableOpInterfaceImpl.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/lib/Dialect/TritonTilingExt/IR/CumSum.cpp b/lib/Dialect/TritonTilingExt/IR/CumSum.cpp
index ebada13..619b653 100644
--- a/lib/Dialect/TritonTilingExt/IR/CumSum.cpp
+++ b/lib/Dialect/TritonTilingExt/IR/CumSum.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/lib/Dialect/TritonTilingExt/IR/TritonTilingExtDialect.cpp b/lib/Dialect/TritonTilingExt/IR/TritonTilingExtDialect.cpp
index 2e90b82..acf118a 100644
--- a/lib/Dialect/TritonTilingExt/IR/TritonTilingExtDialect.cpp
+++ b/lib/Dialect/TritonTilingExt/IR/TritonTilingExtDialect.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
@@ -29,12 +29,12 @@ Value getSlice(OpBuilder &b, Location loc, Value source,
                ArrayRef<OpFoldResult> strides) {
   return TypeSwitch<Type, Value>(source.getType())
       .Case<RankedTensorType>([&](RankedTensorType t) -> Value {
-        return b.create<tensor::ExtractSliceOp>(loc, source, offsets, sizes,
-                                                strides);
+        return tensor::ExtractSliceOp::create(b, loc, source, offsets, sizes,
+                                              strides);
       })
       .Case<MemRefType>([&](MemRefType type) -> Value {
-        return b.create<memref::SubViewOp>(loc, source, offsets, sizes,
-                                           strides);
+        return memref::SubViewOp::create(b, loc, source, offsets, sizes,
+                                         strides);
       })
       .Default([&](Type t) { return nullptr; });
 }
diff --git a/lib/Sanitizer/SanitizerAttributes/SanitizerAttributes.cpp b/lib/Sanitizer/SanitizerAttributes/SanitizerAttributes.cpp
index 3846ee5..a13ed51 100644
--- a/lib/Sanitizer/SanitizerAttributes/SanitizerAttributes.cpp
+++ b/lib/Sanitizer/SanitizerAttributes/SanitizerAttributes.cpp
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #include "llvm/Passes/PassBuilder.h"
 #include "llvm/Passes/PassPlugin.h"
 #include "llvm/Support/CommandLine.h"
diff --git a/lib/Transform/AddLLVMDebugInfo/AddLLVMDebugInfoPass.cpp b/lib/Transform/AddLLVMDebugInfo/AddLLVMDebugInfoPass.cpp
index 2bfe211..c01356b 100644
--- a/lib/Transform/AddLLVMDebugInfo/AddLLVMDebugInfoPass.cpp
+++ b/lib/Transform/AddLLVMDebugInfo/AddLLVMDebugInfoPass.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/lib/Utils/Utils.cpp b/lib/Utils/Utils.cpp
index 7abd47b..2483d53 100644
--- a/lib/Utils/Utils.cpp
+++ b/lib/Utils/Utils.cpp
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #include "triton/Dialect/Triton/IR/Dialect.h"
 
 namespace mlir {
diff --git a/python/examples/bare_matmul.py b/python/examples/bare_matmul.py
index 1793c31..eaa408b 100644
--- a/python/examples/bare_matmul.py
+++ b/python/examples/bare_matmul.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 # this is a benchmark which multiplies square matrices with maximum block size
 # to check the performance of tl.dot operation
 
diff --git a/python/examples/benchmark.py b/python/examples/benchmark.py
index a95c326..f82d344 100644
--- a/python/examples/benchmark.py
+++ b/python/examples/benchmark.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import time
 import numpy as np
 from functools import wraps
diff --git a/python/examples/conftest.py b/python/examples/conftest.py
index 6eafc17..33397c5 100644
--- a/python/examples/conftest.py
+++ b/python/examples/conftest.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import pytest
 import os
 import tempfile
@@ -122,7 +119,8 @@ def pytest_collection_modifyitems(config, items):
             for param_name, param_value in item.callspec.params.items():
                 if (param_name.startswith('dtype') or param_name.endswith('dtype')) and param_value == 'bfloat16':
                     item.add_marker(skip_marker_bfloat)
-                if param_name.startswith('input_precision') and param_value.startswith('tf32'):
+                if param_name.startswith('input_precision') and (param_value.startswith('tf32')
+                                                                 or param_value.startswith('bf16')):
                     item.add_marker(skip_marker_tf32)
                 if (param_name.startswith('dtype') or param_name.endswith('dtype')) and ('float8' in str(param_value)):
                     item.add_marker(skip_marker_float8)
diff --git a/python/examples/test_addptr.py b/python/examples/test_addptr.py
index 53e4ffc..5239c85 100644
--- a/python/examples/test_addptr.py
+++ b/python/examples/test_addptr.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 
 import triton
diff --git a/python/examples/test_blockptr_complex_offset.py b/python/examples/test_blockptr_complex_offset.py
index 2123f1f..94c0cd5 100644
--- a/python/examples/test_blockptr_complex_offset.py
+++ b/python/examples/test_blockptr_complex_offset.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 
 import triton
diff --git a/python/examples/test_early_return.py b/python/examples/test_early_return.py
index 231ea78..2bb59be 100644
--- a/python/examples/test_early_return.py
+++ b/python/examples/test_early_return.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 
 import triton
diff --git a/python/examples/test_einsum_qhmd_hmpd_to_qhmp.py b/python/examples/test_einsum_qhmd_hmpd_to_qhmp.py
index a07553c..ffaa84e 100644
--- a/python/examples/test_einsum_qhmd_hmpd_to_qhmp.py
+++ b/python/examples/test_einsum_qhmd_hmpd_to_qhmp.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 
 import triton
diff --git a/python/examples/test_gather_scatter.py b/python/examples/test_gather_scatter.py
index e448063..db08736 100644
--- a/python/examples/test_gather_scatter.py
+++ b/python/examples/test_gather_scatter.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 
 import triton
diff --git a/python/examples/test_gather_scatter_continuous.py b/python/examples/test_gather_scatter_continuous.py
index 4126408..98fe5c0 100644
--- a/python/examples/test_gather_scatter_continuous.py
+++ b/python/examples/test_gather_scatter_continuous.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import pytest
 import torch
 
diff --git a/python/examples/test_index_select.py b/python/examples/test_index_select.py
index 76de60a..c28b8b1 100644
--- a/python/examples/test_index_select.py
+++ b/python/examples/test_index_select.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import pytest
 import torch
 
diff --git a/python/examples/test_layernorm.py b/python/examples/test_layernorm.py
index abb4dd7..dc9b847 100644
--- a/python/examples/test_layernorm.py
+++ b/python/examples/test_layernorm.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 # This is the Layer Norm forward pass from the Triton tutorial found here:
 # https://github.com/triton-lang/triton/blob/main/python/tutorials/05-layer-norm.py
 
diff --git a/python/examples/test_load_2d_tensor_block.py b/python/examples/test_load_2d_tensor_block.py
index 78d86a0..ec7d53d 100644
--- a/python/examples/test_load_2d_tensor_block.py
+++ b/python/examples/test_load_2d_tensor_block.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 
 import triton
diff --git a/python/examples/test_load_2d_tensor_col.py b/python/examples/test_load_2d_tensor_col.py
index 17055fd..f1ed8ad 100644
--- a/python/examples/test_load_2d_tensor_col.py
+++ b/python/examples/test_load_2d_tensor_col.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 
 import triton
diff --git a/python/examples/test_mask.py b/python/examples/test_mask.py
index 9253187..bc063c1 100644
--- a/python/examples/test_mask.py
+++ b/python/examples/test_mask.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 
 import triton
diff --git a/python/examples/test_mask_loop_iter_arg.py b/python/examples/test_mask_loop_iter_arg.py
index ffef270..3677ed7 100644
--- a/python/examples/test_mask_loop_iter_arg.py
+++ b/python/examples/test_mask_loop_iter_arg.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 import triton
 import pytest
diff --git a/python/examples/test_matmul.py b/python/examples/test_matmul.py
index 2ea8fe2..470006a 100644
--- a/python/examples/test_matmul.py
+++ b/python/examples/test_matmul.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 
 import triton
diff --git a/python/examples/test_mm.py b/python/examples/test_mm.py
index 5d5ba57..5bbdcd5 100644
--- a/python/examples/test_mm.py
+++ b/python/examples/test_mm.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 
 import torch
 import triton
diff --git a/python/examples/test_modulo.py b/python/examples/test_modulo.py
index 130618b..f7e1775 100644
--- a/python/examples/test_modulo.py
+++ b/python/examples/test_modulo.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 
 import triton
diff --git a/python/examples/test_nested_loops.py b/python/examples/test_nested_loops.py
index 5ff3a79..9ce055a 100644
--- a/python/examples/test_nested_loops.py
+++ b/python/examples/test_nested_loops.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 
 import triton
diff --git a/python/examples/test_reduce.py b/python/examples/test_reduce.py
index 3f7339f..2b46a83 100644
--- a/python/examples/test_reduce.py
+++ b/python/examples/test_reduce.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 import pytest
 import math
diff --git a/python/examples/test_scalar_store.py b/python/examples/test_scalar_store.py
index 4249a92..7ef14d4 100644
--- a/python/examples/test_scalar_store.py
+++ b/python/examples/test_scalar_store.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 
 import triton
diff --git a/python/examples/test_sign_extend.py b/python/examples/test_sign_extend.py
index 50928b7..21726e7 100644
--- a/python/examples/test_sign_extend.py
+++ b/python/examples/test_sign_extend.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 
 import triton
diff --git a/python/examples/test_softmax.py b/python/examples/test_softmax.py
index 9bf4257..b3d43df 100644
--- a/python/examples/test_softmax.py
+++ b/python/examples/test_softmax.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 
 import triton
diff --git a/python/examples/test_splat.py b/python/examples/test_splat.py
index dd79e15..a396b5e 100644
--- a/python/examples/test_splat.py
+++ b/python/examples/test_splat.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 
 import triton
diff --git a/python/examples/test_swap.py b/python/examples/test_swap.py
index 5a6a7a1..2693ab6 100644
--- a/python/examples/test_swap.py
+++ b/python/examples/test_swap.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 
 import triton
diff --git a/python/examples/test_tensor_index_iterargs.py b/python/examples/test_tensor_index_iterargs.py
index 159b9d3..4b3ac80 100644
--- a/python/examples/test_tensor_index_iterargs.py
+++ b/python/examples/test_tensor_index_iterargs.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 
 import triton
diff --git a/python/examples/test_unstructured_mask.py b/python/examples/test_unstructured_mask.py
index 58302cc..2b37c2a 100644
--- a/python/examples/test_unstructured_mask.py
+++ b/python/examples/test_unstructured_mask.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 
 import triton
diff --git a/python/examples/test_vec_add.py b/python/examples/test_vec_add.py
index 49e92c6..db2fa09 100644
--- a/python/examples/test_vec_add.py
+++ b/python/examples/test_vec_add.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 
 import triton
diff --git a/test/Conversion/StructuredToMemref/convert_addi_reduce.mlir b/test/Conversion/StructuredToMemref/convert_addi_reduce.mlir
index d8308a1..3d52776 100644
--- a/test/Conversion/StructuredToMemref/convert_addi_reduce.mlir
+++ b/test/Conversion/StructuredToMemref/convert_addi_reduce.mlir
@@ -15,7 +15,6 @@ module {
 
 // CHECK-LABEL:  func.func @addi
 // CHECK-SAME:   ([[PARAM_0_:%.+]]: memref<*xi32>, [[PARAM_1_:%.+]]: i32, [[PARAM_2_:%.+]]: i32, [[PARAM_3_:%.+]]: i32, [[PARAM_4_:%.+]]: i32, [[PARAM_5_:%.+]]: i32, [[PARAM_6_:%.+]]: i32) {
-// CHECK-DAG:       [[CST_0_:%.+]] = arith.constant 0 : index
 // CHECK-DAG:       [[CST_0_1_:%.+]] = arith.constant 0 : i32
 // CHECK-DAG:       [[VAR_0_:%.+]] = tensor.empty() : tensor<4096xi32>
 // CHECK-NOT: separator of consecutive DAGs
@@ -28,7 +27,7 @@ module {
 // CHECK:               linalg.yield [[VAR_3_]] : i32
 // CHECK:             }
 // CHECK-DAG:       [[VAR_extracted_:%.+]] = tensor.extract [[VAR_reduced_]][] : tensor<i32>
-// CHECK-DAG:       [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: {{.}}[[CST_0_]]{{.}}, sizes: [1], strides: [1] : memref<*xi32> to memref<1xi32, strided<[1], offset: ?>>
-// CHECK:           affine.store [[VAR_extracted_]], [[VAR_reinterpret_cast_]][0] : memref<1xi32, strided<[1], offset: ?>>
+// CHECK-DAG:       [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: [0], sizes: [1], strides: [1] : memref<*xi32> to memref<1xi32, strided<[1]>>
+// CHECK:           affine.store [[VAR_extracted_]], [[VAR_reinterpret_cast_]][0] : memref<1xi32, strided<[1]>>
 // CHECK:           return
 // CHECK:         }
diff --git a/test/Conversion/StructuredToMemref/masked_ldst_2d.mlir b/test/Conversion/StructuredToMemref/masked_ldst_2d.mlir
index a753cd6..c2ee2e0 100644
--- a/test/Conversion/StructuredToMemref/masked_ldst_2d.mlir
+++ b/test/Conversion/StructuredToMemref/masked_ldst_2d.mlir
@@ -71,10 +71,9 @@ module {
 // CHECK-DAG:       [[CST_128_:%.+]] = arith.constant 128 : index
 // CHECK-DAG:       [[CST_256_:%.+]] = arith.constant 256 : index
 // CHECK-DAG:       [[CST_0_:%.+]] = arith.constant 0xFF80 : bf16
-// CHECK-DAG:       [[CST_3074_:%.+]] = arith.constant 3074 : index
 // CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:       [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: {{.}}[[CST_3074_]]{{.}}, sizes: [128, 256], strides: [1, 1024] : memref<*xbf16> to memref<128x256xbf16, strided<[1, 1024], offset: ?>>
-// CHECK-DAG:       [[VAR_reinterpret_cast_0_:%.+]] = memref.reinterpret_cast [[PARAM_1_]] to offset: {{.}}[[CST_3074_]]{{.}}, sizes: [128, 256], strides: [1, 1024] : memref<*xbf16> to memref<128x256xbf16, strided<[1, 1024], offset: ?>>
+// CHECK-DAG:       [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: [3074], sizes: [128, 256], strides: [1, 1024] : memref<*xbf16> to memref<128x256xbf16, strided<[1, 1024], offset: 3074>>
+// CHECK-DAG:       [[VAR_reinterpret_cast_0_:%.+]] = memref.reinterpret_cast [[PARAM_1_]] to offset: [3074], sizes: [128, 256], strides: [1, 1024] : memref<*xbf16> to memref<128x256xbf16, strided<[1, 1024], offset: 3074>>
 // CHECK-DAG:       [[VAR_0_:%.+]] = arith.index_cast [[PARAM_2_]] : i32 to index
 // CHECK:           [[VAR_1_:%.+]] = arith.minsi [[VAR_0_]], [[CST_130_]] : index
 // CHECK:           [[VAR_2_:%.+]] = arith.maxsi [[VAR_1_]], [[CST_2_]] : index
@@ -93,12 +92,13 @@ module {
 // CHECK:           scf.if [[VAR_12_]] {
 // CHECK:             linalg.fill ins([[CST_0_]] : bf16) outs([[RES_]] : memref<128x256xbf16>)
 // CHECK:           }
-// CHECK-DAG:       [[VAR_subview_:%.+]] = memref.subview [[VAR_reinterpret_cast_]][0, 0] {{.}}[[VAR_8_]], [[VAR_9_]]{{.}} [1, 1] : memref<128x256xbf16, strided<[1, 1024], offset: ?>> to memref<?x?xbf16, strided<[1, 1024], offset: ?>>
+// CHECK-DAG:       [[VAR_subview_:%.+]] = memref.subview [[VAR_reinterpret_cast_]][0, 0] {{.}}[[VAR_8_]], [[VAR_9_]]{{.}} [1, 1] : memref<128x256xbf16, strided<[1, 1024], offset: 3074>> to memref<?x?xbf16, strided<[1, 1024], offset: 3074>>
 // CHECK-DAG:       [[VAR_subview_1_:%.+]] = memref.subview [[RES_]][0, 0] {{.}}[[VAR_8_]], [[VAR_9_]]{{.}} [1, 1] : memref<128x256xbf16> to memref<?x?xbf16, strided<[256, 1]>>
-// CHECK:           memref.copy [[VAR_subview_]], [[VAR_subview_1_]] : memref<?x?xbf16, strided<[1, 1024], offset: ?>> to memref<?x?xbf16, strided<[256, 1]>>
+// CHECK:           memref.copy [[VAR_subview_]], [[VAR_subview_1_]] : memref<?x?xbf16, strided<[1, 1024], offset: 3074>> to memref<?x?xbf16, strided<[256, 1]>>
 // CHECK:           [[VAR_13_:%.+]] = bufferization.to_tensor [[RES_]] restrict writable : memref<128x256xbf16>
 // CHECK-DAG:       [[VAR_extracted_slice_:%.+]] = tensor.extract_slice [[VAR_13_]][0, 0] {{.}}[[VAR_8_]], [[VAR_9_]]{{.}} [1, 1] : tensor<128x256xbf16> to tensor<?x?xbf16>
-// CHECK-DAG:       [[VAR_subview_2_:%.+]] = memref.subview [[VAR_reinterpret_cast_0_]][0, 0] {{.}}[[VAR_8_]], [[VAR_9_]]{{.}} [1, 1] : memref<128x256xbf16, strided<[1, 1024], offset: ?>> to memref<?x?xbf16, strided<[1, 1024], offset: ?>>
-// CHECK:           bufferization.materialize_in_destination [[VAR_extracted_slice_]] in writable [[VAR_subview_2_]] : (tensor<?x?xbf16>, memref<?x?xbf16, strided<[1, 1024], offset: ?>>) -> ()
+// CHECK-DAG:       [[VAR_subview_2_:%.+]] = memref.subview [[VAR_reinterpret_cast_0_]][0, 0] {{.}}[[VAR_8_]], [[VAR_9_]]{{.}} [1, 1] : memref<128x256xbf16, strided<[1, 1024], offset: 3074>> to memref<?x?xbf16, strided<[1, 1024], offset: 3074>>
+// CHECK:           [[CAST_0:%.+]] = memref.cast [[VAR_subview_2_]] : memref<?x?xbf16, strided<[1, 1024], offset: 3074>> to memref<?x?xbf16, strided<[1, 1024], offset: ?>>
+// CHECK:           bufferization.materialize_in_destination [[VAR_extracted_slice_]] in writable [[CAST_0]] : (tensor<?x?xbf16>, memref<?x?xbf16, strided<[1, 1024], offset: ?>>) -> ()
 // CHECK:           return
 // CHECK:         }
diff --git a/test/Conversion/StructuredToMemref/wraparound_side_by_side.mlir b/test/Conversion/StructuredToMemref/wraparound_side_by_side.mlir
index 5c7f4f4..0075387 100644
--- a/test/Conversion/StructuredToMemref/wraparound_side_by_side.mlir
+++ b/test/Conversion/StructuredToMemref/wraparound_side_by_side.mlir
@@ -57,6 +57,7 @@ module {
 
 // CHECK-LABEL:  func.func @wrap_side_by_side_masked_loop_01234567
 // CHECK-SAME:   ([[PARAM_0_:%.+]]: memref<*xf32>, [[PARAM_1_:%.+]]: memref<*xf32>, [[PARAM_2_:%.+]]: i32, [[PARAM_3_:%.+]]: i32, [[PARAM_4_:%.+]]: i32, [[PARAM_5_:%.+]]: i32, [[PARAM_6_:%.+]]: i32, [[PARAM_7_:%.+]]: i32, [[PARAM_8_:%.+]]: i32, [[PARAM_9_:%.+]]: i32, [[PARAM_10_:%.+]]: i32, [[PARAM_11_:%.+]]: i32, [[PARAM_12_:%.+]]: i32, [[PARAM_13_:%.+]]: i32) {
+// CHECK:           [[CONSTANT_0:%.+]] = arith.constant 1 : index
 // CHECK-DAG:       [[CST_4_:%.+]] = arith.constant 4 : index
 // CHECK-DAG:       [[CST_4_1_:%.+]] = arith.constant 4 : i32
 // CHECK-DAG:       [[CST_2_:%.+]] = arith.constant 2 : i32
@@ -90,17 +91,19 @@ module {
 // CHECK-DAG:         [[VAR_16_:%.+]] = arith.addi [[VAR_14_]], [[CST_4_]] : index
 // CHECK:             [[VAR_17_:%.+]] = arith.minsi [[VAR_16_]], [[VAR_5_]] : index
 // CHECK:             [[VAR_18_:%.+]] = arith.subi [[VAR_17_]], [[VAR_14_]] : index
-// CHECK-DAG:         [[VAR_reinterpret_cast_0_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: {{.}}[[VAR_13_]]{{.}}, sizes: {{.}}[[CST_4_]], [[VAR_18_]]{{.}}, strides: {{.}}[[VAR_0_]], [[VAR_3_]]{{.}} : memref<*xf32> to memref<?x?xf32, strided<[?, ?], offset: ?>>
+// CHECK-DAG:         [[VAR_reinterpret_cast_0_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: {{.}}[[VAR_13_]]{{.}}, sizes: [4, [[VAR_18_]]], strides: {{.}}[[VAR_0_]], [[VAR_3_]]{{.}} : memref<*xf32> to memref<4x?xf32, strided<[?, ?], offset: ?>>
 // CHECK-DAG:         [[VAR_19_:%.+]] = arith.subi [[CST_4_]], [[VAR_18_]] : index
 // CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_reinterpret_cast_1_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: {{.}}[[VAR_15_]]{{.}}, sizes: {{.}}[[CST_4_]], [[VAR_19_]]{{.}}, strides: {{.}}[[VAR_0_]], [[VAR_3_]]{{.}} : memref<*xf32> to memref<?x?xf32, strided<[?, ?], offset: ?>>
+// CHECK-DAG:         [[VAR_reinterpret_cast_1_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: {{.}}[[VAR_15_]]{{.}}, sizes: [4, [[VAR_19_]]], strides: {{.}}[[VAR_0_]], [[VAR_3_]]{{.}} : memref<*xf32> to memref<4x?xf32, strided<[?, ?], offset: ?>>
 // CHECK-DAG:         [[RES_:%.+]] = memref.alloc() : memref<4x4xf32>
 // CHECK:             linalg.fill ins([[CST_minus_9_dot_900000_]] : f32) outs([[RES_]] : memref<4x4xf32>)
-// CHECK-DAG:         [[VAR_subview_:%.+]] = memref.subview [[VAR_reinterpret_cast_0_]][0, 0] [2, [[VAR_18_]]{{.}} [1, 1] : memref<?x?xf32, strided<[?, ?], offset: ?>> to memref<2x?xf32, strided<[?, ?], offset: ?>>
+// CHECK:             [[DIM_0:%.+]] = memref.dim [[VAR_reinterpret_cast_0_]], [[CONSTANT_0]] : memref<4x?xf32, strided<[?, ?], offset: ?>>
+// CHECK:             [[DIM_1:%.+]] = memref.dim [[VAR_reinterpret_cast_1_]], [[CONSTANT_0]] : memref<4x?xf32, strided<[?, ?], offset: ?>>
+// CHECK-DAG:         [[VAR_subview_:%.+]] = memref.subview [[VAR_reinterpret_cast_0_]][0, 0] [2, [[DIM_0]]{{.}} [1, 1] : memref<4x?xf32, strided<[?, ?], offset: ?>> to memref<2x?xf32, strided<[?, ?], offset: ?>>
 // CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_subview_2_:%.+]] = memref.subview [[VAR_reinterpret_cast_1_]][0, 0] [2, [[VAR_19_]]{{.}} [1, 1] : memref<?x?xf32, strided<[?, ?], offset: ?>> to memref<2x?xf32, strided<[?, ?], offset: ?>>
-// CHECK-DAG:         [[VAR_subview_3_:%.+]] = memref.subview [[RES_]][0, 0] [2, [[VAR_18_]]{{.}} [1, 1] : memref<4x4xf32> to memref<2x?xf32, strided<[4, 1]>>
-// CHECK-DAG:         [[VAR_subview_4_:%.+]] = memref.subview [[RES_]][0, [[VAR_18_]]{{.}} [2, [[VAR_19_]]{{.}} [1, 1] : memref<4x4xf32> to memref<2x?xf32, strided<[4, 1], offset: ?>>
+// CHECK-DAG:         [[VAR_subview_2_:%.+]] = memref.subview [[VAR_reinterpret_cast_1_]][0, 0] [2, [[DIM_1]]{{.}} [1, 1] : memref<4x?xf32, strided<[?, ?], offset: ?>> to memref<2x?xf32, strided<[?, ?], offset: ?>>
+// CHECK-DAG:         [[VAR_subview_3_:%.+]] = memref.subview [[RES_]][0, 0] [2, [[DIM_0]]{{.}} [1, 1] : memref<4x4xf32> to memref<2x?xf32, strided<[4, 1]>>
+// CHECK-DAG:         [[VAR_subview_4_:%.+]] = memref.subview [[RES_]][0, [[DIM_0]]{{.}} [2, [[DIM_1]]{{.}} [1, 1] : memref<4x4xf32> to memref<2x?xf32, strided<[4, 1], offset: ?>>
 // CHECK:             memref.copy [[VAR_subview_]], [[VAR_subview_3_]] : memref<2x?xf32, strided<[?, ?], offset: ?>> to memref<2x?xf32, strided<[4, 1]>>
 // CHECK:             memref.copy [[VAR_subview_2_]], [[VAR_subview_4_]] : memref<2x?xf32, strided<[?, ?], offset: ?>> to memref<2x?xf32, strided<[4, 1], offset: ?>>
 // CHECK:             [[VAR_20_:%.+]] = bufferization.to_tensor [[RES_]] restrict writable : memref<4x4xf32>
diff --git a/test/Conversion/StructuredToMemref/wraparound_stacked.mlir b/test/Conversion/StructuredToMemref/wraparound_stacked.mlir
index 4e5ce94..953b372 100644
--- a/test/Conversion/StructuredToMemref/wraparound_stacked.mlir
+++ b/test/Conversion/StructuredToMemref/wraparound_stacked.mlir
@@ -84,17 +84,19 @@ module {
 // CHECK:             [[VAR_14_:%.+]] = arith.subi [[VAR_13_]], [[VAR_11_]] : index
 // CHECK:             [[VAR_15_:%.+]] = arith.divsi [[VAR_14_]], [[VAR_1_]] : index
 // CHECK:             [[VAR_16_:%.+]] = arith.minsi [[VAR_15_]], [[CST_4_]] : index
-// CHECK-DAG:         [[VAR_reinterpret_cast_0_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: {{.}}[[VAR_11_]]{{.}}, sizes: {{.}}[[VAR_16_]], [[CST_4_]]{{.}}, strides: {{.}}[[VAR_1_]], [[VAR_4_]]{{.}} : memref<*xf32> to memref<?x?xf32, strided<[?, ?], offset: ?>>
+// CHECK-DAG:         [[VAR_reinterpret_cast_0_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: {{.}}[[VAR_11_]]{{.}}, sizes: [[[VAR_16_]], 4], strides: {{.}}[[VAR_1_]], [[VAR_4_]]{{.}} : memref<*xf32> to memref<?x4xf32, strided<[?, ?], offset: ?>>
 // CHECK-DAG:         [[VAR_17_:%.+]] = arith.subi [[CST_4_]], [[VAR_16_]] : index
 // CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_reinterpret_cast_1_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: {{.}}[[VAR_12_]]{{.}}, sizes: {{.}}[[VAR_17_]], [[CST_4_]]{{.}}, strides: {{.}}[[VAR_1_]], [[VAR_4_]]{{.}} : memref<*xf32> to memref<?x?xf32, strided<[?, ?], offset: ?>>
+// CHECK-DAG:         [[VAR_reinterpret_cast_1_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: {{.}}[[VAR_12_]]{{.}}, sizes: [[[VAR_17_]], 4], strides: {{.}}[[VAR_1_]], [[VAR_4_]]{{.}} : memref<*xf32> to memref<?x4xf32, strided<[?, ?], offset: ?>>
 // CHECK-DAG:         [[RES_:%.+]] = memref.alloc() : memref<4x4xf32>
 // CHECK:             linalg.fill ins([[CST_minus_9_dot_900000_]] : f32) outs([[RES_]] : memref<4x4xf32>)
-// CHECK-DAG:         [[VAR_subview_:%.+]] = memref.subview [[VAR_reinterpret_cast_0_]][0, 0] {{.}}[[VAR_16_]], 3] [1, 1] : memref<?x?xf32, strided<[?, ?], offset: ?>> to memref<?x3xf32, strided<[?, ?], offset: ?>>
+// CHECK:             [[DIM_0:%.+]] = memref.dim [[VAR_reinterpret_cast_0_]], [[CST_0_1_]] : memref<?x4xf32, strided<[?, ?], offset: ?>>
+// CHECK:             [[DIM_1:%.+]] = memref.dim [[VAR_reinterpret_cast_1_]], [[CST_0_1_]] : memref<?x4xf32, strided<[?, ?], offset: ?>>
+// CHECK-DAG:         [[VAR_subview_:%.+]] = memref.subview [[VAR_reinterpret_cast_0_]][0, 0] {{.}}[[DIM_0]], 3] [1, 1] : memref<?x4xf32, strided<[?, ?], offset: ?>> to memref<?x3xf32, strided<[?, ?], offset: ?>>
 // CHECK-NOT: separator of consecutive DAGs
-// CHECK-DAG:         [[VAR_subview_2_:%.+]] = memref.subview [[VAR_reinterpret_cast_1_]][0, 0] {{.}}[[VAR_17_]], 3] [1, 1] : memref<?x?xf32, strided<[?, ?], offset: ?>> to memref<?x3xf32, strided<[?, ?], offset: ?>>
-// CHECK-DAG:         [[VAR_subview_3_:%.+]] = memref.subview [[RES_]][0, 0] {{.}}[[VAR_16_]], 3] [1, 1] : memref<4x4xf32> to memref<?x3xf32, strided<[4, 1]>>
-// CHECK-DAG:         [[VAR_subview_4_:%.+]] = memref.subview [[RES_]]{{.}}[[VAR_16_]], 0] {{.}}[[VAR_17_]], 3] [1, 1] : memref<4x4xf32> to memref<?x3xf32, strided<[4, 1], offset: ?>>
+// CHECK-DAG:         [[VAR_subview_2_:%.+]] = memref.subview [[VAR_reinterpret_cast_1_]][0, 0] {{.}}[[DIM_1]], 3] [1, 1] : memref<?x4xf32, strided<[?, ?], offset: ?>> to memref<?x3xf32, strided<[?, ?], offset: ?>>
+// CHECK-DAG:         [[VAR_subview_3_:%.+]] = memref.subview [[RES_]][0, 0] {{.}}[[DIM_0]], 3] [1, 1] : memref<4x4xf32> to memref<?x3xf32, strided<[4, 1]>>
+// CHECK-DAG:         [[VAR_subview_4_:%.+]] = memref.subview [[RES_]]{{.}}[[DIM_0]], 0] {{.}}[[DIM_1]], 3] [1, 1] : memref<4x4xf32> to memref<?x3xf32, strided<[4, 1], offset: ?>>
 // CHECK:             memref.copy [[VAR_subview_]], [[VAR_subview_3_]] : memref<?x3xf32, strided<[?, ?], offset: ?>> to memref<?x3xf32, strided<[4, 1]>>
 // CHECK:             memref.copy [[VAR_subview_2_]], [[VAR_subview_4_]] : memref<?x3xf32, strided<[?, ?], offset: ?>> to memref<?x3xf32, strided<[4, 1], offset: ?>>
 // CHECK:             [[VAR_18_:%.+]] = bufferization.to_tensor [[RES_]] restrict writable : memref<4x4xf32>
diff --git a/test/Conversion/TritonToLinalgExperimental/conditional_ptr_as_src.mlir b/test/Conversion/TritonToLinalgExperimental/conditional_ptr_as_src.mlir
index d8c015c..f413899 100644
--- a/test/Conversion/TritonToLinalgExperimental/conditional_ptr_as_src.mlir
+++ b/test/Conversion/TritonToLinalgExperimental/conditional_ptr_as_src.mlir
@@ -29,7 +29,6 @@ module {
 // CHECK-LABEL:  func.func @simple_cf_into_structured_load
 // CHECK-SAME:   ([[PARAM_0_:%.+]]: memref<*xf32>, [[PARAM_1_:%.+]]: memref<*xf32>, [[PARAM_2_:%.+]]: i32, [[PARAM_3_:%.+]]: i32, [[PARAM_4_:%.+]]: i32, [[PARAM_5_:%.+]]: i32, [[PARAM_6_:%.+]]: i32, [[PARAM_7_:%.+]]: i32, [[PARAM_8_:%.+]]: i32) {
 // CHECK-DAG:       [[VAR_0_:%.+]] = tptr.type_offset f32  : i32
-// CHECK-DAG:       [[CST_6_:%.+]] = arith.constant 6 : index
 // CHECK-DAG:       [[CST_2_:%.+]] = arith.constant 2 : i32
 // CHECK-DAG:       [[CST_1_:%.+]] = arith.constant 1 : i32
 // CHECK-DAG:       [[VAR_cast_:%.+]] = memref.cast [[PARAM_0_]] : memref<*xf32> to memref<1xf32>
@@ -48,9 +47,9 @@ module {
 // CHECK:             scf.yield [[VAR_7_1_]] : !ptr.ptr<#tptr.default_memory_space>
 // CHECK:           }
 // CHECK:           [[VAR_4_:%.+]] = tptr.to_memref [[VAR_3_]] : <#tptr.default_memory_space> to memref<1xf32>
-// CHECK-DAG:       [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[VAR_4_]] to offset: {{.}}[[CST_6_]]{{.}}, sizes: [4], strides: [1] : memref<1xf32> to memref<4xf32, strided<[1], offset: ?>>
+// CHECK-DAG:       [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[VAR_4_]] to offset: [6], sizes: [4], strides: [1] : memref<1xf32> to memref<4xf32, strided<[1], offset: 6>>
 // CHECK-DAG:       [[RES_:%.+]] = memref.alloc() : memref<4xf32>
-// CHECK:           memref.copy [[VAR_reinterpret_cast_]], [[RES_]] : memref<4xf32, strided<[1], offset: ?>> to memref<4xf32>
+// CHECK:           memref.copy [[VAR_reinterpret_cast_]], [[RES_]] : memref<4xf32, strided<[1], offset: 6>> to memref<4xf32>
 // CHECK-DAG:       [[VAR_5_:%.+]] = bufferization.to_tensor [[RES_]] restrict writable : memref<4xf32> to tensor<4xf32>
 // CHECK-DAG:       [[VAR_reinterpret_cast_0_:%.+]] = memref.reinterpret_cast [[PARAM_1_]] to offset: [0], sizes: [4], strides: [1] : memref<*xf32> to memref<4xf32, strided<[1]>>
 // CHECK:           bufferization.materialize_in_destination [[VAR_5_]] in writable [[VAR_reinterpret_cast_0_]] : (tensor<4xf32>, memref<4xf32, strided<[1]>>) -> ()
diff --git a/test/Conversion/TritonToLinalgExperimental/convert_unsplat.mlir b/test/Conversion/TritonToLinalgExperimental/convert_unsplat.mlir
index 9fd9cc5..d48ebeb 100644
--- a/test/Conversion/TritonToLinalgExperimental/convert_unsplat.mlir
+++ b/test/Conversion/TritonToLinalgExperimental/convert_unsplat.mlir
@@ -41,8 +41,8 @@ module {
 // CHECK:           } -> tensor<1xi1>
 // CHECK:           [[VAR_extracted_:%.+]] = tensor.extract [[VAR_6_]]{{.}}[[CST_0_1_]]{{.}} : tensor<1xi1>
 // CHECK:           scf.if [[VAR_extracted_]] {
-// CHECK:             [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: {{.}}[[CST_0_1_]]{{.}}, sizes: [1], strides: [1] : memref<*xi32> to memref<1xi32, strided<[1], offset: ?>>
-// CHECK:             affine.store [[CST_42_]], [[VAR_reinterpret_cast_]][0] : memref<1xi32, strided<[1], offset: ?>>
+// CHECK:             [[VAR_reinterpret_cast_:%.+]] = memref.reinterpret_cast [[PARAM_0_]] to offset: [0], sizes: [1], strides: [1] : memref<*xi32> to memref<1xi32, strided<[1]>>
+// CHECK:             affine.store [[CST_42_]], [[VAR_reinterpret_cast_]][0] : memref<1xi32, strided<[1]>>
 // CHECK:           }
 // CHECK:           return
 // CHECK:         }
diff --git a/test/lit.cfg.py b/test/lit.cfg.py
index 54a7d16..72c9cc7 100644
--- a/test/lit.cfg.py
+++ b/test/lit.cfg.py
@@ -1,7 +1,4 @@
 # -*- Python -*-
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 
 import os
 import platform
diff --git a/tools/triton-shared-opt/RegisterTritonSharedDialects.h b/tools/triton-shared-opt/RegisterTritonSharedDialects.h
index 0165b03..d78f96b 100644
--- a/tools/triton-shared-opt/RegisterTritonSharedDialects.h
+++ b/tools/triton-shared-opt/RegisterTritonSharedDialects.h
@@ -1,10 +1,3 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
 #pragma once
 #include "mlir/Dialect/Bufferization/IR/Bufferization.h"
 #include "mlir/Dialect/Func/IR/FuncOps.h"
diff --git a/tools/triton-shared-opt/triton-shared-opt.cpp b/tools/triton-shared-opt/triton-shared-opt.cpp
index ff19e0e..3ed831d 100644
--- a/tools/triton-shared-opt/triton-shared-opt.cpp
+++ b/tools/triton-shared-opt/triton-shared-opt.cpp
@@ -1,6 +1,6 @@
 //===----------------------------------------------------------------------===//
 //
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
+// Copyright (c) Microsoft Corporation.
 // Licensed under the MIT license.
 //
 //===----------------------------------------------------------------------===//
diff --git a/triton-hash.txt b/triton-hash.txt
index 80f8308..0f8b487 100644
--- a/triton-hash.txt
+++ b/triton-hash.txt
@@ -1 +1 @@
-e44bd1c83c1c3e8deac7c4f02683cfb3cc395c8b
+dbfbc1e1e6cca56eeaa853050b7962e7445f0a82
\ No newline at end of file
diff --git a/triton-san/examples/buffer-overflow.py b/triton-san/examples/buffer-overflow.py
index 4f647fb..a0599a0 100644
--- a/triton-san/examples/buffer-overflow.py
+++ b/triton-san/examples/buffer-overflow.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 import triton
 import triton.language as tl
diff --git a/triton-san/examples/data-race.py b/triton-san/examples/data-race.py
index 86a0fed..cfca91b 100644
--- a/triton-san/examples/data-race.py
+++ b/triton-san/examples/data-race.py
@@ -1,6 +1,3 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-# Licensed under the MIT license.
-
 import torch
 import triton
 import triton.language as tl
diff --git a/triton_shared.cc b/triton_shared.cc
index 43058b6..e69ed1d 100644
--- a/triton_shared.cc
+++ b/triton_shared.cc
@@ -1,11 +1,4 @@
-//===----------------------------------------------------------------------===//
-//
-// Copyright (c) Meta Platforms, Inc. and affiliates, Microsoft Corporation.
-// Licensed under the MIT license.
-//
-//===----------------------------------------------------------------------===//
-
-#include <pybind11/pybind11.h>
+#include <pybind11/pybind11.h>
 
 namespace py = pybind11;
 
